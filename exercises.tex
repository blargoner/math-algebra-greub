% Notes and exercises from Linear Algebra and Multilinear Algebra by Greub
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier,diagrams,tikz-cd,stmaryrd,mathtools}
\usepackage[hidelinks]{hyperref}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\um}{\reflectbox{\(\mu\)}}

\newcommand{\from}{\leftarrow}
\newcommand{\iso}{\cong}
\renewcommand{\equiv}{\sim}
\newcommand{\orth}{\perp}
\newcommand{\divides}{\mid}

\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Det}{Det}

% much ado about dots
\makeatletter
\newcommand{\bigcdot}[1]{\mathbin{\mathpalette\bigcdot@{#1}}}
\newcommand{\bigcdot@}[2]{%
  \sbox0{$#1\vcenter{}$}% math axis
  \sbox2{$#1\cdot\m@th$}%
  \hbox{%
    \hfil
    \raise\ht0\hbox{%
      \scalebox{#2}{%
        \lower\ht0\hbox{$#1\bullet\m@th$}%
      }%
    }%
    \hfil
  }%
}
\makeatother

\newcommand{\union}{\cup}
\newcommand{\sect}{\cap}
\newcommand{\ssect}{\mathop{\widehat{\cap}}}
\newcommand{\bigsect}{\bigcap}
\newcommand{\medsect}{{\textstyle\bigsect}}
\newcommand{\after}{\circ}
\newcommand{\join}{\vee}
\newcommand{\meet}{\wedge}
\newcommand{\bigmeet}{\bigwedge}
\newcommand{\dsum}{\oplus}
\newcommand{\bigdsum}{\bigoplus}
\newcommand{\mult}{\cdot}
\newcommand{\cross}{\times}
\newcommand{\tprod}{\otimes}
\newcommand{\bigtprod}{\bigotimes}
\newcommand{\medtprod}{{\textstyle\bigtprod}}
\newcommand{\stprod}{\mathop{\widehat{\otimes}}}
\newcommand{\eprod}{\wedge}
\newcommand{\eprodf}{\mathop{\tilde{\wedge}}}
\newcommand{\bigeprod}{\bigwedge}
\newcommand{\medeprod}{{\textstyle\bigeprod}}
\newcommand{\medeprodf}{\tilde{\medeprod}}
\newcommand{\fprod}{\bigcdot{0.5}}
\newcommand{\mprod}{\bigcdot{0.5}}
\newcommand{\bprod}{\mathop{\square}}
\newcommand{\sbprod}{\mathop{\widehat{\square}}}
\newcommand{\tdot}{\bigcdot{0.75}}
\newcommand{\adot}{\bigcdot{0.75}}

\newcommand{\delete}{\widehat}
\newcommand{\gen}[1]{\langle#1\rangle}
\newcommand{\sprod}[2]{\langle#1,#2\rangle}
\newcommand{\bigsprod}[2]{\big\langle#1,#2\big\rangle}
\newcommand{\ssprod}[2]{\langle\!\langle#1,#2\rangle\!\rangle}
\newcommand{\oc}[1]{#1^{\perp}}
\newcommand{\occ}[1]{#1^{\perp\perp}}
\newcommand{\opp}[1]{#1^{\mathrm{opp}}}
\newcommand{\iprod}[2]{(#1,#2)}
\newcommand{\norm}[1]{|#1|}
\newcommand{\abs}[1]{|#1|}
\newcommand{\adj}[1]{\widetilde{#1}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\proj}[1]{\overline{#1}}
\newcommand{\multi}[4]{#2_{#3}#1\cdots#1#2_{#4}}
\newcommand{\sects}[3]{\multi{\sect}{#1}{#2}{#3}}
\newcommand{\timess}[3]{\multi{\cross}{#1}{#2}{#3}}
\newcommand{\tprods}[3]{\multi{\tprod}{#1}{#2}{#3}}
\newcommand{\eprods}[3]{\multi{\eprod}{#1}{#2}{#3}}
\newcommand{\fprods}[3]{\multi{\fprod}{#1}{#2}{#3}}
\newcommand{\mprods}[3]{\multi{\mprod}{#1}{#2}{#3}}
\newcommand{\bprods}[3]{\multi{\bprod}{#1}{#2}{#3}}
\newcommand{\circled}[1]{\text{\tiny\textcircled{\textit{#1}}}}
\newcommand{\sign}[1]{\varepsilon_{#1}}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\stroked}[1]{\mathrlap{\text{\kern0.05em-}}#1}
\newcommand{\unit}{\stroked{t}}

\newarrow{Dashto}{}{dash}{}{dash}>
\newarrow{Equalto}{}={}={}

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{warn}{Warning}

% Meta
\title{Notes and exercises from\\\textit{Linear Algebra} and \textit{Multilinear Algebra}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

% Intro
\section*{Introduction}
This document contains notes and exercises from \cite{greub1} and~\cite{greub2}.

\bigskip
\noindent
{\boldmath\textbf{Unless otherwise stated, \(\Gamma\)~denotes a field of characteristic~\(0\) over which all vector spaces are defined.}}

% Linear algebra
\newpage
\part*{Linear Algebra}
\section*{Chapter~I}
\subsection*{\S~1}
\begin{rmk}
The free vector space~\(C(X)\) is intuitively the space of all ``formal linear combinations'' of \(x\in X\).
\end{rmk}

\subsection*{\S~2}
\begin{exer}[5 - Universal property of~\(C(X)\)]
Let \(X\)~be a set and \(C(X)\)~the free vector space on~\(X\) (subsection~1.7). Recall
\[C(X)=\{\,f:X\to\Gamma\mid f(x)=0\text{ for all but finitely many }x\in X\,\}\]
The inclusion map \(i_X:X\to C(X)\) is defined by \(a\mapsto f_a\) where \(f_a\)~is the ``characteristic function'' of~\(a\): \(f_a(a)=1\) and \(f_a(x)=0\) for all \(x\ne a\). For \(f\in C(X)\), \(f=\sum_{a\in X}f(a)f_a\).
\begin{enumerate}
\item[(i)] If \(F\)~is a vector space and \(f:X\to F\), there is a unique \emph{linear} \(\varphi:C(X)\to F\) ``extending~\(f\)'' in the sense that \(\varphi\after i_X=f\):
\begin{diagram}[nohug]
X	&\rTo^{i_X}	&C(X)\\
	&\rdTo_f	&\dDashto>{\varphi}\\
	&			&F
\end{diagram}

\item[(ii)] If \(\alpha:X\to Y\), there is a unique \emph{linear} \(\alpha_*:C(X)\to C(Y)\) which makes the following diagram commute:
\begin{diagram}
X			&\rTo^{\alpha}			&Y\\
\dTo<{i_X}	&						&\dTo>{i_Y}\\
C(X)		&\rDashto_{\alpha_*}	&C(Y)
\end{diagram}
If \(\beta:Y\to Z\), then \((\beta\after\alpha)_*=\beta_*\after\alpha_*\).

\item[(iii)] If \(E\)~is a vector space, there is a unique linear map \(\pi_E:C(E)\to E\) such that \(\pi_E\after i_E=\iota_E\) (where \(\iota_E:E\to E\) is the identity map):
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&C(E)\\
	&\rdTo_{\iota_E}	&\dDashto>{\pi_E}\\
	&			&E
\end{diagram}

\item[(iv)] If \(E\) and~\(F\) are vector spaces and \(\varphi:E\to F\), then \(\varphi\)~is linear if and only if \(\pi_F\after\varphi_*=\varphi\after\pi_E\):
\begin{diagram}
E				&\rTo^{\varphi}		&F\\
\uTo<{\pi_E}	&					&\uTo>{\pi_F}\\
C(E)			&\rTo_{\varphi_*}	&C(F)
\end{diagram}

\item[(v)] Let \(E\)~be a vector space and \(N(E)\)~the subspace of~\(C(E)\) generated by all elements of the form
\[f_{\lambda a+\mu b}-\lambda f_a-\mu f_b\qquad(a,b\in E\text{ and }\lambda,\mu\in\Gamma)\]
Then \(\ker\pi_E=N(E)\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] By Proposition~II, since \(i_X(X)\)~is a basis of~\(C(X)\).

\item[(ii)] By~(i), applied to~\(i_Y\after\alpha\). Note \(\beta_*\after\alpha_*\)~is linear such that
\[(\beta_*\after\alpha_*)\after i_X=i_Z\after(\beta\after\alpha)\]
so \(\beta_*\after\alpha_*=(\beta\after\alpha)_*\) by uniqueness:
\begin{diagram}
X			&\rTo^{\alpha}		&Y			&\rTo^{\beta}	&Z\\
\dTo<{i_X}	&					&\dTo>{i_Y}	&				&\dTo>{i_Z}\\
C(X)		&\rTo_{\alpha_*}	&C(Y)		&\rTo_{\beta_*}	&C(Z)
\end{diagram}

\item[(iii)] By~(i), applied to~\(\iota_E\).

\item[(iv)] If \(\varphi\)~is linear, then \(\varphi\after\pi_E:C(E)\to F\) is linear and extends~\(\varphi\) in the sense that \(\varphi\after\pi_E\after i_E=\varphi\after\iota_E=\varphi\). However, \(\pi_F\after\varphi_*:C(E)\to F\) is also linear and extends~\(\varphi\) since
\[\pi_F\after\varphi_*\after i_E=\pi_F\after i_F\after\varphi=\iota_F\after\varphi=\varphi\]
By uniqueness, these two maps must be equal. Conversely, if these two maps are equal, then \(\varphi\)~is linear since \(\pi_F\after\varphi_*\)~is linear and \(\pi_E\)~is surjective.

\item[(v)] By~(iii),
\begin{align*}
\pi_E(f_{\lambda a+\mu b}-\lambda f_a-\mu f_b)&=\pi_E(f_{\lambda a+\mu b})-\lambda\pi_E(f_a)-\mu\pi_E(f_b)\\
	&=\lambda a+\mu b-\lambda a-\mu b\\
	&=0
\end{align*}
for all \(a,b\in E\) and \(\lambda,\mu\in\Gamma\). It follows that \(N(E)\subseteq\ker\pi_E\) since \(N(E)\)~is the \emph{smallest} subspace containing these elements and \(\ker\pi_E\)~is a subspace.

On the other hand, it follows from the fact that \(N(E)\)~is a subspace that
\[\sum\lambda_i f_{a_i}-f_{\;\sum\lambda_i a_i}\in N(E)\]
for all (finite) linear combinations. Now if \(g=\sum_{a\in E}g(a)f_a\in\ker\pi_E\), then
\[0=\pi_E(g)=\sum_{a\in E}g(a)\pi_E(f_a)=\sum_{a\in E}g(a)a\]
This implies \(f_{\;\sum_{a\in E}g(a)a}=f_0\in N(E)\). But by the above, \(g-f_0\in N(E)\), so \(g\in N(E)\). Therefore also \(\ker\pi_E\subseteq N(E)\).\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
Note (i)~shows that \(C(X)\)~is a universal (initial) object in the category of ``vector spaces with maps of~\(X\) into them''. In this category, the objects are maps \(X\to F\), for vector spaces~\(F\), and the arrows are \emph{linear} (i.e. structure-preserving) maps \(F\to G\) between the vector spaces which respect the mappings of~\(X\):
\begin{diagram}[nohug]
X	&\rTo	&F\\
	&\rdTo	&\dTo\\
	&		&G
\end{diagram}
By~(i), every object \(X\to F\) in this category can be obtained from the inclusion map \(X\to C(X)\) in a unique way. This is why \(C(X)\) is called ``universal''. This is only possible because \(C(X)\)~is free from any nontrivial relations among the elements of~\(X\), so any relations among the images of those elements in~\(F\) can be obtained starting from~\(C(X)\). This is why \(C(X)\)~is called ``free''. It is immediate from the universal property that \(C(X)\)~is unique up to isomorphism: if \(X\to U\) is also universal, then the composites \(\psi\after\varphi\) and~\(\varphi\after\psi\) of the induced linear maps \(\varphi:C(X)\to U\) and \(\psi:U\to C(X)\) are linear and extend the inclusion maps, so must be the identity maps on \(C(X)\) and~\(U\) by uniqueness; that is, \(\varphi\) and~\(\psi\) are mutually inverse and hence \emph{isomorphisms}. In fact they are also unique by the universal property.

Now (ii)~shows that we have a \emph{functor} from the category of sets into the category of vector spaces, which sends sets \(X\) and~\(Y\) to the vector spaces \(C(X)\) and~\(C(Y)\), and which sends a set map \(\alpha:X\to Y\) to the linear map \(\alpha_*:C(X)\to C(Y)\). The functor preserves the category structure of composites of arrows.

In~(iii), we are ``forgetting'' the linear structure of~\(E\) when forming~\(C(E)\). For example, if \(E=\R^2\), then \((1,1)=(1,0)+(0,1)\) in~\(E\), but \emph{not} in~\(C(E)\). The ``formal'' linear combination
\[(1,1)-(1,0)-(0,1)\]
is not zero in~\(C(E)\) because the pairs are unrelated elements (symbols) which are \emph{linearly independent}. Note \(\pi_E\)~is surjective (since \(\iota_E\)~is), so \(E\)~is a projection of~\(C(E)\). In~(iv), we see that \(\varphi:E\to F\) is linear if and only if it is a ``projection'' of \(\varphi_*:C(E)\to C(F)\).

In~(v), we see that \(\pi_E\)~just recalls the linear structure of~\(E\) that was forgotten in~\(C(E)\). In particular, \(C(E)/N(E)\iso E\). In other words, if you start with~\(E\), then forget about its linear structure, then recall that linear structure, you just get \(E\)~again.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
If
\[E_1\dsum E_2=E=F_1\dsum F_2\]
are two direct sum decompositions of~\(E\) with \(E_i\subseteq F_i\) for \(i=1,2\), then \(E_i=F_i\) for \(i=1,2\).
\end{rmk}
\begin{proof}
If \(x\in F_i\), then \(x=x_1+x_2\) with \(x_j\in E_j\). But then we must have \(x=x_i\) since \(x_j\in F_j\), so \(x\in E_i\).
\end{proof}

\begin{rmk}
If \(E_1\) and~\(E_2\) are subspaces of~\(E\), then
\[\frac{E_1+E_2}{E_1\sect E_2}\ =\ \frac{E_1}{E_1\sect E_2}\dsum\frac{E_2}{E_1\sect E_2}\]
\end{rmk}
\begin{proof}
Let \(E_{12}=E_1\sect E_2\). Viewing both sides of the above equation as subspaces of~\(E/E_{12}\), it is clear that there is a sum since the canonical projection is linear. If \(\proj{x_1}=\proj{x_2}\) with \(x_1\in E_1\) and \(x_2\in E_2\), then
\[\proj{x_1-x_2}=\proj{x_1}-\proj{x_2}=0\]
so \(x_1-x_2\in E_{12}\), which implies that \(x_1=(x_1-x_2)+x_2\in E_{12}\), so \(\proj{x_1}=0\). It follows that \((E_1/E_{12})\sect(E_2/E_{12})=0\), so the sum is direct.
\end{proof}

\subsection*{\S~4}
\begin{exer}[11]
Let \(E\)~be a real vector space and \(E_1\)~a vector hyperplane in~\(E\) (that is, a subspace of codimension~\(1\)). Define an equivalence relation on \(E^1=E-E_1\) as follows: for \(x,y\in E^1\), \(x\equiv y\) if the segment
\[x(t)=(1-t)x+ty\qquad(0\le t\le 1)\]
is disjoint from~\(E_1\). Then there are precisely two equivalence classes.
\end{exer}
\begin{proof}
Fix \(e\in E^1\) with \(E=E_1\dsum\gen{e}\) and define \(\alpha:E\to\R\) by \(x-\alpha(x)e\in E_1\) for all \(x\in E\). It is clear that \(\alpha\)~is linear, and \(x\in E_1\) if and only if \(\alpha(x)=0\). For \(x,y\in E^1\), it follows that \(x\equiv y\) if and only if
\[0\ne\alpha(x(t))=\alpha((1-t)x+ty)=(1-t)\alpha(x)+t\alpha(y)\]
for all \(0\le t\le 1\). But this is just equivalent to \(\alpha(x)\alpha(y)>0\).

Now if \(x\in E^1\), then \(\alpha(x)\ne0\), so \(\alpha(x)^2>0\) and \(x\equiv x\). If \(x\equiv y\), then \(\alpha(y)\alpha(x)=\alpha(x)\alpha(y)>0\), so \(y\equiv x\). If also \(y\equiv z\), then \(\alpha(y)\alpha(z)>0\), so \(\alpha(x)\alpha(z)>0\) and \(x\equiv z\). In other words, this is indeed an equivalence relation.

Note there are at least two equivalence classes since \(\alpha(e)=1\) and \(\alpha(-e)=-1\), so \(\alpha(e)\alpha(-e)=-1<0\) and \(e\not\equiv -e\). On the other hand, there are at most two classes since if \(x\in E^1\), then either \(\alpha(x)>0\) and \(x\equiv e\) or \(\alpha(x)<0\) and \(x\equiv -e\).
\end{proof}
\begin{rmk}
This result shows that the hyperplane separates the vector space into two disjoint half-spaces.
\end{rmk}

\newpage
\section*{Chapter~II}
\subsection*{\S~1}
\begin{rmk}
In subsection~2.4, intuitively the kernel of a linear function is ``large'' because it has codimension at most 1, so the intersection of the kernels of finitely many linear functions is also ``large'' in the sense that the codimension is at most the number of functions. However the dimension need not be high. For example in a finite-dimensional space the intersection of the kernels of the coordinate functions for a basis is zero.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In subsection~2.11, in the second part of the proof of Proposition~I, just let \(\psi:E\from F\) be any linear mapping extending \(\varphi_1^{-1}:E\from\im\varphi\).\footnote{See Corollary~I to Proposition~I in subsection~1.15.}
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The direct sum \(E\dsum F\) is a coproduct in the category of vector spaces in the following sense: if \(\varphi:E\to G\) and \(\psi:F\to G\) are linear maps, there is a unique linear map \(\chi:E\dsum F\to G\) such that \(\varphi=\chi\after i_E\) and \(\psi=\chi\after i_F\), where \(i_E\) and~\(i_F\) are the canonical injections:
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&E\dsum F		&\lTo^{i_F}		&F\\
	&\rdTo<{\varphi}	&\dDashto{\chi}	&\ldTo>{\psi}	&\\
	&					&G				&				&
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x+y)=\varphi(x)+\psi(y)\) for \(x\in E\), \(y\in F\). It is the unique linear map ``extending'' both \(\varphi\) and~\(\psi\). This property makes \(E\dsum F\) unique up to a unique isomorphism.

Dually, \(E\dsum F\) is a product in the following sense: if \(\varphi:G\to E\) and \(\psi:G\to F\) are linear maps, there is a unique linear map \(\chi:G\to E\dsum F\) such that \(\varphi=\pi_E\after\chi\) and \(\psi=\pi_F\after\chi\):
\begin{diagram}[nohug]
	&					&G				&				&\\
	&\ldTo<{\varphi}	&\dDashto{\chi}	&\rdTo>{\psi}	&\\
E	&\lTo^{\pi_E}		&E\dsum F		&\rTo^{\pi_F}	&F
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x)=\varphi(x)+\psi(x)\), and ``combines'' \(\varphi\) and~\(\psi\). This property also makes \(E\dsum F\) unique up to a unique isomorphism. An infinite direct sum is also a coproduct, but \emph{not} a product, essentially because it has no infinite sums of elements.

In the proof of Proposition~I, \(\sigma\)~is the product map and \(\tau\)~is the coproduct map. If \(\varphi_1:E_1\to F_1\) and \(\varphi_2:E_2\to F_2\) are linear maps, then \(\varphi=\varphi_1\dsum\varphi_2\) is both a coproduct and product map:
\begin{diagram}
E_1				&\pile{\rTo\\\lTo}	&E_1\dsum E_2	&\pile{\lTo\\\rTo}	&E_2\\
\dTo<{\varphi_1}&					&\dTo>{\varphi}	&					&\dTo>{\varphi_2}\\
F_1				&\pile{\rTo\\\lTo}	&F_1\dsum F_2	&\pile{\lTo\\\rTo}	&F_2
\end{diagram}
The structure of~\(\varphi\) is completely determined by the structures of \(\varphi_1\) and~\(\varphi_2\). In particular, \(\varphi\)~is injective (surjective, bijective) if and only if \(\varphi_1\) and~\(\varphi_2\) are.
\end{rmk}

\begin{rmk}
If \(E=E_1\dsum E_2\) and \(F=F_1\dsum F_2\), and \(\varphi:E\to F\) is defined by \(\varphi=\varphi_1\dsum\varphi_2\) where \(\varphi_1:E_1\to F_1\) and \(\varphi_2:E_2\to F_2\), then the following diagrams commute:
\begin{diagram}
E_1			&\rTo^{\varphi_1}		&F_1		&&E_2			&\rTo^{\varphi_2}		&F_2\\
\dTo<{\iso}	&						&\dTo>{\iso}&&\dTo<{\iso}	&						&\dTo>{\iso}\\
E/E_2		&\rTo_{\proj{\varphi}}	&F/F_2		&&E/E_1			&\rTo_{\proj{\varphi}}	&F/F_1
\end{diagram}
Here the isomorphisms are those induced by the canonical projections.
\end{rmk}

\begin{rmk}
If \(E_1\) and~\(E_2\) are subspaces of~\(E\), then there is a natural isomorphism\footnote{Compare with the result for internal direct sums in chapter~I, \S~3 above.}
\[\frac{E_1+E_2}{E_1\sect E_2}\ \iso\ \frac{E_1}{E_1\sect E_2}\dsum\frac{E_2}{E_1\sect E_2}\]
\end{rmk}
\begin{proof}
We assume without loss of generality that \(E=E_1+E_2\), and we also write \(E_{12}=E_1\sect E_2\). Then we have the following commutative diagram:
\begin{diagram}
E			&\lTo^{i_1}	&E_1			&\rTo^{i_1}			&E			&\lTo^{i_2}			&E_2			&\rTo^{i_2}	&E\\
\dTo<{\pi_2}&			&\dTo<{\rho_1}	&					&\dTo>{\rho}&					&\dTo>{\rho_2}	&			&\dTo>{\pi_1}\\
E/E_2		&\lTo_{\iso}&E_1/E_{12}		&\rTo_{\proj{i_1}}	&E/E_{12}	&\lTo_{\proj{i_2}}	&E_2/E_{12}		&\rTo_{\iso}&E/E_1
\end{diagram}
The horizontal arrows on top are inclusions, the vertical arrows are canonical projections, and the horizontal arrows on bottom are induced. Note \(\proj{i_1}\) and~\(\proj{i_2}\) are also just inclusions. Define
\[f=\proj{\bigl((\proj{\pi_2 i_1})^{-1}\pi_2,(\proj{\pi_1 i_2})^{-1}\pi_1\bigr)}:E/E_{12}\to(E_1/E_{12})\dsum(E_2/E_{12})\]
where the bars denote induced maps and the parentheses denote pairing, and
\[g=\bigl[\proj{i_1},\proj{i_2}\bigr]:(E_1/E_{12})\dsum(E_2/E_{12})\to E/E_{12}\]
where the brackets denote copairing. It is easy to verify that \(f\after g=\iota\) and \(g\after f=\iota\), so \(f\) and~\(g\) are isomorphisms.
\end{proof}

\begin{rmk}
If \(E=\bigdsum_{i=1}^n E_i\) and \(F=\bigdsum_{j=1}^m F_j\), then there is a natural isomorphism
\[L(E;F)\iso\bigdsum_{i,j}L(E_i;F_j)\]
given by the mapping
\[\varphi\mapsto\sum_{i,j}\pi_j\after\varphi\after i_i\]
where \(i_i:E_i\to E\) is the canonical injection and \(\pi_j:F\to F_j\) is the canonical projection.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
The definition of dual space is fundamentally \emph{symmetrical} between \(E\) and~\(E^*\), as is the definition of dual mapping between \(\varphi\) and~\(\varphi^*\). This symmetry often allows us to use bidirectional reasoning and derive two theorems from one proof. For example, (2.48) actually follows from~(2.47) by symmetry of \(\varphi\) and~\(\varphi^*\). The proof of Proposition~I in subsection~2.23 exploits symmetry, as do other proofs in the book.

Many other books simply \emph{define} the dual space of~\(E\) to be~\(L(E)\) (no doubt in light of Proposition~I of this subsection), at the expense of this symmetry. Such books make use of \emph{reflexivity}.\footnote{See the remark in \S~6 below.}
\end{rmk}

\begin{rmk}
The definition of duality can be generalized to modules. Let \(R\)~be a ring (with unit), \(M\)~a \emph{left} \(R\)-module, and \(N\)~a \emph{right} \(R\)-module. A binary mapping \(\Phi:M\times N\to R\) is \emph{bilinear} if
\[\Phi(\lambda x_1+\mu x_2,y)=\lambda\Phi(x_1,y)+\mu\Phi(x_2,y)\]
and
\[\Phi(x,y_1\lambda+y_2\mu)=\Phi(x,y_1)\lambda+\Phi(x,y_2)\mu\]
for all \(x,x_1,x_2\in M\), \(y,y_1,y_2\in N\) and \(\lambda,\mu\in R\). The map is \emph{nondegenerate} if
\[\{\,x\in M\mid\Phi(x,y)=0\text{ for all }y\in N\,\}=0\]
and
\[\{\,y\in N\mid\Phi(x,y)=0\text{ for all }x\in M\,\}=0\]
Now \(M\)~is \emph{dual} to~\(N\) if there is a nondegenerate bilinear map \(\sprod{-}{-}:M\times N\to R\). Note this relation is not in general symmetric unless \(R\)~is commutative.
\end{rmk}

\begin{rmk}
The results in subsection~2.23 show that quotient spaces are dual to subspaces.
\end{rmk}

\begin{rmk}
If \(E,E^*\) and \(F,F^*\) are pairs of dual spaces and \(\varphi:E\to F\) is linear, then \(\varphi^*:E^*\from F^*\) is dual to~\(\varphi\) if and only if the following diagram commutes:
\begin{diagram}
F^*\times E&\rTo^{\varphi^*\times\iota_E}&E^*\times E\\
\dTo<{\iota_{F^*}\times\varphi}&		&\dTo>{\sprod{\ }{\ }}\\
F^*\times F&\rTo_{\sprod{\ }{\ }}&\Gamma
\end{diagram}
\end{rmk}

\begin{rmk}
Let \(E\)~be a vector space and \((x_{\alpha})_{\alpha\in A}\) be a basis of~\(E\). For each \(x\in E\), write \(x=\sum_{\alpha\in A}f_{\alpha}(x)x_{\alpha}\). Then \(f_{\alpha}\in L(E)\) for each \(\alpha\in A\). The function \(f_{\alpha}\) is called the \emph{\(\alpha\)-th coordinate function} for the basis.

Coordinate functions can be used in an alternative proof of Proposition~IV. If \(E_1\)~is a subspace of~\(E\), let \(B_1\)~be a basis of~\(E_1\) and extend it to a basis \(B\) of~\(E\). For each \(x_{\alpha}\in B-B_1\), we have \(f_{\alpha}\in\oc{E_1}\). If \(x\in\occ{E_1}\), then \(f_{\alpha}(x)=\sprod{f_{\alpha}}{x}=0\) for all such \(\alpha\), so \(x\in E_1\). In other words, \(\occ{E_1}\subseteq E_1\).
\end{rmk}

\begin{rmk}
In the corollary to Proposition~V, for \(f\in L(E)\) let \(f_k=f\after i_k\after \pi_k\) where \(i_k:E_k\to E\) is the \(k\)-th canonical injection and \(\pi_k:E\to E_k\) is the \(k\)-th canonical projection. Then \(f=\sum_k f_k\) and \(f_k\in\oc{F_k}\) for all~\(k\), so \(L(E)=\sum_k\oc{F_k}\). The sum is direct since if \(f\in\oc{F_k}\sect\sum_{j\ne k}\oc{F_j}\), then \(f\)~kills \(\sum_{j\ne k}E_j\) and~\(E_k\), so \(f=0\). A scalar product is induced between \(E_k,\oc{F_k}\) since \(E_k\sect F_k=0\) and \(\oc{F_k}\sect\oc{E_k}=0\).\footnote{See subsection~2.23.} The induced injection \(\oc{F_k}\to L(E_k)\) is surjective since every linear function on~\(E_k\) can be extended to a linear function on~\(E\) which kills~\(F_k\).
\end{rmk}

\begin{rmk}
Let \(E,E^*\) be a pair of dual finite-dimensional vector spaces. If \(F\)~is a subspace of~\(E\), then Propositions V and~VI together show that a subspace~\(H\) of~\(E\) is supplementary\footnote{Greub says \emph{complementary}, but this risks confusion with the orthogonal complement.} to~\(F\) in~\(E\) if and only if \(H\)~is dual to~\(\oc{F}\) in~\(E^*\). In other words, \emph{the supplements of a subspace are the duals of its complement}, or equivalently, \emph{the duals of a subspace are the supplements of its complement}.
\end{rmk}

\begin{rmk}
For \(\varphi:E\to F\) a linear map, let \(L(\varphi):L(E)\from L(F)\) be the dual map given by \(L(\varphi)(f)=f\after\varphi\) (2.50). Then \(L\)~linearly embeds \(L(E;F)\) in \(L(L(F);L(E))\), by (2.43) and~(2.44). Also, \(L(\psi\after\varphi)=L(\varphi)\after L(\psi)\) and \(L(\iota_{E})=\iota_{L(E)}\). This shows that \(L\)~is a contravariant functor in the category of vector spaces. This functor preserves exactness of sequences (see~2.29), and finite direct sums, which are just (co)products in the category (see~2.30), among other things.
\end{rmk}

\begin{exer}[10]
If \(\varphi:E\to F\) is a linear map with restriction \(\varphi_1:E_1\to F_1\) and dual map \(\varphi^*:E^*\from F^*\), then \(\varphi^*\)~can be restricted to \(\oc{F_1},\oc{E_1}\) and the induced map \(\proj{\varphi^*}:E^*/\oc{E_1}\from F^*/\oc{F_1}\) is dual to~\(\varphi_1\).
\end{exer}
\begin{proof}
If \(y^*\in\oc{F_1}\) and \(x\in E_1\), then
\[\sprod{\varphi^*y^*}{x}=\sprod{y^*}{\varphi x}=0\]
so \(\varphi^*\)~maps \(\oc{F_1}\) into~\(\oc{E_1}\). We know that the pairs \(E_1,E^*/\oc{E_1}\) and \(F_1,F^*/\oc{F_1}\) are dual under the induced scalar products. For \(\proj{y^*}\in F^*/\oc{F_1}\) and \(x\in E_1\),
\begin{align*}
\sprod{\proj{\varphi^*}\ \proj{y^*}}{x}&=\sprod{\proj{\varphi^* y^*}}{x}\\
	&=\sprod{\varphi^* y^*}{x}\\
	&=\sprod{y^*}{\varphi_1 x}\\
	&=\sprod{\proj{y^*}}{\varphi_1 x}\qedhere
\end{align*}
\end{proof}
\begin{rmk}
This result shows that quotient maps are dual to restriction maps. The examples in subsections 2.24 and~2.27 are special cases.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(E\)~is finite-dimensional, then since \(E\)~is dual to~\(L(E)\) and \(L(E)\)~is dual to~\(L(L(E))\), it follows from Corollary~II that there is a natural isomorphism \(E\to L(L(E))\) given by
\[x\mapsto(f\mapsto f(x))\]
This is called \emph{reflexivity}, and means that the vector~\(x\) may be naturally \emph{identified} with the linear function ``evaluation at~\(x\)''.
\end{rmk}

\begin{rmk}
If \(E\)~is finite-dimensional, then every basis of a dual space~\(E^*\) is a dual basis. Indeed, if \(f_1,\ldots,f_n\) is a basis of~\(E^*\), let \(e_1,\ldots,e_n\) be its dual basis in~\(E\). Then \(\sprod{f_i}{e_j}=\delta_{ij}\) by~(2.62), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\), again by~(2.62).

Alternatively, with \(E^*=L(E)\), let \(f_1^*,\ldots,f_n^*\) be the dual basis of \(f_1,\ldots,f_n\) in \(E^{**}=L(L(E))\), so \(\sprod{f_j^*}{f_i}=\delta_{ij}\) by~(2.62). Let \(e_1,\ldots,e_n\in E\) be defined by \(\sprod{f_j^*}{f}=\sprod{f}{e_j}\) for all \(f\in E^*\).\footnote{See problem~3 in \S~5 or the prior remark.} Then \(\sprod{f_i}{e_j}=\sprod{f_j^*}{f_i}=\delta_{ij}\), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\).

The first proof here uses the symmetry between \(E\) and~\(E^*\), while the second uses reflexivity. In either case, we see that the relationship between dual bases is fundamentally symmetrical.
\end{rmk}

\begin{rmk}
In Proposition~III,
\[\Phi(x^*,x)=\sprod{x^*}{\varphi x}=\sprod{\varphi^* x^*}{x}\]
so there is also a natural isomorphism \(B(E^*,E)\iso L(E^*;E^*)\). We see that \(\varphi^*\) is ``left-dual'' to~\(\Phi\) while \(\varphi\)~is ``right-dual'' to~\(\Phi\). It might be said that \(\Phi\), \(\varphi\), \(\varphi^*\) enter into a holy trinity.\footnote{No one says this.}
\end{rmk}

\begin{exer}[9]
If \(E\) and~\(F\) are finite-dimensional, then the mapping
\[\Phi:L(E;F)\to L(F^*;E^*)\]
defined by \(\varphi\mapsto\varphi^*\) is a linear isomorphism.
\end{exer}
\begin{proof}
By the remark in~\S~5 above, and the fact that \(\varphi^{**}=\varphi\).
\end{proof}

\newpage
\section*{Chapter~III}
\begin{warn}
Greub's notational choices in this chapter are insane. In particular, although he uses left-hand mapping notation (writing \(\varphi x\) instead of~\(x\varphi\), and \(\varphi\psi\) to mean \emph{\(\varphi\) after \(\psi\)}), and follows the usual ``row-by-column'' convention for matrix multiplication, his convention for the matrix of a linear mapping is the transpose of that normally used in these circumstances. This has the following unfortunate consequences:
\begin{itemize}
\item The matrix of the linear mapping naturally associated with a system of linear equations has the coefficients from each equation appear \emph{vertically in columns}.
\item If \(M(x)\)~is the \emph{column vector} representing~\(x\), then \(M(\varphi x)=M(\varphi)^*M(x)\), and if \(M(x)\)~is the \emph{row vector} representing~\(x\), then \(M(\varphi x)=M(x)M(\varphi)\).
\item \(M(\varphi\psi)=M(\psi)M(\varphi)\)
\end{itemize}
Compounding the insanity, Greub (inspired by tensor notation) indexes over columns instead of rows when working in dual spaces. This further increases the risk of confusion and error, as we see below. Greub says that ``it would be very undesirable\dots to agree once and for all to always let the subscript count the rows'', but we couldn't disagree more in this context.
\end{warn}

\subsection*{\S~3}
\begin{rmk}
In subsection~3.13, although \(\beta^{\varrho}_{\nu}=\check{\alpha}_{\nu}^{\varrho}\), we must remember that \(\nu\)~indexes the \emph{columns} of the matrix of the dual basis transformation \(x^{*\nu}\mapsto\bar{x}^{*\nu}\) by~(3.24), whereas \(\nu\)~indexes the \emph{rows} of the matrix of the basis transformation \(x_{\nu}\mapsto\bar{x}_{\nu}\) by~(3.22). In other words, the matrix of the dual basis transformation \(x^{*\nu}\mapsto\bar{x}^{*\nu}\) is the \emph{transpose} of the inverse of the matrix of the basis transformation \(x_{\nu}\mapsto\bar{x}_{\nu}\), contrary to what the book says.\footnote{Compare this to the proof of equation~(3.4) in subsection~3.3.} It's easier to remember that the matrix of \(x^{*\nu}\mapsfrom\bar{x}^{*\nu}\) (arrow reversed!) is the transpose of the matrix of \(x_{\nu}\mapsto\bar{x}_{\nu}\).
\end{rmk}

\begin{rmk}
In subsection 3.13, we see that if a basis transformation is effected by~\(\tau\), then the corresponding \emph{coordinate} transformation is effected by~\(\tau^{-1}\). The coordinates of a vector are transformed ``exactly in the same way'' as the vectors of the dual basis, despite the previous remark, because of Greub's notational choices.
\end{rmk}

\newpage
\section*{Chapter~IV}
\begin{rmk}
In this chapter, it is implicitly assumed that all vector spaces have dimension \(n\ge 1\), except in the definition of intersection number (subsection 4.31) where \(n=0\). Here we summarize results for the case \(n=0\):
\begin{itemize}
\item For a set~\(X\), \(X^0=\{\emptyset\}\). Therefore maps \(\Phi:X^0\to Y\) can be identified with elements of~\(Y\).
\item For vector spaces \(E\) and~\(F\), a map \(\Phi:E^0\to F\) is vacuously \(0\)-linear. Since the only permutation in~\(S_0\) is the identity \(\iota=\emptyset\), \(\Phi\)~is also trivially skew symmetric.
\end{itemize}
In particular if \(E=0\), the following results hold:
\begin{itemize}
\item Determinant functions in~\(E\) are just scalars in~\(\Gamma\), and dual determinant functions are just reciprocal scalars.
\item The only transformation of~\(E\) is the zero transformation, which is also the identity transformation. It has determinant~\(1\), trace~\(0\), and constant characteristic polynomial~\(1\). It has no eigenvalues or eigenvectors. Its adjoint is also the zero transformation. Its matrix on the empty basis is empty.
\item If \(E\)~is real (\(\Gamma=\R\)), the orientations in~\(E\) are represented by the scalars~\(\pm 1\), and determine whether the empty basis is positive or negative. The zero transformation is orientation preserving. The empty basis is deformable into itself.
\end{itemize}
\end{rmk}

\subsection*{\S~1}
\begin{rmk} To see why~(4.1) holds, observe by definition of~\(\tau(\sigma\Phi)\) that
\[(\tau(\sigma\Phi))(x_1,\ldots,x_p)=(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})\]
Let \(y_i=x_{\tau(i)}\). Then by definition of \(\sigma\Phi\) and~\((\tau\sigma)\Phi\),
\begin{align*}
(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})&=(\sigma\Phi)(y_1,\ldots,y_p)\\
	&=\Phi(y_{\sigma(1)},\ldots,y_{\sigma(p)})\\
	&=\Phi(x_{\tau(\sigma(1))},\ldots,x_{\tau(\sigma(p))})\\
	&=\Phi(x_{(\tau\sigma)(1)},\ldots,x_{(\tau\sigma)(p)})\\
	&=((\tau\sigma)\Phi)(x_1,\ldots,x_p)
\end{align*}
Therefore \(\tau(\sigma\Phi)=(\tau\sigma)\Phi\).
\end{rmk}

\begin{rmk}
By Proposition~I(iii) and Proposition~II, a determinant function \(\Delta\ne 0\) ``determines'' linear independence in the sense that \(\Delta(x_1,\ldots,x_n)\ne0\) if and only if \(x_1,\ldots,x_n\) are linearly independent. By~(4.8), it follows that \(\det\varphi\) ``determines'' whether a linear transformation~\(\varphi\) preserves linear independence, i.e. whether or not \(\varphi\)~is invertible.

Geometrically, \(\Delta(x_1,\ldots,x_n)\) measures the oriented (signed) volume of the \(n\)-dimensional parallelepiped determined by the vectors \(x_1,\ldots,x_n\). Therefore \(\det\varphi\) is the factor by which \(\varphi\)~changes oriented volume. Since a small change in the vectors \(x_1,\ldots,x_n\) results in a small change in the oriented volume, \(\Delta\)~is continuous.
\end{rmk}

\begin{rmk}
We provide an alternative proof of Proposition~IV. First note
\[(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)=\Delta(x_1,\ldots,x,\ldots,x_n)\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{\(\delete{x_j}\)~denotes deletion of \(x_j\)~from the sequence on the left.} Therefore
\[\sum_{j=1}^n(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)x_j=\Delta(x,x_2,\ldots,x_n)x_1+\cdots+\Delta(x_1,\ldots,x_{n-1},x)x_n\]
Viewing this as a function of \(x_1,\ldots,x_n\) (that is, a set map from \(E^n\to L(E;E)\)), it is obviously multilinear and skew symmetric (by Proposition~I(ii)). Therefore if \(x_1,\ldots,x_n\) are linearly dependent, it is zero and hence equal to \(\Delta(x_1,\ldots,x_n)x\) (by Proposition~I(iii)). On the other hand if \(x_1,\ldots,x_n\) are linearly independent (and hence a basis), then viewing it as a function of~\(x\), its value at~\(x_i\) is just \(\Delta(x_1,\ldots,x_n)x_i\) (by Proposition~I(ii)), so it agrees on a basis with \(\Delta(x_1,\ldots,x_n)x\) and hence is equal to it.\footnote{See also the remark in chapter~5, \S~7 of~\cite{greub2} below.} Note this result shows that \(\ad\iota=\iota\).
\end{rmk}

\begin{rmk}
Let \(E\)~be a vector space with \(\dim E=n>1\) and \(E_1\)~a subspace with \(\dim E_1=1\). Let \(\Delta\)~be a determinant function in~\(E\) with \(\Delta(e_1,\ldots,e_n)=1\) where \(e_1\in E_1\). Then \(\Delta\)~induces a determinant function~\(\Delta_1\) in~\(E/E_1\) by
\[\Delta_1(\,\proj{x_2},\ldots,\proj{x_n}\,)=\Delta(e_1,x_2,\ldots,x_n)\]
with \(\Delta_1(\,\proj{e_2},\ldots,\proj{e_n}\,)=1\). Define \(D:E^n\to\Gamma\) by
\[D(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{j-1}\Delta_1(\,\proj{x_1},\ldots,\widehat{\proj{x_j}},\ldots,\proj{x_n}\,)\mult\pi_1(x_j)\]
where \(\pi_1:E\to\Gamma\) is the coordinate function for~\(e_1\). Then \(D\)~is skew symmetric and \(n\)-linear with \(D(e_1,\ldots,e_n)=1\), so \(D=\Delta\) by uniqueness (Proposition~III). Therefore
\[\boxed{\Delta(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{j-1}\Delta_1(\,\proj{x_1},\ldots,\widehat{\proj{x_j}},\ldots,\proj{x_n}\,)\mult\pi_1(x_j)}\]
This result, which can also be obtained by applying~\(\pi_1\) to both sides of~(4.6) with \(x=e_1\), expresses a fundamental relationship between an \(n\)-dimensional determinant function and an \((n-1)\)-dimensional one. The cofactor expansion formulas for the determinant (subsection 4.15) follow immediately.

Note that this relationship can also be exploited in reverse to \emph{define} an \(n\)-dimensional determinant function in terms of an \((n-1)\)-dimensional one. In fact, if \(\Phi:E^{n-1}\to\Gamma\) is a skew symmetric \((n-1)\)-linear function in~\(E\), then there is a vector \(a\in E\) such that
\[\Phi(x_1,\ldots,x_{n-1})=\Delta(x_1,\ldots,x_{n-1},a)\tag{1}\]
To see this, define \(\varphi:E^n\to E\) by
\[\varphi(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{n-j}\Phi(x_1,\ldots,\widehat{x_j},\ldots,x_n)\mult x_j\]
Then \(\varphi\)~is skew symmetric and \(n\)-linear, so \(\varphi=\Delta a\) where \(a=\varphi(e_1,\ldots,e_n)\). This means
\[\Delta(x_1,\ldots,x_n)a=\sum_{j=1}^n (-1)^{n-j}\Phi(x_1,\ldots,\widehat{x_j},\ldots,x_n)\mult x_j\tag{2}\]
If \(x_1,\ldots,x_{n-1}\) are linearly dependent, then (1)~holds trivially, so we may assume that they are linearly independent. If \(a\)~is not in their span, then (1)~follows from~(2) with \(x_n=a\); if \(a\)~is in the span, then (1)~follows from~(2) with \(x_n\) chosen so that \(\Delta(x_1,\ldots,x_n)=1\).

Geometrically, this result expresses a relationship between \(n\)-dimensional volume and \((n-1)\)-dimensional volume. We also see from~(1) that the latter is just the former measured \emph{relative to a fixed vector}. The fact that the volume of an \(n\)-dimensional parallelepiped is equal to the product of the volume of any \((n-1)\)-dimensional ``base'' and the corresponding ``height'' (subsection 7.15) is a special case.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In subsection~4.6, we want a transformation~\(\psi\) with \(\psi\varphi=(\det\varphi)\iota\). We can choose a basis \(x_1,\ldots,x_n\) in~\(E\) with \(\Delta(x_1,\ldots,x_n)=1\), for which we want
\begin{align*}
\psi(\varphi x_i)=(\psi\varphi)x_i&=(\det\varphi)x_i\\
	&=(\det\varphi)\Delta(x_1,\ldots,x_n)x_i\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_n)x_i
\end{align*}
To obtain this, we can define
\[\psi(x)=\sum_{j=1}^n\Delta(\varphi x_1,\ldots,x,\ldots,\varphi x_n)x_j\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{See the remark on Proposition~IV above.} Then \(\psi\)~obviously satisfies the above properties, by multilinearity and skew symmetry of~\(\Delta\).

To obtain~\(\psi\) in a ``coordinate-free''  manner (without choosing a basis), we observe that the construction on the right is multilinear and skew symmetric in \(x_1,\ldots,x_n\) when viewed as a mapping \(\Phi:E^n\to L(E;E)\). By the universal property of~\(\Delta\) (Proposition~III), there is a unique \(\psi\in L(E;E)\) satisfying the above; this~\(\psi\) is also seen to be independent of the choice of~\(\Delta\).
\end{rmk}

\begin{rmk}
Alternatively, fix a vector~\(x\) and define \(\Phi:E^{n-1}\to\Gamma\) by
\[\Phi(x_1,\ldots,x_{n-1})=\Delta(\varphi x_1,\ldots,\varphi x_{n-1},x)\]
By a remark above, there is a unique vector~\(x'\) with
\[\Phi(x_1,\ldots,x_{n-1})=\Delta(x_1,\ldots,x_{n-1},x')\]
for all vectors \(x_1,\ldots,x_{n-1}\). Writing \(x'=\psi x\), we obtain
\[\Delta(\varphi x_1,\ldots,\varphi x_{n-1},x_n)=\Delta(x_1,\ldots, x_{n-1},\psi x_n)\tag{1}\]
for all vectors \(x_1,\ldots,x_n\). It follows from~(1) that \(\psi:E\to E\) is linear and uniquely determined by~\(\varphi\). It is easy to verify that \(\psi=\ad\varphi\).
\end{rmk}

\begin{rmk}
In subsection~4.7, observe that
\[\Delta(x_1,\ldots,x_p,y_1,\ldots,y_q)\]
induces a determinant function in~\(E_2\) when \(x_1,\ldots,x_p\in E\) are fixed, and induces a determinant function in~\(E_1\) when \(y_1,\ldots,y_q\in E\) are fixed. Now let \(a_1,\ldots,a_p\) be a basis of~\(E_1\), so \(a_1,\ldots,a_p,b_1,\ldots,b_q\) is a basis of~\(E\). Then by~(4.8),
\begin{align*}
\det\varphi\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)&=\Delta(\varphi_1 a_1,\ldots,\varphi_1 a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\Delta(a_1,\ldots,a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\det\varphi_2\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)
\end{align*}
Since \(\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)\ne0\), it follows that \(\det\varphi=\det\varphi_1\mult\det\varphi_2\). Note this result shows that
\[\det(\varphi_1\dsum\varphi_2)=\det\varphi_1\mult\det\varphi_2\]
\end{rmk}

\begin{exer}[2]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\det\varphi=\det\varphi_1\mult\det\proj{\varphi}\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) where \(e_1,\ldots,e_p\) is a basis of~\(E_1\). Let \(\Delta\ne 0\) be a determinant function in~\(E\). First observe that
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,\varphi e_{p+1},\ldots,\varphi e_n)\tag{1}\]
is a determinant function in~\(E_1\) and
\[\Delta_2(\,\proj{x_{p+1}},\ldots,\proj{x_n}\,)=\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)\tag{2}\]
is a well-defined determinant function in~\(E/E_1\). Now
\[\det\proj{\varphi}\mult\Delta_2(\,\proj{x_{p+1}},\ldots,\proj{x_n}\,)=\Delta_2(\,\proj{\varphi}\ \proj{x_{p+1}},\ldots,\proj{\varphi}\ \proj{x_n}\,)=\Delta_2(\,\proj{\varphi x_{p+1}},\ldots,\proj{\varphi x_n}\,)\tag{3}\]
It follows from (2) and~(3) that
\[\det\proj{\varphi}\mult\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)=\Delta(e_1,\ldots,e_p,\varphi x_{p+1},\ldots,\varphi x_n)\tag{4}\]
Now
\begin{align*}
\det\varphi\mult\Delta(e_1,\ldots,e_n)&=\Delta(\varphi e_1,\ldots,\varphi e_n)&&\\
	&=\Delta_1(\varphi_1 e_1,\ldots,\varphi_1 e_p)&&\text{by~(1)}\\
	&=\det\varphi_1\mult\Delta_1(e_1,\ldots,e_p)&&\\
	&=\det\varphi_1\mult\det\proj{\varphi}\mult\Delta(e_1,\ldots,e_n)&&\text{by (1), (4)}
\end{align*}
Since \(\Delta(e_1,\ldots,e_n)\ne 0\), the result follows.
\end{proof}
\begin{rmk}
It follows that if \(A\)~is an \(n\times n\) matrix of the form
\[A=\begin{pmatrix}
A_1&\\
*&A_2
\end{pmatrix}\]
where \(A_1\)~is \(p\times p\) and \(A_2\)~is \((n-p)\times(n-p)\), then
\[\det A=\det A_1\mult\det A_2\]
Indeed, let \(E\)~be an \(n\)-dimensional vector space and \(\varphi:E\to E\) be defined by \(M(\varphi;e_1,\ldots,e_n)=A\), so \(\det A=\det\varphi\). Let \(E_1=\gen{e_1,\ldots,e_p}\) and \(E_2=\gen{e_{p+1},\ldots,e_n}\). Then \(E=E_1\dsum E_2\) and \(E_1\)~is stable under~\(\varphi\). If \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced maps, then \(A_1=M(\varphi_1)\) and \(A_2=M(\proj{\varphi})\), so \(\det A_1=\det\varphi_1\) and \(\det A_2=\det\proj{\varphi}\). The result now follows from the problem.
\end{rmk}

\begin{exer}[7]\
\begin{enumerate}
\item[(i)] \(\ad(\psi\varphi)=\ad(\varphi)\ad(\psi)\)
\item[(ii)] \(\det\ad\varphi=(\det\varphi)^{n-1}\)
\end{enumerate}
\end{exer}
\begin{proof}
For~(i), by a remark above we have for all vectors \(x_1,\ldots,x_n\)
\begin{align*}
\Delta(x_1,\ldots,x_{n-1},\ad(\psi\varphi)x_n)&=\Delta(\psi\varphi x_1,\ldots,\psi\varphi x_{n-1},x_n)\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_{n-1},\ad(\psi)x_n)\\
	&=\Delta(x_1,\ldots,x_{n-1},\ad(\varphi)\ad(\psi)x_n)
\end{align*}
For~(ii), since \(\ad(\varphi)\varphi=(\det\varphi)\iota\), we have
\[\det\ad\varphi\mult\det\varphi=\det(\ad(\varphi)\varphi)=\det((\det\varphi)\iota)=(\det\varphi)^n\]
If \(\det\varphi\ne0\), then the result follows, so suppose \(\det\varphi=0\). If \(\varphi=0\), then \(\ad\varphi=0\) and the result holds. If \(\varphi\ne 0\), then since \(\ad(\varphi)\varphi=0\), we must have \(\det\ad\varphi=0\) and the result holds.
\end{proof}

\subsection*{\S~5}
\begin{rmk}
Recall that the system (4.39) is equivalent to \(\varphi x=y\) where \(\varphi:\Gamma^n\to\Gamma^n\) is defined by \(M(\varphi)=(\alpha^j_k)=A\), \(x=(\xi^i)\), and \(y=(\eta^j)\). If \(\det A\ne0\), then \(\varphi\)~is invertible and
\[x=\varphi^{-1}y=\frac{1}{\det A}\ad(\varphi)(y)\]
It follows from the analysis of the adjoint matrix in subsection~4.13 that
\[\xi^i=\frac{1}{\det A}\sum_j\cof(\alpha^j_i)\eta^j\]
Moreover, it follows from~(4.38) that \(\sum_j\cof(\alpha^j_i)\eta^j=\det A_i\) where \(A_i\)~is the matrix obtained from~\(A\) by replacing the \(i\)-th row with~\(y\).\footnote{The cofactors of \(A_i\) and~\(A\) along the \(i\)-th row agree since \(A_i\) and~\(A\) agree on the other rows.} Therefore
\[\xi^i=\frac{\det A_i}{\det A}\]
\end{rmk}

\begin{rmk}
In subsection~4.14, \(\det B^j_i=\det S^j_i\) does \emph{not} follow from~(4.38), which only tells us that \(\det B^j_i=\det B^j_i\). However, it follows from~(4.16), or from our remarks in~\S~4 above.
\end{rmk}

\subsection*{\S~6}
\begin{exer}[5]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\chi_{\varphi_1\dsum\varphi_2}=\chi_{\varphi_1}\chi_{\varphi_2}\]
where \(\chi_{\varphi}\)~denotes the characteristic polynomial of~\(\varphi\).
\end{exer}
\begin{proof}
This follows from the result in subsection~4.7 and the fact that
\[\varphi_1\dsum\varphi_2-\lambda\iota=(\varphi_1-\lambda\iota_{E_1})\dsum(\varphi_2-\lambda\iota_{E_2})\qedhere\]
\end{proof}

\begin{exer}[6]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\chi_{\varphi}=\chi_{\varphi_1}\chi_{\proj{\varphi}}\]
\end{exer}
\begin{proof}
This follows from problem~2 in \S~2, the fact that \(\varphi-\lambda\iota_E\) restricted to~\(E_1\) is just \(\varphi_1-\lambda\iota_{E_1}\), and \(\proj{\varphi-\lambda\iota_E}=\proj{\varphi}-\lambda\iota_{E/E_1}\).
\end{proof}
\begin{rmk}
Taking \(E_1=\ker\varphi\), we have \(\chi_{\varphi_1}=\chi_{0_{E_1}}=(-\lambda)^p\) where \(p=\dim E_1\), so \(\chi_{\varphi}=(-\lambda)^p\chi_{\proj{\varphi}}\).
\end{rmk}

\begin{exer}[7]
A linear map \(\varphi:E\to E\) is nilpotent if and only if \(\chi_{\varphi}=(-\lambda)^n\).
\end{exer}
\begin{proof}
If \(\varphi\)~is nilpotent, we proceed by induction on~\(k\) least such that \(\varphi^k=0\). If \(k=1\), the result is trivial. If \(k>1\), let \(E_1=\ker\varphi\) and \(\proj{\varphi}:E/E_1\to E/E_1\) the induced map. Then \(\proj{\varphi}^{k-1}=0\) since
\[\proj{\varphi}^{k-1}(\proj{x})=\proj{\varphi^{k-1}}(\proj{x})=\proj{\varphi^{k-1}(x)}=0\]
as \(\varphi^{k-1}(x)\in E_1\). By the induction hypothesis, \(\chi_{\proj{\varphi}}=(-\lambda)^{n-p}\) where \(p=\dim E_1\), so by the previous problem,
\[\chi_{\varphi}=(-\lambda)^p(-\lambda)^{n-p}=(-\lambda)^n\]
Conversely, if \(\varphi\ne0\) and \(\chi_{\varphi}=(-\lambda)^n\), then the constant term \(\det\varphi=0\), so \(p>0\) and by the previous problem \((-\lambda)^n=(-\lambda)^p\chi_{\proj{\varphi}}\), which implies \(\chi_{\proj{\varphi}}=(-\lambda)^{n-p}\). By induction, \(\proj{\varphi}\)~is nilpotent. If \(\proj{\varphi}^k=0\), then \(\varphi^{k+1}=0\), so \(\varphi\)~is nilpotent.
\end{proof}

\subsection*{\S~7}
\begin{rmk}
It follows by symmetry from~(4.61) that
\[\tr\varphi^*=\sum_i\sprod{\varphi^* e^{*i}}{e_i}=\sum_i\sprod{e^{*i}}{\varphi e_i}=\tr\varphi\]
where \(\varphi^*\)~is dual to~\(\varphi\).
\end{rmk}

\begin{exer}[5]
If \(f:L(E;E)\to\Gamma\) is linear, there is \(\alpha\in L(E;E)\) unique with
\[f(\varphi)=\tr(\varphi\after\alpha)\]
\end{exer}
\begin{proof}
Since \(L(E;E)\)~is finite-dimensional and dual to itself under the scalar product
\[\sprod{\varphi}{\psi}=\tr(\psi\after\varphi)\]
it follows from Proposition~I in chapter~II, \S~6 that there is \(\alpha\)~unique with
\[f(\varphi)=\sprod{\alpha}{\varphi}=\tr(\varphi\after\alpha)\qedhere\]
\end{proof}

\begin{exer}[6]
If \(f:L(E;E)\to\Gamma\) is linear with
\[f(\psi\after\varphi)=f(\varphi\after\psi)\]
there is \(\lambda\in\Gamma\) with
\[f(\varphi)=\lambda\mult\tr\varphi\]
\end{exer}
\begin{proof}
By problem~5, there is~\(\alpha\) with
\[f(\varphi)=\tr(\varphi\after\alpha)=\tr(\alpha\after\varphi)\]
We claim \(\alpha=\lambda\iota\) for some \(\lambda\in\Gamma\), from which the result follows. By assumption,
\[\tr(\alpha\after\varphi\after\psi)=\tr(\alpha\after\psi\after\varphi)=\tr(\varphi\after\alpha\after\psi)\tag{1}\]
for all \(\varphi\) and~\(\psi\). Fix a basis of~\(E\). Let \(A=M(\alpha)\), let \(R_l\)~be the matrix with \(1\)'s~in the \(l\)-th row and \(0\)'s elsewhere, and let \(C_k\)~be the matrix with \(1\)'s~in the \(k\)-th column and \(0\)'s elsewhere. By direct computation,
\[(C_kR_lA)_{ij}=0\qquad(k\ne l)\]
while
\[(C_kAR_l)_{ij}=A_{kl}\]
It follows from~(1) with \(R_l=M(\varphi)\) and \(C_k=M(\psi)\) that \(A\)~is diagonal. Now let \(T_{kl}\)~be the matrix with \(1\)'s~in entries \((k,l)\) and~\((l,k)\) and \(0\)'s elsewhere. It follows from~(1) with \(T_{kl}=M(\varphi)\) and \(C_k=M(\psi)\) that
\[A_{ll}=\tr(C_lA)=\tr(C_kT_{kl}A)=\tr(C_kAT_{kl})=A_{kk}\]
Therefore the claim holds with \(\lambda=A_{11}\).
\end{proof}

\begin{exer}[10]
If \(A:L(E;E)\to L(E;E)\) is linear and ``functorial'' in that
\[A(\varphi\after\psi)=A(\varphi)\after A(\psi)\qquad\text{and}\qquad A(\iota)=\iota\]
then \(\tr A(\varphi)=\tr\varphi\).
\end{exer}
\begin{proof}
The function \(\tr\after A:L(E;E)\to\Gamma\) is linear and
\[(\tr\after A)(\varphi\after\psi)=\tr(A(\varphi)\after A(\psi))=\tr(A(\psi)\after A(\varphi))=(\tr\after A)(\psi\after\varphi)\]
so \(\tr\after A=\lambda\mult\tr\) for some \(\lambda\in\Gamma\) by problem~6. But this implies
\[\lambda\mult\tr\iota=(\tr\after A)(\iota)=\tr\iota\]
so \(\lambda=1\) and \(\tr\after A=\tr\).
\end{proof}

\begin{exer}[12]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\tr(\varphi_1\dsum\varphi_2)=\tr\varphi_1+\tr\varphi_2\]
\end{exer}
\begin{proof}
Immediate since
\[M(\varphi_1\dsum\varphi_2)=\begin{pmatrix}
M(\varphi_1)&\\
&M(\varphi_2)
\end{pmatrix}\qedhere\]
\end{proof}

\subsection*{\S~8}
\begin{rmk}
In~(4.68), if instead we define
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,e_{p+1},\ldots,e_n)\]
then \(\Delta_1\)~represents the original orientation in~\(E_1\). Indeed, in this case
\[\Delta_1(e_1,\ldots,e_p)=\Delta(e_1,\ldots,e_p,e_{p+1},\ldots,e_n)=\Delta_2(e_{p+1},\ldots,e_n)>0\]
\end{rmk}

\newpage
\section*{Chapter~V}
\subsection*{\S~1}
\begin{rmk}
An algebra~\(A\) is a \emph{zero algebra} if \(xy=0\) for all \(x,y\in A\); this is equivalent to \(A^2=0\). As an example, \emph{the zero algebra} is the algebra \(A=0\). A zero algebra is unital if and only if it is the zero algebra.
\end{rmk}

\begin{rmk}
Let \(\varphi:A\to B\) be a homomorphism of algebras. If \(A_1\)~is a subalgebra of~\(A\) and \(B_1\)~is a subalgebra of~\(B\) and \(\varphi(A_1)\subseteq B_1\), then the restriction \(\varphi_1:A_1\to B_1\) of~\(\varphi\) to \(A_1,B_1\) is a homomorphism.

If \(A_1\) and~\(B_1\) are \emph{ideals}, then the induced linear map \(\proj{\varphi}:A/A_1\to B/B_1\) is also a homomorphism since
\[\proj{\varphi}(\proj{x}\,\proj{y})=\proj{\varphi}(\proj{xy})=\proj{\varphi(xy)}=\proj{\varphi(x)\varphi(y)}=\proj{\varphi(x)}\,\proj{\varphi(y)}=\proj{\varphi}(\proj{x})\,\proj{\varphi}(\proj{y})\]
\end{rmk}

\noindent In the problems below, \(E\)~is a finite-dimensional vector space.
\begin{exer}[12]
The mapping
\[\Phi:A(E;E)\to\opp{A(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
\(\Phi\)~is a linear isomorphism by problem 9 of chapter II, \S~6, and preserves products since \((\varphi\after\psi)^*=\psi^*\after\varphi^*\).
\end{proof}

\begin{exer}[16]
Every algebra automorphism \(\Phi:A(E;E)\to A(E;E)\) is an \emph{inner} automorphism; that is, there exists \(\alpha\in GL(E)\) such that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{exer}
\begin{proof}
First, observe that every basis~\((e_i)\) of~\(E\) induces a basis~\((\varphi_{ij})\) of~\(A(E;E)\) defined by \(\varphi_{ij}(e_k)=\delta_{jk}e_i\). This basis satisfies
\[\varphi_{ij}\varphi_{lk}=\delta_{jl}\varphi_{ik}\qquad\text{and}\qquad\sum_i\varphi_{ii}=\iota\tag{1}\]
Conversely, every basis satisfying these properties is induced by a basis of~\(E\) in this manner (see problem~14). Moreover, any two of these bases are conjugate to each other via the change of basis transformation between their inducing bases of~\(E\) (see problem~15).

Now fix \((e_i)\) and~\((\varphi_{ij})\) as above. Since \(\Phi\)~is an automorphism, \((\Phi(\varphi_{ij}))\)~is also a basis of~\(A(E;E)\) which satisfies~(1), so there is \(\alpha\in GL(E)\) with \(\Phi(\varphi_{ij})=\alpha\varphi_{ij}\alpha^{-1}\) for all \(i,j\). It follows that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{proof}
\begin{rmk}
The result is true for any nonzero endomorphism~\(\Phi\), since \(A(E;E)\)~is simple (see subsection~5.12).
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
We see the following examples of change of coefficient field of a vector space:
\begin{itemize}
\item Taking the underlying real space of a complex space (5.16): for example changing from \(\C\) over~\(\C\) to \(\C\) over~\(\R\). In this case the dimension is doubled. Moreover, the underlying real space can be decomposed into ``real'' and ``imaginary'' parts of equal dimension (11.7).
\item Complexifying a real space (2.16): for example changing from \(\R\) over~\(\R\) to \(\R^2\) over~\(\C\). In this case the dimension is preserved.
\item Inducing complex structure on a real space (8.21): for example changing from \(\R^2\) over~\(\R\) to \(\R^2\) over~\(\C\). In this case the dimension is halved.
\end{itemize}
\end{rmk}

\begin{rmk}
Let \(\varphi:\R^2\to\R^2\) be counterclockwise rotation about the origin by \(\pi/2\) radians. With respect to the standard basis over~\(\R\), the matrix of~\(\varphi\) is
\[M_{\R}(\varphi)=\begin{pmatrix}
0&1\\
-1&0
\end{pmatrix}\]
The characteristic polynomial of~\(\varphi\) and~\(M_{\R}(\varphi)\) over~\(\R\) is \(\chi_{\varphi}^{\R}=\lambda^2+1\).

If we induce the canonical complex structure on~\(\R^2\), then the matrix of~\(\varphi\) with respect to the standard basis over~\(\C\) is
\[M_{\C}(\varphi)=\begin{pmatrix}i\end{pmatrix}\]
The characteristic polynomial of~\(\varphi\) and~\(M_{\C}(\varphi)\) over~\(\C\) is \(\chi_{\varphi}^{\C}=-\lambda+i\). By contrast, the characteristic polynomial of \(M_{\R}(\varphi)\) viewed as a matrix over~\(\C\) (representing a transformation \(\C^2\to\C^2\)) is again \(\lambda^2+1\).

This example shows that the characteristic polynomial of a \emph{transformation} is not in general preserved under field extension, although the characteristic polynomial of a \emph{matrix} is. The same is true of the minimum polynomial.\footnote{See Chapter~XIII. In this example, \(\mu_{\varphi}^{\R}=t^2+1\) while \(\mu_{\varphi}^{\C}=t-i\).}
\end{rmk}

\begin{rmk}
Let \(\Delta\subseteq\Gamma\) be a subfield and \(E\)~be a \(\Gamma\)-vector space. A \(\Delta\)-subspace \(F\) of~\(E\) is a \(\Gamma\)-subspace if and only if \(F\)~is stable under the \(\Delta\)-linear transformations
\[\varepsilon_{\alpha}:x\mapsto\alpha x\qquad\alpha\in\Gamma\]
This stability condition for subspaces is analogous to the commutativity condition for linear transformations in the proposition of subsection~5.19.
\end{rmk}

\newpage
\section*{Chapter~VI}
\subsection*{\S~1}
\begin{rmk}
The space of polynomials in one variable is positively graded by the degrees of monomials. More generally, the space of polynomials in \(p\)~variables is \(p\)-graded by the multidegrees of monomials.
\end{rmk}

\begin{rmk}
Let \(E=\sum_{\alpha\in I}E_{\alpha}\) be a \(G\)-graded space with degree mapping \(k:I\to G\). If \(j:G\to H\) is an injective mapping into an abelian group~\(H\), then by~(6.1) \(E\)~is \(H\)-graded with the elements of~\(E_{\alpha}\) having degree~\((j\after k)(\alpha)\).

In particular if \(p\)~is a fixed integer such that the mapping of~\(G\) defined by \(k\mapsto pk\) is injective, then we may consider the \(G\)-gradation of~\(E\) under which the elements of~\(E_{\alpha}\) have degree~\(pk(\alpha)\). This is done for example in the construction of the exterior algebra over a graded space.\footnote{See subsection~5.21 of~\cite{greub2}.}
\end{rmk}

\begin{rmk}
If \(E=\sum_{k\in G}E_k\) is a \(G\)-graded space and \(F=\sum_{k\in G}F\sect E_k\) is a \(G\)-graded subspace, then \(E/F=\sum_{k\in G}E_k/(F\sect E_k)\) is the \(G\)-graded factor space.\footnote{See problem~2 in chapter~II, \S~4.}
\end{rmk}

\begin{rmk}
The zero map between two \(G\)-graded vector spaces is homogeneous of every degree. A nonzero homogeneous map has a unique degree.
\end{rmk}

\begin{rmk}
Let \(E\) and~\(F\) be \(G\)-graded vector spaces. If \(\varphi:E\to F\) is linear and homogeneous of degree~\(k\) and \(\varphi x\) is homogeneous of degree~\(l\), then we may assume without loss of generality that \(x\)~is homogeneous of degree \(l-k\). Indeed, writing \(x=\sum_m x_m\) with \(\deg x_m=m\), we have \(\varphi x=\sum_m\varphi x_m\) with \(\deg(\varphi x_m)=m+k\). Since \(\deg(\varphi x)=l\), we must have \(\varphi x_m=0\) for \(m\ne l-k\), so \(\varphi x=\varphi x_{l-k}\).
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional \(G\)-graded vector space and \(\varphi:E\to E\) is linear and homogeneous with \(\deg\varphi\ne 0\), then \(\tr\varphi=0\).
\end{rmk}
\begin{proof}
Write \(E=E_{k_1}\dsum\cdots\dsum E_{k_n}\) with \(k_i\in G\) and \(d_i=\dim E_{k_i}<\infty\). Let \((e_{ij})\)~be a basis of~\(E\) such that for each \(1\le i\le n\), \((e_{ij})\)~is a basis of~\(E_{k_i}\) for \(1\le j\le d_i\). Let \(\Delta\)~be a determinant function in~\(E\) with \(\Delta(e_{ij})=1\). Then
\[\tr\varphi=\sum_{i,j}\Delta(e_{11},\ldots,e_{1d_1},\ldots,\varphi(e_{ij}),\ldots,e_{n1},\ldots,e_{n\,d_n})\]
By assumption, \(\varphi(e_{ij})\in E_{k_l}\) for some \(l\ne i\), so each term in this sum is zero, and hence \(\tr\varphi=0\).
\end{proof}
\noindent As an example, formal differentiation in the space of polynomials of degree at most~\(n\) (graded by the degrees of monomials) is homogeneous of degree~\(-1\), so has zero trace. This is also obvious from its matrix representation with respect to the standard basis.

\begin{exer}[6]
Let \(E,E^*\) and \(F,F^*\) be pairs of dual \(G\)-graded vector spaces and let \(\varphi:E\to F\) and \(\varphi^*:E^*\from F^*\) be dual linear maps. If \(\varphi\)~is homogeneous of degree~\(k\), then \(\varphi^*\)~is homogeneous of degree~\(-k\).
\end{exer}
\begin{proof}
We have direct sum decompositions
\[E=\sum_{m\in G}E_m\qquad E^*=\sum_{m\in G}E^{*m}\]
and
\[F=\sum_{n\in G}F_n\qquad F^*=\sum_{n\in G}F^{*n}\]
where the pairs \(E_m,E^{*m}\) and \(F_n,F^{*n}\) are dual for all \(m,n\) under the restrictions of the scalar products between \(E,E^*\) and \(F,F^*\), respectively (see subsection~6.5). We also have \(\varphi E_m\subseteq F_{m+k}\) for all~\(m\). We must prove \(\varphi^* F^{*n}\subseteq E^{*n-k}\) for all~\(n\).

Let \(y^*\in F^{*n}\) and \(x\in E\). Write \(x=\sum_m x_m\) where \(x_m\in E_m\). Then
\[\sprod{\varphi^* y^*}{x}=\sprod{y^*}{\varphi x}=\sum_m\sprod{y^*}{\varphi x_m}=\sprod{y^*}{\varphi x_{n-k}}=\sprod{\varphi^* y^*}{x_{n-k}}\]
which implies
\[\sprod{\varphi^* y^*}{x-\pi_{n-k}x}=0\tag{1}\]
where \(\pi_{n-k}:E\to E_{n-k}\) is the canonical projection. Now write \(\varphi^* y^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*m}=0\) for all \(m\ne n-k\). Indeed, for \(m\ne n-k\) and \(x\in E_m\) we have \(\pi_{n-k}x=0\), so by~(1)
\[\sprod{x^{*m}}{x}=\sum_p\sprod{x^{*p}}{x}=\sprod{\varphi^* y^*}{x}=0\]
Therefore \(x^{*m}=0\). It follows that \(\varphi^* y^*=x^{*n-k}\in E^{*n-k}\), as desired.
\end{proof}
\begin{rmk}
It follows that the restrictions \(\varphi_m:E_m\to F_{m+k}\) and \(\varphi^*_{m+k}:E^*_m\from F^*_{m+k}\) are dual.
\end{rmk}

\begin{exer}[8]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. If \(F\)~is a \(G\)-graded subspace of~\(E\), then \(\oc{F}\)~is a \(G\)-graded subspace of~\(E^*\) and \(\occ{F}=F\).
\end{exer}
\begin{proof}
We have direct sums \(E=\sum_{m\in G}E_m\) and \(E^*=\sum_{m\in G}E^{*m}\) where the pairs \(E_m,E^{*m}\) are dual under the restrictions of the scalar product between \(E,E^*\) and \(\dim E_m=\dim E^{*m}<\infty\) for all~\(m\). By assumption, \(F=\sum_{m\in G}F\sect E_m\).
We must prove
\[\oc{F}=\sum_{m\in G}\oc{F}\sect E^{*m}\tag{1}\]
Let \(x^*\in\oc{F}\) and write \(x^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*n}\in\oc{F}\) for all~\(n\). Indeed, if \(x\in F\), write \(x=\sum_m x_m\) where \(x_m\in F\sect E_m\). Then
\[\sprod{x^{*n}}{x}=\sum_m\sprod{x^{*n}}{x_m}=\sprod{x^{*n}}{x_n}=\sum_m\sprod{x^{*m}}{x_n}=\sprod{x^*}{x_n}=0\]
This establishes~(1). By symmetry, we have
\[\occ{F}=\sum_{m\in G}\occ{F}\sect E_m\tag{2}\]
We claim \(\occ{F}\sect E_n\subseteq F\sect E_n\) for all~\(n\). To prove this, we first show
\[\occ{F}\sect E_n\subseteq(F\sect E_n)^{\perp_n\perp_n}\tag{3}\]
where \(\perp_n\)~is taken relative to the scalar product between \(E_n,E^{*n}\). Indeed, let \(x\in\occ{F}\sect E_n\) and \(x^*\in(F\sect E_n)^{\perp_n}\subseteq E^{*n}\). If \(y\in F\), write \(y=\sum_m y_m\) where \(y_m\in F\sect E_m\). Then
\[\sprod{x^*}{y}=\sum_m\sprod{x^*}{y_m}=\sprod{x^*}{y_n}=0\]
This implies \(x^*\in\oc{F}\), which implies \(\sprod{x^*}{x}=0\), which in turn implies~(3). Now \((F\sect E_n)^{\perp_n\perp_n}=F\sect E_n\) since \(\dim E_n<\infty\), which establishes the claim. Finally, it follows from~(2) that \(\occ{F}=F\).
\end{proof}

\subsection*{\S~2}
\begin{rmk}
Let \(A=\sum_k A_k\) be a positively graded algebra. If \(b\in A_q\) with \(b\ne 0\) and
\[b=\sum_i u_ia_i\qquad(u_i\in A,a_i\in A_p)\]
then \(q\ge p\) and we may assume that \(u_i\in A_{q-p}\) for all~\(i\).
\end{rmk}
\begin{proof}
By bilinearity of the algebra multiplication, we may assume that each~\(u_i\) is homogeneous, say \(u_i\in A_{k_i}\). Since \(\deg b=q\) and \(\deg(u_ia_i)=k_i+p\), it follows that \(\sum_{k_i+p\ne q}u_ia_i=0\). On the other hand since \(b\ne 0\), there is at least one~\(i\) with \(k_i+p=q\), so \(q\ge p\).
\end{proof}

\begin{exer}[1]
Let \(A\)~be a \(G\)-graded algebra. If \(x\in A\) is an invertible element homogeneous of degree~\(k\), then \(x^{-1}\)~is homogeneous of degree~\(-k\). If \(A\)~is nonzero and positively graded, then \(k=0\).
\end{exer}
\begin{proof}
Write \(A=\sum_{m\in G}A_m\) and \(x^{-1}=\sum_m x_m\) with \(x_m\in A_m\). Then
\[e=xx^{-1}=\sum_m xx_m\]
Since \(\deg e=0\) and \(\deg(xx_m)=m+k\), it follows that \(xx_m=0\) for all \(m\ne-k\). Therefore \(e=xx_{-k}\) and \(x^{-1}=x_{-k}\), so \(x^{-1}\)~is homogeneous of degree~\(-k\).

If \(A\ne 0\), then \(x\ne 0\) and \(x^{-1}\ne 0\), so \(A_k\ne 0\) and \(A_{-k}\ne 0\). If \(A\)~is positively graded, this forces \(k=0\).
\end{proof}

\begin{exer}[4]
Let \(E\)~be a \(G\)-graded vector space. Then the subspace \(A_G(E;E)\) of~\(A(E;E)\) generated by homogeneous linear transformations of~\(E\) forms a \(G\)-graded subalgebra of~\(A(E;E)\).
\end{exer}
\begin{proof}
First observe that \(A_G(E;E)\)~is naturally graded as a vector space by the degrees of homogeneous transformations (see problem~3). If \(\varphi,\psi\in A_G(E;E)\) are homogeneous with \(\deg\varphi=m\) and \(\deg\psi=n\), then it is obvious that \(\varphi\psi\)~is homogeneous with \(\deg(\varphi\psi)=m+n\). It follows from this that \(A_G(E;E)\)~is a \(G\)-graded subalgebra.
\end{proof}

\begin{exer}[7]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. Then the mapping
\[\Phi:A_G(E;E)\to\opp{A_G(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
\(\Phi\)~is well defined by problems 6 and~10 of~\S~1, and is an isomorphism by problem~12 of chapter~V, \S~1.
\end{proof}

\newpage
\section*{Chapter~VII}
\textbf{In this chapter, all vector spaces are real.}

\subsection*{\S~1}
\begin{rmk}
The Riesz representation theorem shows that for a finite-dimensional \emph{inner product space}~\(E\), there is a \emph{natural} isomorphism between \(E\) and its dual space~\(L(E)\) given by \(x\mapsto\iprod{x}{-}\). This is unlike for a finite-dimensional vector space, where the isomorphism is in general non-natural, and means we may naturally \emph{identify} a vector~\(x\) with its \emph{dual vector} or \emph{covector} \(\iprod{x}{-}\).
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional inner product space and \(E_1\)~is a subspace of~\(E\), then \(E\)~induces an inner product in~\(E/E_1\) by
\[\iprod{\proj{x}}{\proj{y}}=\iprod{x}{y}\]
where \(x,y\) are the unique representatives of \(\proj{x},\proj{y}\) in~\(\oc{E_1}\) (see problem~5).
\end{rmk}
\begin{proof}
The only thing to check is bilinearity, which follows from the fact that \(\oc{E_1}\)~is a subspace of~\(E\).
\end{proof}

\begin{exer}[5]
If \(E\)~is a finite-dimensional inner product space and \(E_1\)~is a subspace of~\(E\), then every element of~\(E/E_1\) has exactly one representative in~\(\oc{E_1}\).
\end{exer}
\begin{proof}
By duality, \(\dim E=\dim E_1+\dim\oc{E_1}\), and by definiteness of the inner product, \(E_1\sect\oc{E_1}=0\), so \(E=E_1\dsum\oc{E_1}\). For \(x+E_1\in E/E_1\), let \(y=\pi(x)-x\) where \(\pi\)~is the canonical projection onto~\(\oc{E_1}\). Then \(y\in E_1\) so \(x+y\in x+E_1\) and \(x+y\in\oc{E_1}\), as desired. If \(z\in E_1\) and \(x+z\in\oc{E_1}\), then
\[y-z=(y+x)-(x+z)\in E_1\sect\oc{E_1}=0\]
so \(y=z\), establishing uniqueness.
\end{proof}

\subsection*{\S~2}
\begin{rmk}
Let \(E\)~be an inner product space of finite dimension~\(n\). We provide an inductive proof of the existence of an orthonormal basis in~\(E\).

For \(n=0,1\), the result is trivial. For \(n>1\), fix a unit vector \(e_1\in E\) and let \(E_1=\gen{e_1}\). By induction, there is an orthonormal basis \(\proj{e_2},\ldots,\proj{e_n}\) in the induced inner product space \(E/E_1\) (see above). Letting \(e_2,\ldots,e_n\) be the representatives in~\(\oc{E_1}\), it follows that \(e_1,\ldots,e_n\) is an orthonormal basis in~\(E\).
\end{rmk}

\begin{rmk}
In the Gram-Schmidt process, we can just let \(e_1=a_1/\norm{a_1}\) and
\[e_k=\frac{a_k-\iprod{a_k}{e_1}e_1-\cdots-\iprod{a_k}{e_{k-1}}e_{k-1}}{\norm{a_k-\iprod{a_k}{e_1}e_1-\cdots-\iprod{a_k}{e_{k-1}}e_{k-1}}}\qquad(k=2,\ldots,n)\]
At each step, we compute the difference between the current vector~\(a_k\) and its orthogonal projection onto the subspace generated by the previous vectors, then normalize (compare (7.19)).
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional inner product space and \(\varphi:E\to E\) is linear, then the dual transformation \(\varphi^*:E\to E\) satisfies
\[\iprod{\varphi^* x}{y}=\iprod{x}{\varphi y}\]
If \(\varphi\)~preserves inner products, then it also preserves orthonormal bases and is invertible. In this case, \(\varphi^{-1}\)~also preserves inner products, so
\[\iprod{\varphi^{-1}x}{y}=\iprod{\varphi^{-1}x}{\varphi^{-1}\varphi y}=\iprod{x}{\varphi y}\]
and it follows that \(\varphi^{-1}=\varphi^*\). Conversely if \(\varphi^{-1}=\varphi^*\), then
\[\iprod{\varphi x}{\varphi y}=\iprod{\varphi^*\varphi x}{y}=\iprod{x}{y}\]
so \(\varphi\)~preserves inner products.

Such a~\(\varphi\) is called an \emph{orthogonal} transformation. Note \(\varphi\)~is orthogonal if and only if \(M(\varphi)\)~is an orthogonal matrix relative to an orthonormal basis.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
A determinant function~\(\Delta\) in an inner product space~\(E\) is normed if and only if \(\abs{\Delta(e_1,\ldots,e_n)}=1\) for any orthonormal basis \(e_1,\ldots,e_n\). If \(E\)~is oriented and \(\Delta\)~is the normed determinant function representing the orientation, then \(\Delta(e_1,\ldots,e_n)=1\) if \(e_1,\ldots,e_n\) is positive. Geometrically, this is just \emph{the} oriented volume function in~\(E\). By comparing (4.26) and~(7.23), we see that \emph{the normed determinant functions are precisely the self-dual determinant functions}.

If \(\Delta_0\ne 0\) is a determinant function in~\(E\) and \(\Delta_0^*\)~is its dual, then \(\Delta_0^*=\alpha\Delta_0\) for some real number~\(\alpha\) by uniqueness of~\(\Delta_0\), so
\[\alpha\Delta_0(x_1,\ldots,x_n)\Delta_0(y_1,\ldots,y_n)=\det\iprod{x_i}{y_j}\qquad(x_i,y_i\in E)\]
Substituting \(x_i=y_i=e_i\) shows that \(\alpha>0\). It follows that \(\Delta_1=\pm\sqrt{\alpha}\mult\Delta_0\) is a normed determinant function.
\end{rmk}

\begin{rmk}
It follows from~(7.37) that if \(x\) and~\(y\) are linearly independent, then
\[\norm{x\cross y}=\Delta(x,y,z)\]
where \(z=(x\cross y)/\norm{x\cross y}\). Since \(z\)~is a unit vector orthogonal to \(x\) and~\(y\), it follows (subsection 7.15) that \(\Delta(x,y,z)\)~is just the area of the parallelogram determined by \(x\) and~\(y\). In other words,
\[\norm{x\cross y}=\norm{x}\norm{y}\sin\theta\]
where \(0\le\theta\le\pi\) is the angle between \(x\) and~\(y\). This last equation obviously still holds if \(x\) and~\(y\) are linearly dependent but nonzero.
\end{rmk}

\begin{exer}[12]
For vectors \(x_1,\ldots,x_p\),
\[G(x_1,\ldots,x_p)\le\norm{x_1}^2\cdots\norm{x_p}^2\tag{1}\]
Additionally
\[\begin{vmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nn}
\end{vmatrix}^2\le\sum_{k=1}^n\abs{a_{1k}}^2\cdots\sum_{k=1}^n\abs{a_{nk}}^2\tag{2}\]
\end{exer}
\begin{proof}
By induction with the ``base times height'' rule for volume (subsection 7.15), \(V(u_1,\ldots,u_p)\le 1\) for linearly independent \emph{unit} vectors \(u_1,\ldots,u_p\).

Now if \(x_1,\ldots,x_p\) are linearly dependent, then \(G(x_1,\ldots,x_p)=0\) and (1)~holds trivially. Otherwise, setting \(u_i=x_i/\norm{x_i}\) we have
\[\sqrt{G(x_1,\ldots,x_p)}=V(x_1,\ldots,x_p)=\norm{x_1}\cdots\norm{x_p}\mult V(u_1,\ldots,u_p)\le\norm{x_1}\cdots\norm{x_p}\]
Squaring both sides yields~(1). Since the determinant of a matrix is a (normed) determinant function of the rows, (2)~follows.
\end{proof}
\begin{rmk}
These results both simply express an upper bound for the volume of a parallelepiped in terms of the lengths of the edges.
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The covariant components of a vector are just its coordinates in the dual space, which are also just the entries of its matrix (as a linear function). They are called ``covariant'' because they vary in the same way as basis vectors in the original space under a change of basis (see subsections 3.13--14). This is unlike the regular components of the vector, which vary inversely to the basis vectors and may be called the \emph{contravariant components}.

To see why the covariant components of a vector are its inner products with the basis vectors, observe from (7.42) and~(7.49) that
\[\iprod{x}{e_{\nu}}=\sprod{\tau x}{e_{\nu}}=\sum_{\lambda}\xi_{\lambda}\sprod{e^{*\lambda}}{e_{\nu}}=\xi_{\nu}\]
Dually, the contravariant components of a vector are its inner products with the dual basis vectors:
\[\iprod{\tau x}{e^{*\nu}}=\xi^{\nu}\]
\end{rmk}

\begin{rmk}
If \(e_{\nu}\)~is an orthonormal basis of~\(E\), then the dual basis \(e^{*\nu}=\tau e_{\nu}\) is an orthonormal basis of~\(E^*\).
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
In subsection~7.22, observe that \(Q\)~is just the set of unit vectors under the norm defined in example~3 in subsection~7.20. Since norm functions are continuous under the natural topology, \(Q\)~is closed under this topology, and \(Q\)~is also clearly bounded under this topology. It follows that \(Q\)~is compact under this topology since \(E\)~is finite-dimensional.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(x=\lambda e+x_1\) and \(y=\mu e+y_1\) are quaternions (\(\lambda,\mu\in\R\) and \(x_1,y_1\in E_1\)), then by the definition of quaternion multiplication
\[xy=\bigl(\lambda\mu-\iprod{x_1}{y_1}\bigr)e+\lambda y_1+\mu x_1+x_1\cross y_1\]
\end{rmk}

\begin{rmk}
In subsection~7.24, in the proof of Lemma~I, observe that the result holds trivially if \(y=\pm x\) by taking \(\lambda=\mp 1\). If \(y\ne\pm x\), then \(e\ne 0\). Suppose
\[\alpha x+\beta y+\gamma e=0\qquad(\alpha,\beta,\gamma\in\R)\]
If \(\alpha\ne 0\), then \(x=\beta_1y+\gamma_1e\) for \(\beta_1,\gamma_1\in\R\), so
\[-e=x^2=(\beta_1y+\gamma_1e)^2=2\beta_1\gamma_1y+(\gamma_1^2-\beta_1^2)e\]
which implies
\[2\beta_1\gamma_1y=(\beta_1^2-\gamma_1^2-1)e\]
If \(\beta_1=0\), it follows that \(\gamma_1^2=-1\), which is impossible. If \(\gamma_1=0\), it follows that \(\beta_1=\pm 1\), so \(x=\pm y\) contrary to assumption. Therefore \(\beta_1\gamma_1\ne 0\), so \(y=\delta e\) for \(\delta\in\R\). But then \(-e=y^2=\delta^2 e\), so \(\delta^2=-1\), which is impossible. It follows that \(\alpha=0\). Similarly \(\beta=0\). Finally, \(\gamma=0\). This result shows that the vectors \(x,y,e\) are linearly independent.

Now \(x+y\) and \(x-y\) are roots of polynomials of degree \(1\) or~\(2\), but the linear independence of \(x,y,e\) implies that these polynomials must have degree~\(2\), so (7.60) and~(7.61) follow.
\end{rmk}

\newpage
\section*{Chapter~VIII}
\textbf{In this chapter, all vector spaces are real and finite-dimensional.}

\begin{rmk}
In this chapter, it is useful to think intuitively of transformations like complex numbers, which induce transformations of the complex plane through multiplication. Under this analogy, adjoints correspond to complex conjugates; self-adjoint transformations, to real numbers; positive transformations, to nonnegative real numbers; skew transformations, to purely imaginary numbers; and isometries, to complex numbers on the unit circle.
\end{rmk}

\subsection*{\S~1}
\begin{rmk}
In subsection~8.2, if the bases \((x_{\nu})\) and~\((y_{\mu})\) are orthonormal, then they are self-dual,\footnote{See problem~2 in chapter~VII, \S~2.} so \(M(\adj{\varphi},y_{\mu},x_{\nu})=M(\varphi,x_{\nu},y_{\mu})^*\) by~(3.4). It follows that
\[\tilde{\alpha}^{\varrho}_{\mu}=M(\adj{\varphi})^{\varrho}_{\mu}=M(\varphi)^{\mu}_{\varrho}=\alpha^{\mu}_{\varrho}\]
\end{rmk}

\begin{rmk}
In subsection~8.4, the natural isomorphism \(B(E,E)\iso L(E;E)\) follows from Proposition~III in chapter~II, \S~6. There is also a natural isomorphism
\[B(E,E)\iso L(E;L(E))\]
given by
\[\Phi\mapsto(x\mapsto(y\mapsto\Phi(x,y)))\tag{1}\]
Recall from subsection~7.18 the natural isomorphism \(\tau:E\to L(E)\) given by \(x\mapsto\iprod{x}{-}\), which maps each vector to its dual vector. For a linear transformation \(\varphi:E\to E\) and its corresponding bilinear function~\(\Phi\) defined by \(\Phi(x,y)=\iprod{\varphi x}{y}\), the linear map~\(\Phi'\) corresponding to~\(\Phi\) under the isomorphism~(1) is~\(\tau\varphi\):
\begin{diagram}[nohug]
x	&\rMapsto^{\varphi}	&\varphi x\\
	&\rdMapsto<{\Phi'}	&\dMapsto>{\tau}\\
	&					&\iprod{\varphi x}{-}
\end{diagram}
In these ways, \(\Phi\)~is naturally dual to~\(\varphi\). Properties of~\(\Phi\) naturally correspond to those of~\(\varphi\); for example, \(\Phi\)~is symmetric if and only if \(\varphi\) is symmetric (self-adjoint), and \(\Phi\)~is skew symmetric if and only if \(\varphi\)~is skew symmetric. Since
\[\Phi(x,y)=\iprod{\varphi x}{y}=\iprod{x}{\adj{\varphi}y}\]
we see that \(\varphi\)~is ``left-dual'' to~\(\Phi\), while \(\adj{\varphi}\)~is ``right-dual'' to~\(\Phi\).
\end{rmk}

\begin{rmk}
In subsection~8.5, observe for a direct sum \(E=\sum E_i\) and \(\varphi=\sum\varphi_i\) with \(\varphi_i:E_i\to E_i\), if \(E_i\)~is stable under~\(\adj{\varphi}\) then the restriction of~\(\adj{\varphi}\) to~\(E_i\) is the adjoint of~\(\varphi_i\). In other words, if \(\adj{\varphi}_i\)~denotes the restriction, then
\[\adj{\varphi}_i=\adj{\varphi_i}\]
Indeed, for \(x,y\in E_i\),
\[\iprod{\adj{\varphi}_i x}{y}=\iprod{\adj{\varphi}x}{y}=\iprod{x}{\varphi y}=\iprod{x}{\varphi_i y}\]
It follows that \(\adj{\varphi}=\sum\adj{\varphi_i}\). If additionally the \(E_i\)~are orthogonal, we see that \(\varphi\)~is normal if and only if each \(\varphi_i\)~is normal.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
Geometrically, a self-adjoint transformation just independently scales the axes in some system of orthogonal axes for the space.
\end{rmk}

\begin{rmk}
For any transformation~\(\varphi\), the transformations
\[\varphi+\adj{\varphi}\qquad\varphi\adj{\varphi}\qquad\adj{\varphi}\varphi\]
are self-adjoint.
\end{rmk}

\begin{rmk}
If \(E=E_1\dsum E_2\) with \(E_1\orth E_2\), and \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are self-adjoint, then \(\varphi=\varphi_1\dsum\varphi_2\) is self-adjoint.
\end{rmk}
\begin{proof}
For \(x=x_1+x_2\) and \(y=y_1+y_2\) with \(x_i,y_i\in E_i\),
\begin{align*}
\iprod{\varphi x}{y}&=\iprod{\varphi_1 x_1+\varphi_2 x_2}{y_1+y_2}\\
	&=\iprod{\varphi_1 x_1}{y_1}+\iprod{\varphi_2 x_2}{y_2}\\
	&=\iprod{x_1}{\varphi_1 y_1}+\iprod{x_2}{\varphi_2 y_2}\\
	&=\iprod{x_1+x_2}{\varphi_1 y_1+\varphi_2 y_2}\\
	&=\iprod{x}{\varphi y}\qedhere
\end{align*}
\end{proof}

\begin{rmk}
We provide an alternative proof of the existence of eigenvectors for self-adjoint transformations (subsection 8.6).

Let \(\varphi:E\to E\) be a transformation. First observe that for \(u\ne 0\), \(\varphi u=\lambda u\) for some~\(\lambda\) if and only if \(\varphi u\in\occ{u}\), which is true if and only if \(\iprod{\varphi u}{v}=0\) for all \(v\in\oc{u}\). This motivates us to study the bilinear function
\[\Phi(x,y)=\iprod{\varphi x}{y}\]
We may assume without loss of generality that \(\norm{u}=\norm{v}=1\). Consider a unit vector~\(w\) coincident with~\(u\) and moving in the direction of~\(v\):
\[w(t)=(\cos t)u+(\sin t)v\qquad 0\le t\le\pi/2\]
Note \(\norm{w(t)}=1\) and \(w(0)=u\). Also
\[w'(t)=(-\sin t)u+(\cos t)v\]
so \(w'(0)=v\). This means \(\Phi(u,v)=\Phi(w(0),w'(0))\). If \(\varphi\)~is self-adjoint, then \(\Phi\)~is symmetric and
\[2\mult\Phi(u,v)=\Phi(w'(0),w(0))+\Phi(w(0),w'(0))=(\Psi\after w)'(0)\]
where \(\Psi(x)=\Phi(x,x)\) is the quadratic function associated with~\(\Phi\). Now we have \((\Psi\after w)'(0)=0\) if \(t=0\) minimizes (or maximizes) \(\Psi\after w\), which is true if \(w(0)=u\) minimizes (or maximizes) \(\Psi\). But we know such a~\(u\) exists since the unit sphere is compact in~\(E\), and the argument above then shows that \(u\)~is an eigenvector of~\(\varphi\). Note that
\[\Psi(u)=\Phi(u,u)=\iprod{\varphi u}{u}=\iprod{\lambda u}{u}=\lambda\iprod{u}{u}=\lambda\]
is the eigenvalue, which is the minimum (resp. maximum) eigenvalue of~\(\varphi\). In summary: the quadratic function associated with a self-adjoint transformation attains its extreme values on the unit sphere at eigenvectors, where its values are the extreme eigenvalues.
\end{rmk}

\begin{exer}[5]
Every positive transformation~\(\varphi\) has a unique positive square root (that is, a positive transformation~\(\psi\) such that \(\psi^2=\varphi\)).
\end{exer}
\begin{proof}
There is an orthonormal basis \(e_1,\ldots,e_n\) of eigenvectors of~\(\varphi\), so that \(\varphi e_i=\lambda_i e_i\) for \(\lambda_i\in\R\). Now \(\lambda_i=\iprod{e_i}{\varphi e_i}\ge 0\), so setting \(\psi e_i=\sqrt{\lambda_i}e_i\) it follows that \(\psi\)~is positive with \(\psi^2=\varphi\).

If \(\psi_1\)~is positive with \(\psi_1^2=\varphi\), then the eigenvalues of~\(\psi_1\) must be \(\sqrt{\lambda_i}\). Also, if \(\psi_1x=\sqrt{\lambda_i}x\), then \(\varphi x=\lambda_i x\), so \(E_{\psi_1}(\sqrt{\lambda_i})\subseteq E_{\varphi}(\lambda_i)\). By~(8.21), it follows that \(E_{\psi_1}(\sqrt{\lambda_i})=E_{\varphi}(\lambda_i)\), so \(\psi_1 e_i=\sqrt{\lambda_i}e_i\) and \(\psi_1=\psi\).
\end{proof}

\subsection*{\S~3}
\begin{rmk}
In subsection~2.19, we see that a linear transformation~\(\pi\) is a projection operator (that is, \(\pi^2=\pi\)) if and only if \(\pi=0_{\ker\pi}\dsum\iota_{\im\pi}\). In subsection~8.11, we see that \(\pi\)~is additionally an \emph{orthogonal} projection if and only if \(\ker\pi\orth\im\pi\). In summary, for a projection operator~\(\pi\), the following are equivalent:
\begin{itemize}[itemsep=0pt]
\item \(\ker\pi\orth\im\pi\).
\item \(\pi\)~is self-adjoint.
\item \(\pi\)~is normal.
\end{itemize}
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
Geometrically, a skew transformation, apart from possibly killing off part of the space, induces scaled 90-degree rotations\footnote{No one calls these ``scrotations''.} on orthogonal stable planes in the space.
\end{rmk}

\begin{rmk}
For any transformation~\(\varphi\), the transformation \(\varphi-\adj{\varphi}\) is skew. Also
\[\varphi=\tfrac{1}{2}(\varphi+\adj{\varphi})+\tfrac{1}{2}(\varphi-\adj{\varphi})\]
uniquely represents~\(\varphi\) as a sum of self-adjoint and skew transformations.
\end{rmk}

\begin{rmk}
If \(\varphi\)~is a skew transformation of~\(E\) and \(F\)~is a stable subspace of~\(E\), then \(\oc{F}\)~is also stable.
\end{rmk}
\begin{proof}
If \(x\in F\) and \(y\in\oc{F}\), then \(\iprod{x}{\varphi y}=-\iprod{\varphi x}{y}=0\).
\end{proof}

\begin{rmk}
In subsection~8.16, the proof of the normal form~(8.35) is incorrect because it is not true in general that the~\(a_n\) defined form an orthonormal basis of the space. For example in~\(\R^4\), if we define the transformation \(\psi\) by
\[e_1\mapsto e_2\qquad e_2\mapsto -e_1\qquad e_3\mapsto e_4\qquad e_4\mapsto -e_3\]
where \(e_i\)~is the \(i\)-th standard basis vector, then \(\psi\)~is skew and \(\varphi=\psi^2=-\iota\) is diagonalized by the standard basis. If we follow the proof for this example, we get \(a_1=e_1\), \(a_2=\psi e_1=e_2\), \(a_3=e_2\), and \(a_4=\psi e_2=-e_1\), so the~\(a_n\) do not form a basis of~\(\R^4\).

To prove the result, let \(\lambda_1,\ldots,\lambda_r\) be the distinct eigenvalues of \(\varphi=\psi^2\) and write the orthogonal decomposition~(8.21)
\[E=E_1\dsum\cdots\dsum E_r\]
where \(E_i\)~is the eigenspace of~\(\varphi\) corresponding to~\(\lambda_i\). Observe that \(E_i\)~is stable under~\(\psi\), for if \(x\in E_i\) then
\[\varphi(\psi x)=\psi(\varphi x)=\psi(\lambda_i x)=\lambda_i(\psi x)\]
so \(\psi x\in E_i\). As in the book, the~\(\lambda_i\) are negative or zero. If \(\lambda_i<0\), construct a basis for \(F=E_i\) in the following way: let \(a_1\)~be an arbitrary unit vector in~\(F\) and \(a_2=\kappa_i^{-1}\psi a_1\) where \(\kappa_i=\sqrt{-\lambda_i}\). It is immediate that \(a_2\)~is a unit vector in~\(F\) orthogonal to~\(a_1\) and \(H=\gen{a_1,a_2}\) is stable under~\(\psi\). Let \(\oc{H}\)~be the orthogonal complement of~\(H\) in~\(F\). If \(\oc{H}=0\), take the basis \(a_1,a_2\). Otherwise, adjoin to \(a_1,a_2\) the result of applying this procedure recursively to \(F=\oc{H}\) (recall that \(\oc{H}\)~is stable under~\(\psi\) by the remark above). If \(\lambda_i=0\), assume that \(i=r\) and choose any orthonormal basis of~\(E_r\), noting that \(\psi\)~is zero on~\(E_r\) since
\[\norm{\psi x}^2=\iprod{\psi x}{\psi x}=-\iprod{x}{\varphi x}=0\]
for \(x\in E_r\). Combine the resulting bases of the~\(E_i\) to obtain an orthonormal basis of~\(E\) with respect to which the matrix of~\(\psi\) has the form~(8.35).\footnote{See \url{https://math.stackexchange.com/q/3402347}.}
\end{rmk}

\begin{rmk}
If \(E\)~is an oriented Euclidean \(3\)-space and \(\varphi:E\to E\) is linear, then
\[\varphi x\cross\varphi y=\adj{(\ad\varphi)}(x\cross y)\]
Moreover if \(\varphi\)~is invertible, then
\[\varphi(x\cross y)=\det\varphi\mult\adj{\varphi}^{-1}x\cross\adj{\varphi}^{-1}y\]
\end{rmk}
\begin{proof}
For all vectors~\(z\),
\begin{align*}
\iprod{\varphi x\cross\varphi y}{z}&=\Delta(\varphi x,\varphi y,z)\\
	&=\Delta(x,y,(\ad\varphi)z)\\
	&=\iprod{x\cross y}{(\ad\varphi)z}\\
	&=\iprod{\adj{(\ad\varphi)}(x\cross y)}{z}
\end{align*}
so the first equation follows by definiteness of the inner product. The second equation is proved in a similar way.
\end{proof}

\begin{exer}[1]
If \(\varphi\)~is a skew transformation of a plane, then
\[\iprod{\varphi x}{\varphi y}=\det\varphi\mult\iprod{x}{y}\]
\end{exer}
\begin{proof}
If \(\varphi=0\), then the result is trivial; if \(\varphi\ne 0\), then \(\varphi\)~is an automorphism since it has even rank. Now \(\Delta(x,y)=\iprod{\varphi x}{y}\) is a determinant function, so
\[\iprod{\varphi^2 x}{\varphi y}=\Delta(\varphi x,\varphi y)=\det\varphi\mult\Delta(x,y)=\det\varphi\mult\iprod{\varphi x}{y}\]
Substituting \(\varphi^{-1}x\) for~\(x\) yields the result.
\end{proof}

\begin{exer}[2 - Skew transformations of 3-space]
Let \(E\)~be an oriented Euclidean 3-space.
\begin{enumerate}
\item[(i)] For \(a\in E\), \(\varphi_a(x)=a\cross x\) is a skew transformation of~\(E\).
\item[(ii)] For \(a,b\in E\), \(\varphi_{a\cross b}=\varphi_a\varphi_b-\varphi_b\varphi_a\).
\item[(iii)] If \(\varphi\)~is a skew transformation of~\(E\), then \(\varphi=\varphi_a\) for a unique \(a\in E\).
\item[(iv)] The vector~\(a\) in~(iii) is given by
\[a=\alpha_{23}e_1+\alpha_{31}e_2+\alpha_{12}e_3\]
where \(e_1,e_2,e_3\) is a positive orthonormal basis of~\(E\) and \((\alpha_{ij})=M(\varphi;e_i)\).
\item[(v)] If \(a\ne 0\), then \(\ker\varphi_a=\gen{a}\) and \(\oc{a}\)~is stable under~\(\varphi_a\).
\item[(vi)] If \(e_1,e_2\) are normal such that \(e_1,e_2,a\) is a positive orthogonal basis of~\(E\), then
\[M(\varphi_a;e_1,e_2,a)=\begin{pmatrix}
0&\norm{a}&0\\
-\norm{a}&0&0\\
0&0&0
\end{pmatrix}\]
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] It is a transformation since the cross product is bilinear, and it is skew since \(\iprod{x}{a\cross x}=0\) for all \(x\in E\).
\item[(ii)] By the vector triple product formula~(7.41),
\[(a\cross b)\cross x=a\cross(b\cross x)-b\cross(a\cross x)\]
\item[(iii)] For uniqueness, note that if \(\varphi_a=\varphi_b\), then \(a\cross x=b\cross x\) for all \(x\in E\), so \((a-b)\cross x=0\) for all \(x\in E\), so \(a-b=0\) and \(a=b\).

For existence, note that the bilinear function \(\iprod{\varphi x}{y}\) is skew symmetric, and hence a determinant function in any plane in~\(E\). Define
\[\Phi(x,y,z)=\iprod{\varphi y}{z}x+\iprod{\varphi z}{x}y+\iprod{\varphi x}{y}z\]
Then \(\Phi\)~is clearly trilinear and skew symmetric. By the universal property of determinants if \(\Delta\)~is the normed determinant function representing the orientation in~\(E\) and \(e_1,e_2,e_3\) is an orthonormal basis with \(\Delta(e_1,e_2,e_3)=1\), then \(\Phi=\Delta a\) where \(a=\Phi(e_1,e_2,e_3)\).\footnote{See Proposition~III in subsection~4.3.} By direct computation it is easily verified that \(a\cross e_i=\varphi e_i\), so \(\varphi=\varphi_a\).
\item[(iv)] By the proof of~(iii), noting that \(a=\Phi(e_1,e_2,e_3)\) and \(\alpha_{ij}=\iprod{\varphi e_i}{e_j}\).
\item[(v)] By the fact that \(a\cross x=0\) if and only if \(a\) and~\(x\) are linearly dependent.
\item[(vi)] By direct computation.\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
The definition of~\(\Phi\) in~(iii) is motivated by the fundamental relationship between \(n\)-dimensional and \((n-1)\)-dimensional determinant functions discussed in \S~1 of chapter~IV above. If \(\varphi\)~is skew, then its dual bilinear function \(\Delta_1(x,y)=\iprod{\varphi x}{y}\) is a determinant function in any plane in~\(E\). If \(\varphi=\varphi_a\), then
\[\Delta_1(x,y)=\iprod{a\cross x}{y}=\Delta(a,x,y)\]
where \(\Delta\)~is the normed determinant function representing the orientation in~\(E\). In other words, \(\Delta_1\)~is just the 2-dimensional area function induced by the 3-dimensional volume function~\(\Delta\) and the vector~\(a\). Therefore if we \emph{define} a 3-dimensional volume map~\(\Phi\) in terms of~\(\Delta_1\), we should expect that \(\Phi=\Delta a\).

This result, which shows that any skew linear \emph{transformation} of the space can be represented by a unique vector under the \emph{cross} product, is analogous to the Riesz representation theorem, which shows that any linear \emph{function} of the space can be represented by a unique vector under the \emph{inner} product.
\end{rmk}

\begin{exer}[3]
If \(\varphi\ne 0\) and~\(\psi\) are skew transformations of an oriented Euclidean 3-space with \(\ker\varphi\subseteq\ker\psi\), then \(\psi=\lambda\varphi\) for some \(\lambda\in\R\).
\end{exer}
\begin{proof}
By the previous problem, we can write \(\varphi(x)=a\cross x\) and \(\psi(x)=b\cross x\) with \(a\ne 0\). By assumption, \(a\) and~\(b\) are orthogonal to the same plane, so \(b=\lambda a\) for some \(\lambda\in\R\) and hence \(\psi=\lambda\varphi\).
\end{proof}

\begin{exer}[4]
\[(a_1\cross a_2)\cross a_3=a_2(a_1,a_3)-a_1(a_2,a_3)\]
\end{exer}
\begin{proof}
Without loss of generality, we may assume that \(a_1,a_2\) are orthonormal. In particular, \(a_1\cross a_2\ne 0\). Define
\[\varphi(x)=(a_1\cross a_2)\cross x\qquad\text{and}\qquad\psi(x)=a_2(a_1,x)-a_1(a_2,x)\]
Then \(\varphi\ne 0\) is skew, and \(\psi\)~is skew since \(\iprod{x}{\psi x}=0\) for all~\(x\). The kernel of~\(\varphi\) is the line determined by~\(a_1\cross a_2\), which is killed by~\(\psi\). By the previous problem, \(\psi=\lambda\varphi\) for some \(\lambda\in\R\). Substituting \(x=a_1+a_2\) into this equation, it follows that \(a_2-a_1=\lambda(a_2-a_1)\), so \(\lambda=1\) and \(\psi=\varphi\).
\end{proof}

\begin{exer}[5]
A linear transformation~\(\varphi\) satisfies \(\adj{\varphi}=\lambda\varphi\) for \(\lambda\in\R\) if and only if \(\varphi\)~is self-adjoint or skew.
\end{exer}
\begin{proof}
If \(\varphi\ne 0\) and \(\adj{\varphi}=\lambda\varphi\) for some \(\lambda\in\R\), then there exist vectors \(x,y\) such that \(\iprod{\varphi x}{y}\ne 0\) and
\[\iprod{\varphi x}{y}=\lambda\iprod{x}{\varphi y}=\lambda^2\iprod{\varphi x}{y}\]
so \(\lambda=\pm 1\) and \(\varphi\)~is self-adjoint or skew, respectively. The rest is obvious.
\end{proof}

\begin{exer}[6]
If \(\Phi\)~is a skew symmetric bilinear function in an oriented Euclidean 3-space, then there is a unique vector~\(a\) such that
\[\Phi(x,y)=\iprod{a}{x\cross y}\]
\end{exer}
\begin{proof}
By problem~2, the skew transformation~\(\varphi\) dual to~\(\Phi\) can be written in the form \(\varphi(x)=a\cross x\) for some vector~\(a\). Then
\[\Phi(x,y)=\iprod{\varphi x}{y}=\iprod{a\cross x}{y}=\iprod{a}{x\cross y}\]
Uniqueness is obvious.
\end{proof}
\begin{rmk}
This result shows that any function measuring 2-dimensional area in planes in 3-space actually measures 3-dimensional volume relative to some fixed vector in the space.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Geometrically, a rotation (and more generally an isometry) preserves length and angle. A proper rotation additionally preserves orientation, whereas an improper rotation reverses it.
\end{rmk}

\begin{exer}[2]
A linear transformation~\(\varphi\) is regular and preserves orthogonality (that is, \(\iprod{\varphi x}{\varphi y}=0\) whenever \(\iprod{x}{y}=0\)) if and only if \(\varphi=\lambda\tau\) where \(\lambda\ne 0\) and \(\tau\)~is a rotation.
\end{exer}
\begin{proof}
For the forward direction, let \(e_1,\ldots,e_n\) be an orthonormal basis. Then \(\varphi e_1,\ldots,\varphi e_n\) is an orthogonal basis. Also
\[\norm{\varphi e_i}^2-\norm{\varphi e_j}^2=\iprod{\varphi e_i-\varphi e_j}{\varphi e_i+\varphi e_j}=\iprod{\varphi(e_i-e_j)}{\varphi(e_i+e_j)}=0\]
since
\[\iprod{e_i-e_j}{e_i+e_j}=1-1=0\]
for all~\(i,j\). Let \(\lambda=\norm{\varphi e_i}>0\). Then clearly \(\tau=\lambda^{-1}\varphi\) is a rotation.

The reverse direction is trivial.
\end{proof}
\begin{rmk}
This proof is motivated by the geometrical fact that a rectangle is a square if and only if its diagonals are orthogonal.
\end{rmk}

\begin{exer}[5]
If \(\varphi:E\to E\) is a mapping such that \(\varphi(0)=0\) and
\[\norm{\varphi x-\varphi y}=\norm{x-y}\]
for all \(x,y\in E\), then \(\varphi\)~is linear.
\end{exer}
\begin{proof}
Substituting \(y=0\), we have \(\norm{\varphi x}=\norm{x}\) for all~\(x\), so
\[\iprod{\varphi x-\varphi y}{\varphi x-\varphi y}=\norm{x}^2-2\iprod{\varphi x}{\varphi y}+\norm{y}^2\]
On the other hand,
\[\iprod{\varphi x-\varphi y}{\varphi x-\varphi y}=\norm{\varphi x-\varphi y}^2=\norm{x-y}^2=\norm{x}^2-2\iprod{x}{y}+\norm{y}^2\]
Therefore
\[\iprod{\varphi x}{\varphi y}=\iprod{x}{y}\]
It now follows that
\[\norm{\varphi(x+y)-\varphi x-\varphi y}^2=\iprod{\varphi(x+y)-\varphi x-\varphi y}{\varphi(x+y)-\varphi x-\varphi y}=0\]
so \(\varphi(x+y)=\varphi x+\varphi y\). Similarly \(\varphi(\lambda x)=\lambda\varphi x\).
\end{proof}

\subsection*{\S~6}
\begin{rmk}
In subsection~8.21, \(j\)~is called the canonical ``complex structure'' on~\(E\) because it induces a complex vector space structure on the underlying set of~\(E\) in which scalar multiplication is defined by
\[(\alpha+\beta i)\mult x=\alpha x+\beta jx\qquad\alpha,\beta\in\R\]
\end{rmk}

\begin{rmk}
In subsection~8.21, to derive~(8.40) from (8.39) and~(8.41), first observe that \(j\varphi=\varphi j\). Indeed, for \(z\ne 0\), \(\iprod{jz}{z}=0\), so \(\iprod{\varphi jz}{\varphi z}=0\). On the other hand, \(\iprod{j\varphi z}{\varphi z}=0\). Since \(E\)~is a plane, it follows that \(j\varphi z=\lambda\varphi jz\) for some \(\lambda\in\R\). But
\begin{align*}
\lambda\norm{z}^2&=\lambda\norm{\varphi jz}^2\\
	&=\lambda\iprod{\varphi jz}{\varphi jz}\\
	&=\iprod{j\varphi z}{\varphi jz}\\
	&=\Delta(\varphi z,\varphi jz)\\
	&=\Delta(z,jz)\\
	&=\iprod{jz}{jz}\\
	&=\iprod{z}{z}=\norm{z}^2
\end{align*}
so \(\lambda=1\) and \(j\varphi z=\varphi j z\).

From this and~(8.39), it follows that
\[\varphi^{-1}x=x\mult\cos(-\Theta)+jx\mult\sin(-\Theta)=x\mult\cos\Theta-jx\mult\sin\Theta\]
where \(x\) and~\(\Theta\) are as in~(8.39). In~(8.41),
\[\Delta(x,\varphi y)=\iprod{jx}{\varphi y}=\iprod{\varphi^{-1}jx}{y}=\iprod{j\varphi^{-1}x}{y}=\Delta(\varphi^{-1}x,y)\]
so
\[\Delta(\varphi x,y)+\Delta(x,\varphi y)=\Delta(\varphi x+\varphi^{-1}x,y)=2\cos\Theta\mult\Delta(x,y)\]
and it follows that \(\cos\Theta=\tfrac{1}{2}\tr\varphi\). Similar reasoning shows that \(\sin\Theta=-\tfrac{1}{2}\tr(j\varphi)\), contrary to what the book says.
\end{rmk}

\begin{rmk}
In subsection~8.21, to make sense of the ``definition'' in~(8.43), fix \(x\ne 0\) and let \(0\le\Theta\le\pi\) be the angle between \(x\) and~\(\varphi x\). Fix an orientation in~\(E\) so that \(\Theta\)~is the \emph{oriented} angle between \(x\) and~\(\varphi x\). Then (8.40)~holds, so \(\cos\Theta=\tfrac{1}{2}\tr\varphi\). Clearly \(\Theta\)~is independent of~\(x\) and depends only on~\(\varphi\), so can be written as~\(\Theta(\varphi)\) and satisfies~(8.43).
\end{rmk}

\begin{rmk}
In subsection~8.22, why would we expect the rotation vector~\(u\) in~(8.45) to lie on the rotation axis? Since \(\psi\)~is skew, \(\Psi(x,y)=\iprod{\psi x}{y}\) measures oriented area in planes in~\(E\), and since \(\psi\)~kills off~\(E_1\), \(\Psi(x,y)=0\) if \(x\in E_1\) or \(y\in E_1\). But we know from previous results that \(\Psi\)~actually measures oriented \emph{volume} relative to~\(u\), so it follows that \(u\)~must lie in~\(E_1\).

Note that \(\psi\)~stabilizes \(F=\oc{E_1}\), and if \(\psi_1\)~denotes the restriction, then
\[\psi_1=\tfrac{1}{2}(\varphi_1-\adj{\varphi_1})=\tfrac{1}{2}(\varphi_1-\varphi_1^{-1})\]
where \(\varphi_1\)~denotes the restriction of~\(\varphi\). If \(F\)~is oriented by~\(E\) and by \(u\ne 0\), and \(j\)~is the induced canonical complex structure on~\(F\), then
\[\psi_1=j\mult\sin\Theta\]
where \(0<\Theta<\pi\) is the oriented angle of rotation. If \(x\in F\) is any unit vector, then
\[\norm{u}=\norm{u\cross x}=\norm{\psi_1 x}=\norm{jx}\sin\Theta=\sin\Theta\]
as expected.
\end{rmk}

\begin{rmk}
In subsection~8.24, for the proof of the first part of Proposition~I, let \(q_i=p_i-\lambda_i e\) where \(\lambda_i=\iprod{p_i}{e}\) for \(i=1,2\). Since \(q_1\) and~\(q_2\) each generate the same axis of rotation, either \(q_1=0=q_2\) in which case \(p_1=\pm e\) and \(p_2=\pm e\) and the result holds, or else \(q_1\ne 0\) and \(q_2=\alpha q_1\) for some \(\alpha\ne 0\). If \(\alpha>0\), then \(q_1\) and~\(q_2\) induce the same orientation in their orthogonal plane (in~\(E_1\)), so the oriented angle~\(\Theta\) of rotation in that plane is the same. It follows that
\[\lambda_1=\cos\frac{\Theta}{2}=\lambda_2\qquad\text{and}\qquad\norm{q_1}=\sin\frac{\Theta}{2}=\norm{q_2}\]
The second equation implies that \(\alpha=1\), so \(q_1=q_2\) and \(p_1=p_2\). On the other hand if \(\alpha<0\), then \(-p_2\)~is a unit quaternion inducing the same rotation as~\(p_1\) and its pure part~\(-q_2\) satisfies \(-q_2=(-\alpha)q_1\) with \(-\alpha>0\), so by the previous case \(p_1=-p_2\).
\end{rmk}

\begin{exer}[16]
If \(p\ne\pm e\) is a unit quaternion, then the rotation vector induced by~\(p\) is
\[u=2\lambda(p-\lambda e)\qquad\lambda=\iprod{p}{e}\]
\end{exer}
\begin{proof}
Let \(q=p-\lambda e\). By assumption, \(q\ne 0\) and \(u=\alpha q\) since \(q\) and~\(u\) both lie on the axis of rotation. If \(\alpha>0\), then \(q\) and~\(u\) both induce the same orientation on the orthogonal plane and
\[\alpha\norm{q}=\norm{u}=\sin\Theta=2\lambda\norm{q}\]
where \(\Theta\)~is the oriented angle of rotation. Therefore \(\alpha=2\lambda\) and the result holds. If \(\alpha<0\), then \(-p\)~induces the same rotation vector and satisfies the hypotheses of the previous case since \(u=(-\alpha)(-q)\) with \(-\alpha>0\), so \(-\alpha=2(-\lambda)\) and \(\alpha=2\lambda\) and the result holds. If \(\alpha=0\), then \(u=0\) and \(\Theta=\pi\), so \(\lambda=\cos(\pi/2)=0\) and the result holds.
\end{proof}

\begin{exer}[17]
Let \(p\ne\pm e\) be a unit quaternion. If \(F\)~denotes the plane generated by \(e\) and~\(p\), then the rotations \(\varphi x=px\) and \(\psi x=xp\) agree on~\(F\) and stabilize \(F\) and~\(\oc{F}\).
\end{exer}
\begin{proof}
The rotations agree on~\(e\) and~\(p\), hence on~\(F\). By definition of quaternion multiplication, \(p^2\in F\), so they also stabilize \(F\) and~\(\oc{F}\).
\end{proof}

\newpage
\section*{Chapter~XI}
\begin{rmk}
In parts of this chapter, it is implicitly assumed that unitary spaces have dimension \(n\ge 1\).
\end{rmk}

\begin{rmk}
A map \(\varphi:E\to F\) of complex vector spaces \(E,F\) is \emph{conjugate-linear} if
\[\varphi(\lambda x+\mu y)=\conj{\lambda}\varphi x+\conj{\mu}\varphi y\]
for all \(x,y\in E\) and \(\lambda,\mu\in\C\), where \(\conj{\lambda},\conj{\mu}\) denote the complex conjugates of \(\lambda,\mu\) respectively.
\end{rmk}

\subsection*{\S~1}
\begin{rmk}
A sesquilinear function \(\Phi(x,y)\) is linear in~\(x\) and conjugate-linear in~\(y\).
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
The identity map \(\kappa:F\to\conj{F}\) is conjugate-linear. A map \(\varphi:E\to F\) is conjugate-linear if and only if \(\kappa\varphi:E\to\conj{F}\) is linear. This yields a natural bijective correspondence between conjugate-linear maps \(E\to F\) and linear maps \(E\to\conj{F}\).
\end{rmk}

\begin{rmk}
If \(z\mapsto\conj{z}\) is a conjugation in~\(E\), then for \(\lambda=\alpha+i\beta\) with \(\alpha,\beta\in\R\),
\[\conj{\lambda z}=\conj{\alpha z+i\beta z}=\alpha\conj{z}-i\beta\,\conj{z}=\conj{\lambda}\,\conj{z}\]
Therefore a conjugation is just a conjugate-linear involution. By the previous remark, a conjugation can also be viewed as a linear map \(E\to\conj{E}\).
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
If \(E\)~is a unitary space, then there exists a conjugation in~\(E\). In fact, if \(z_1,\ldots,z_n\) is a basis of~\(E\) and \(F\)~is the real span of \(z_1,\ldots,z_n\), then \(F\)~is a real form of~\(E\) and the map \(z\mapsto\conj{z}\) defined by \(x+iy\mapsto x-iy\) for \(x,y\in F\) is a conjugation in the \emph{vector space}~\(E\). It is also a conjugation in the \emph{unitary space}~\(E\) since for \(z_1=x_1+iy_1\) and \(z_2=x_2+iy_2\) with \(x_j,y_j\in F\),
\[\iprod{z_1}{z_2}=\iprod{x_1}{x_2}+\iprod{y_1}{y_2}+i\left[\iprod{y_1}{x_2}-\iprod{x_1}{y_2}\right]\]
and
\[\iprod{\,\conj{z_1}}{\conj{z_2}\,}=\iprod{x_1}{x_2}+\iprod{y_1}{y_2}-i\left[\iprod{y_1}{x_2}-\iprod{x_1}{y_2}\right]\]
so \(\iprod{\,\conj{z_1}}{\conj{z_2}\,}=\conj{\iprod{z_1}{z_2}}\) as required.\footnote{See problem~2(i) in~\S~2.}
\end{rmk}

\begin{rmk}
If \(E,F\) are unitary spaces and \(\conj{E},\conj{F}\) the corresponding conjugate spaces, then \(E\)~is dual to~\(\conj{E}\) under the scalar product
\[\sprod{x}{x^*}=\iprod{x}{\kappa_E^{-1}x^*}\]
where \(\kappa_E:E\to\conj{E}\) is the identity map; similarly \(F\)~is dual to~\(\conj{F}\) under the scalar product
\[\sprod{y}{y^*}=\iprod{y}{\kappa_F^{-1}y^*}\]
where \(\kappa_F:F\to\conj{F}\) is the identity map.

If \(\varphi:E\to F\) is a linear map, then the dual map \(\varphi^*:\conj{E}\from\conj{F}\) satisfies
\[\iprod{\varphi x}{\kappa_F^{-1}y^*}=\sprod{\varphi x}{y^*}=\sprod{x}{\varphi^* y^*}=\iprod{x}{\kappa_E^{-1}\varphi^* y^*}\]
Taking \(y^*=\kappa_F y\) yields
\[\iprod{\varphi x}{y}=\iprod{x}{\kappa_E^{-1}\varphi^*\kappa_F y}\]
Therefore \(\adj{\varphi}=\kappa_E^{-1}\varphi^*\kappa_F\):
\begin{diagram}
E				&\lTo^{\adj{\varphi}}	&F\\
\dTo<{\kappa_E}	&						&\dTo>{\kappa_F}\\
\conj{E}		&\lTo_{\varphi^*}		&\conj{F}
\end{diagram}
This construction of the adjoint avoids using conjugations in \(E,F\) by using the conjugate spaces \(\conj{E},\conj{F}\) instead.

If \(E=F\), \(\Delta\ne 0\) is a determinant function in~\(E\), and \(\Delta^*\)~is the corresponding determinant function in~\(\conj{E}\) (11.6), then
\begin{align*}
\det\adj{\varphi}\mult\Delta(x_1,\ldots,x_n)&=\Delta(\adj{\varphi}x_1,\ldots,\adj{\varphi}x_n)\\
	&=\Delta(\kappa^{-1}\varphi^*\kappa x_1,\ldots,\kappa^{-1}\varphi^*\kappa x_n)\\
	&=\conj{\Delta^*(\varphi^*\kappa x_1,\ldots,\varphi^*\kappa x_n)}\\
	&=\conj{\det\varphi^*}\mult\conj{\Delta^*(\kappa x_1,\ldots,\kappa x_n)}\\
	&=\conj{\det\varphi}\mult\Delta(x_1,\ldots,x_n)
\end{align*}
where \(\kappa=\kappa_E\), so \(\det\adj{\varphi}=\conj{\det\varphi}\). This derivation of~(11.21) similarly avoids using conjugations in~\(E\).
\end{rmk}

\begin{rmk}
The mapping \(\varphi\mapsto\adj{\varphi}\) is conjugate-linear.
\end{rmk}

\begin{rmk}
If \(z\mapsto\conj{z}\) is a conjugation in~\(E\) and \(\Delta\)~is a determinant function in~\(E\), then
\[\conj{\Delta}(z_1,\ldots,z_n)=\conj{\Delta(\conj{z_1},\ldots,\conj{z_n})}\]
is also a determinant function in~\(E\), called the \emph{conjugate determinant function}. The proof of~(11.21) in the book uses one of these.
\end{rmk}

\begin{exer}[1]
If \(E\)~is a unitary space and \(\varphi:E\to E\) is linear, then
\[\Phi(x,y)=\iprod{\varphi x}{y}\]
is sesquilinear. Conversely, every sesquilinear function in~\(E\) can be uniquely represented in this way. The adjoint~\(\adj{\varphi}\) represents the adjoint~\(\adj{\Phi}\).
\end{exer}
\begin{proof}
The forward direction is trivial. If \(\Phi\)~is sesquilinear, then for any fixed vector~\(x\) the function
\[y\mapsto\conj{\Phi(x,y)}\]
is linear, so by the Riesz theorem (11.5) there is a unique vector~\(\varphi x\) such that
\[\conj{\Phi(x,y)}=\iprod{y}{\varphi x}=\conj{\iprod{\varphi x}{y}}\]
and therefore
\[\Phi(x,y)=\iprod{\varphi x}{y}\]
Clearly \(\varphi\)~is linear and is uniquely determined by~\(\Phi\). Also
\[\iprod{\adj{\varphi}x}{y}=\conj{\iprod{y}{\adj{\varphi}x}}=\conj{\iprod{\varphi y}{x}}=\conj{\Phi(y,x)}=\adj{\Phi}(x,y)\qedhere\]
\end{proof}
\begin{rmk}
This result shows that the linear transformation \(\varphi\) is naturally left-dual to the sesquilinear function~\(\Phi\). Observe that \(\Phi\)~is Hermitian if and only if \(\varphi\)~is Hermitian (self-adjoint) and \(\Phi\)~is skew-Hermitian if and only if \(\varphi\)~is skew-Hermitian.
\end{rmk}

\newpage
\section*{Chapter~XII}
\subsection*{\S~1}
\begin{rmk}
The pair \((\Gamma[t], t)\) is a universal (initial) object in the category of pointed algebras \((A,a)\), where \(A\) is an associative unital \(\Gamma\)-algebra with unit \(e\in A\) and distinguished element \(a\in A\). In this category, an arrow \(\varphi:(A,a)\to(B,b)\) is a unital \(\Gamma\)-algebra homomorphism \(\varphi:A\to B\) with \(\varphi a=b\). This result follows from the existence and uniqueness of the evaluation homomorphism \(\Gamma[t]\to A\).

Identifying the scalars \(\alpha\in\Gamma\) with the elements \(\alpha\mult e\in A\), this result shows that \(\Gamma[t]\)~is universal as an algebra obtained by adjoining a single element to~\(\Gamma\), in the sense that every such algebra can be obtained from~\(\Gamma[t]\) in a unique way through evaluation.
\end{rmk}

\begin{exer}[2]
Define the linear mapping \(\int:\Gamma[t]\to\Gamma[t]\) by
\[\int t^p=\frac{t^{p+1}}{p+1}\qquad(p\ge 0)\]
Then \(\int\)~is homogeneous (of degree~1) with \(d\after\int=\iota\), and is unique with these properties.
\end{exer}
\begin{proof}
If \(\int\)~satisfies the properties, then we must have
\[\int t^p=\frac{t^{p+1}}{p+1}+\alpha_p\qquad(\alpha_p\in\Gamma)\]
since \(d\after\int=\iota\). But \(\alpha_p=0\) for all~\(p\) since \(\int\)~is homogeneous. Finally, \(\int\)~is uniquely determined on~\(\Gamma[t]\) since \(\int\)~is linear and the~\(t^p\) (\(p\ge 0\)) form a basis of~\(\Gamma[t]\).
\end{proof}

\begin{exer}[3]
If \(\int:\Gamma[t]\to\Gamma[t]\) is the integration operator, then \(\int\after d=\iota-\varrho\), where \(\varrho:\Gamma[t]\to\Gamma\) is the scalar projection \(\varrho f=f(0)\). It follows that
\[\int fg'=fg-\varrho(fg)-\int gf'\]
\end{exer}
\begin{proof}
The first part is easily verified on the basis~\(t^p\) (\(p\ge 0\)). The second part then follows since
\[fg-\varrho(fg)=\int(fg)'=\int(f'g+fg')=\int f'g+\int fg'\qedhere\]
\end{proof}

\subsection*{\S~3}
\begin{rmk}
If \(A=0\), then the minimum polynomial of \(a=0\) is \(\mu=1\).
\end{rmk}

\begin{rmk}
In the proof of Proposition~I, to see why the elements \(1,\proj{t},\ldots,\proj{t}^{r-1}\) form a basis of \(\Gamma[t]/I_{\mu}\), first observe that every element of \(\Gamma[t]/I_{\mu}\) is of the form \(f(\proj{t})\) for some \(f\in\Gamma[t]\). Write \(f=g\mu+h\) where \(h=0\) or \(\deg h<r\). Then
\[f(\proj{t})=g(\proj{t})\mu(\proj{t})+h(\proj{t})=h(\proj{t})\]
since \(\mu(\proj{t})=0\), and clearly \(h(\proj{t})\)~is in the span of \(1,\proj{t},\ldots,\proj{t}^{r-1}\).

On the other hand, suppose
\[\alpha_0+\alpha_1\proj{t}+\cdots+\alpha_{r-1}\proj{t}^{r-1}=0\]
Define \(f=\alpha_0+\cdots+\alpha_{r-1}t^{r-1}\). Then \(f=0\) or \(\deg f<r\). But \(f\in\ker\pi=I_{\mu}\), so \(\mu\divides f\), which forces \(f=0\) and \(\alpha_0=\cdots=\alpha_{r-1}=0\). It follows that the elements \(1,\proj{t},\ldots,\proj{t}^{r-1}\) are linearly independent.
\end{rmk}

\newpage
\section*{Chapter~XIII}
\begin{rmk}
In parts of this chapter, it is implicitly assumed that the vector space \(E\)~has dimension \(n\ge 1\).
\end{rmk}

\subsection*{\S~1}
In the following remarks, \(f\in\Gamma[t]\).
\begin{rmk}
Why is it natural to apply polynomials to a linear transformation? If we think of a linear transformation \(\varphi:E\to E\) as being adjoined to the scalar transformations in~\(A(E;E)\), then we know that the structure of the resulting subalgebra is obtained from that of~\(\Gamma[t]\) in a unique way through evaluation \(f\mapsto f(\varphi)\), by the universal property of~\(\Gamma[t]\).\footnote{See the remark in chapter~XII, \S~1 above.} This fact makes it natural to apply polynomials to~\(\varphi\) in order to study the structure of~\(\varphi\).
\end{rmk}

\begin{rmk}
The results of this section are summarized in the following diagram, which shows that \(K\)~is a homomorphism from the lattice of polynomials in~\(\Gamma[t]\) under divisibility to the lattice of subspaces of~\(E\) under inclusion:
\begin{center}
\begin{tikzcd}[sep={4em,between origins},tips=false]
h &&&&&&&\\
&& \mu \ar[rrrr,maps to,tips=true,shorten=3em,bend left=15,"K"{yshift=0.5em}] &&&& K(\mu)=E & \\
& h\join\mu \ar[luu] \ar[ru] &&&& K(h)=K(h\join\mu) \ar[ru] && \\
&& f\meet g \ar[uu] &&&& K(f\meet g)=K(f)+K(g) \ar[uu] & \\
& f \ar[ru] && g \ar[lu] && K(f) \ar[ru] && K(g) \ar[lu] \\
&& f\join g \ar[lu] \ar[ru] &&&& K(f\join g)=K(f)\sect K(g) \ar[lu] \ar[ru] & \\
&& 1 \ar[u] &&&& K(1)=0 \ar[u] &
\end{tikzcd}
\end{center}
Recall from chapter~XII that \(f\join g\) denotes the greatest common divisor of \(f\) and~\(g\), while \(f\meet g\) denotes the least common multiple of \(f\) and~\(g\)---notation inspired by the lattice of \emph{ideals}.\footnote{See also problem~11.}
\end{rmk}

\begin{rmk}
If \(\varphi:E\to E\) is linear and \(E_1\)~is stable under~\(\varphi\), then \(E_1\)~is also stable under~\(f(\varphi)\) and
\[f(\varphi)_{E_1}=f(\varphi_{E_1})\]
Dually,
\[\proj{f(\varphi)}=f(\proj{\varphi})\]
\end{rmk}
\begin{proof}
Stabilization of~\(E_1\) is preserved by addition, scalar multiplication, and composition, so is preserved by~\(f\). It follows that \(f(\varphi)_{E_1}\) and~\(f(\varphi_{E_1})\) are both transformations of~\(E_1\) with the same action, so are equal. Finally,
\[\proj{f(\varphi)}(\proj{x})=\proj{f(\varphi)(x)}=f(\proj{\varphi})(\proj{x})\qedhere\]
\end{proof}

\begin{rmk}
It follows that if \(K_1(f)=\ker f(\varphi_{E_1})\), then
\[K_1(f)=K(f)\sect E_1\]
Dually, if \(\proj{K}(f)=\ker f(\proj{\varphi})\), then
\[\proj{K}(f)\supseteq\proj{K(f)}\]
The reverse inclusion does not hold in general. For a counterexample, consider \(f=t\) with \(\varphi:\R^2\to\R^2\) defined by \(\varphi(x,y)=(y,0)\) and \(E_1=\R\times 0\).
\end{rmk}

\begin{rmk}
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[f(\varphi_1\dsum\varphi_2)=f(\varphi_1)\dsum f(\varphi_2)\]
\end{rmk}
\begin{proof}
Write \(\varphi=\varphi_1\dsum\varphi_2\), so \(\varphi_{E_1}=\varphi_1\) and \(\varphi_{E_2}=\varphi_2\). Then
\[f(\varphi)=f(\varphi)_{E_1}\dsum f(\varphi)_{E_2}=f(\varphi_1)\dsum f(\varphi_2)\qedhere\]
\end{proof}

\begin{rmk}
It follows that
\[r(f(\varphi))=r(f(\varphi_1))+r(f(\varphi_2))\]
where \(r(\psi)\)~denotes the rank of~\(\psi\).
\end{rmk}

\begin{rmk}
If \(E,E^*\) are dual spaces and \(\varphi:E\to E\) and \(\varphi^*:E^*\from E^*\) are dual linear transformations, then
\[f(\varphi)^*=f(\varphi^*)\]
\end{rmk}
\begin{proof}
By the results of subsection~2.25.
\end{proof}

\begin{rmk}
It follows that \(f(\varphi)=0\) if and only if \(f(\varphi^*)=0\), so \(\mu_{\varphi^*}=\mu_{\varphi}\).
\end{rmk}

\begin{rmk}
In particular, if \(E\)~is an inner product space or a unitary space, then
\[\adj{f(\varphi)}=\conj{f}(\adj{\varphi})\]
where \(\conj{f}\)~is obtained from~\(f\) by conjugating the coefficients.\footnote{In the case of an inner product space, \(\conj{f}=f\).}
\end{rmk}
\begin{proof}
Let \(\conj{E}\)~be the conjugate space and \(\kappa:E\to\conj{E}\) the conjugate-linear identity map. Then
\[\adj{f(\varphi)}=\kappa^{-1}f(\varphi)^*\kappa=\kappa^{-1}f(\varphi^*)\kappa=\conj{f}(\kappa^{-1}\varphi^*\kappa)=\conj{f}(\adj{\varphi})\qedhere\]
\end{proof}

\begin{rmk}
It follows that \(f(\varphi)=0\) if and only if \(\conj{f}(\adj{\varphi})=0\), so \(\mu_{\adj{\varphi}}=\conj{\mu_{\varphi}}\).
\end{rmk}

\begin{rmk}
If \(\varphi:E\to E\) and \(\psi:F\to F\) are linear with
\[\psi=\alpha\after\varphi\after\alpha^{-1}\]
where \(\alpha:E\iso F\), then
\[f(\psi)=\alpha\after f(\varphi)\after\alpha^{-1}\]
\end{rmk}
\begin{proof}
Conjugation by~\(\alpha\) is an algebra isomorphism \(A(E;E)\iso A(F;F)\).
\end{proof}

\begin{rmk}
It follows that \(f(\psi)=0\) if and only if \(f(\varphi)=0\), so \(\mu_{\psi}=\mu_{\varphi}\).
\end{rmk}

\begin{rmk}
In particular if \(E=E_1\dsum E_2\) and \(\varphi:E\to E\) is given by \(\varphi=\varphi_1\dsum\varphi_2\) where \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\), then \(\mu_{\varphi_1}=\mu_{\proj{\varphi}}\) where \(\proj{\varphi}:E/E_2\to E/E_2\) is the induced transformation. Dually, \(\mu_{\varphi_2}=\mu_{\proj{\varphi}}\) where \(\proj{\varphi}:E/E_1\to E/E_1\) is induced.\footnote{See the conjugacy diagrams in chapter~II, \S~4 above.}
\end{rmk}

\begin{rmk}
If \(\varphi:E\to E\) is linear, then relative to a fixed basis of~\(E\)
\[M(f(\varphi))=f(M(\varphi))\]
\end{rmk}
\begin{proof}
\(M\)~is an algebra isomorphism.
\end{proof}

\begin{rmk}
Let \(\varphi:E\to E\) be linear. If \(\mu=\mu_{\varphi}=fg\) and
\[E=K(f)\dsum E_1\tag{1}\]
where \(E_1\)~is stable under~\(\varphi\), then \(f\) and~\(g\) are relatively prime and \(E_1=K(g)\).
\end{rmk}
\begin{proof}
We have
\[g(\varphi)E_1\subseteq K(f)\sect E_1=0\]
so
\[E_1\subseteq K(g)\tag{2}\]
It follows from (1) and~(2) that \(E=K(f\meet g)\), so \(\mu\divides f\meet g\). This implies \(fg=f\meet g\) and therefore \(f\) and~\(g\) are relatively prime. Now
\[E=K(f)\dsum K(g)\tag{3}\]
and it follows from (1), (2), and~(3) that \(E_1=K(g)\).
\end{proof}
\begin{rmk}
This result is a converse of Corollary~I to Proposition~II. The proof is essentially the same as those of Proposition~I and Theorem~V in \S~6.
\end{rmk}

\begin{exer}[2] Consider the linear transformations \(\varphi,\psi:\R^2\to\R^2\) defined by
\[\varphi(x,y)=(y,0)\qquad\text{and}\qquad\psi(x,y)=(0,y)\]
Then \(\varphi\psi=\varphi\) while \(\psi\varphi=0\), so
\[\mu_{\varphi\psi}=t^2\ne t=\mu_{\psi\varphi}\]
\end{exer}

\begin{exer}[3]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\mu_{\varphi_1\dsum\varphi_2}=\mu_{\varphi_1}\meet\mu_{\varphi_2}\]
\end{exer}
\begin{proof}
Write \(\varphi=\varphi_1\dsum\varphi_2\). By a remark above, \(f(\varphi)=f(\varphi_1)\dsum f(\varphi_2)\), so \(f(\varphi)=0\) if and only if \(f(\varphi_1)=0\) and \(f(\varphi_2)=0\). It follows that \(\mu_{\varphi}\)~is the least common multiple of \(\mu_{\varphi_1}\) and~\(\mu_{\varphi_2}\).
\end{proof}

\begin{exer}[5]
If \(\varphi:E\to E\) stabilizes~\(E_1\), and \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced transformations, then
\[\proj{\mu}\meet\mu_1\divides\mu\divides\proj{\mu}\mu_1\]
where \(\mu=\mu_{\varphi}\), \(\mu_1=\mu_{\varphi_1}\), and \(\proj{\mu}=\mu_{\proj{\varphi}}\).
\end{exer}
\begin{proof}
By a remark above, \(\mu(\varphi_1)=0\) and \(\mu(\proj{\varphi})=0\), so \(\mu_1\divides\mu\) and \(\proj{\mu}\divides\mu\). On the other hand,
\[\proj{\mu}\mu_1(\varphi)(x)=\mu_1(\varphi)(\proj{\mu}(\varphi)(x))\]
But \(\proj{\mu}(\varphi)(x)\)~is in~\(E_1\) since
\[\proj{\proj{\mu}(\varphi)(x)}=\proj{\proj{\mu}(\varphi)}(\proj{x})=\proj{\mu}(\proj{\varphi})(\proj{x})=0\]
so
\[\mu_1(\varphi)(\proj{\mu}(\varphi)(x))=\mu_1(\varphi_1)(\proj{\mu}(\varphi)(x))=0\]
It follows that \(\proj{\mu}\mu_1(\varphi)=0\), so \(\mu\divides\proj{\mu}\mu_1\).
\end{proof}

\begin{exer}[6]
The minimum polynomial~\(\mu\) of \(\varphi:E\to E\) can be constructed in the following way: select \(x_1\in E\) and let \(k_1\)~be least such that the vectors
\[x_1,\varphi x_1,\ldots,\varphi^{k_1}x_1\tag{1}\]
are linearly dependent, with
\[\lambda_0 x_1+\lambda_1\varphi x_1+\cdots+\lambda_{k_1}\varphi^{k_1}x_1=0\]
where the~\(\lambda_i\) are not all zero, so in particular \(\lambda_{k_1}\ne 0\). Define the polynomial
\[f_1=\lambda_0+\lambda_1 t+\cdots+\lambda_{k_1}t^{k_1}\]
If the vectors~(1) do not span~\(E\), select \(x_2\in E\) not in their span and construct the corresponding polynomial~\(f_2\). Repeat this procedure until \(E\)~is exhausted. Then \(\mu\)~is the (monic) least common multiple of the~\(f_i\).
\end{exer}
\begin{proof}
Let \(E_i\)~be the span of the vectors
\[x_i,\varphi x_i,\ldots,\varphi^{k_i-1}x_i\tag{2}\]
By construction, (2)~is a basis of~\(E_i\) with \(\varphi^{k_i}x_i\in E_i\), and we have the direct sum \(E=\sum_i E_i\). The minimum polynomial of the restriction \(\varphi_i:E_i\to E_i\) is just \(\lambda_{k_i}^{-1}f_i\), so the result follows from problem~3.
\end{proof}
\begin{rmk}
The subspaces~\(E_i\) are cyclic, so this procedure just produces a cyclic decomposition of~\(E\), from which the minimum polynomial is easily computed.\footnote{See Proposition~IV and its corollary in \S~3.}
\end{rmk}

\begin{exer}[8]
If \(E\)~is an inner product space and \(\varphi:E\to E\) is a rotation, then the coefficients~\(\alpha_i\) of the minimum polynomial~\(\mu\) of~\(\varphi\) satisfy
\[\alpha_i=\varepsilon\alpha_{k-i}\qquad k=\deg\mu\]
where \(\varepsilon=\pm 1\).
\end{exer}
\begin{proof}
Since \(\adj{\varphi}=\varphi^{-1}\), we have \(\mu_{\varphi^{-1}}=\mu\) by a remark above, so
\[\alpha_0+\alpha_1\varphi^{-1}+\cdots+\alpha_k\varphi^{-k}=\mu(\varphi^{-1})=0\tag{1}\]
Multiplying both sides of~(1) by~\(\varphi^k\) yields
\[\alpha_0\varphi^k+\alpha_1\varphi^{k-1}+\cdots+\alpha_k=\um(\varphi)=0\tag{2}\]
where \(\um=\sum_i\alpha_{k-i}t^i\). It follows from~(2) that \(\mu\divides\um\), and in fact \(\alpha_0\mu=\um\). Since \(\alpha_0^2=\alpha_k=1\), the result holds with \(\varepsilon=\alpha_0\).
\end{proof}

\begin{exer}[9]
The minimum polynomial~\(\mu\) of a self-adjoint transformation of a unitary space has real coefficients.
\end{exer}
\begin{proof}
By a remark above, \(\conj{\mu}=\mu\).
\end{proof}

\begin{exer}[12]
If \(\varphi:E\to E\) is regular, then \(\varphi^{-1}=f(\varphi)\) for some \(f\in\Gamma[t]\).
\end{exer}
\begin{proof}
Let \(\mu\)~be the minimum polynomial of~\(\varphi\) and \(\alpha=\mu(0)\). Since \(\varphi\)~is regular, \(0\)~is not an eigenvalue of~\(\varphi\), so \(\alpha\ne0\). Write
\[\alpha^{-1}\mu=1-tf\qquad f\in\Gamma[t]\tag{1}\]
Evaluating both sides of~(1) at~\(\varphi\) yields \(\varphi f(\varphi)=\iota\) and therefore \(\varphi^{-1}=f(\varphi)\).
\end{proof}

\subsection*{\S~2}
\begin{rmk}
If the minimum polynomial of \(\varphi:E\to E\) has the prime decomposition
\[\mu=f_1^{k_1}\cdots f_r^{k_r}\]
and \(E\)~has the generalized eigenspace decomposition
\[E=E_1\dsum\cdots\dsum E_r\]
define
\[g_i=f_1^{k_1}\cdots\delete{f_i^{k_i}}\cdots f_r^{k_r}\]
and
\[F_i=E_1\dsum\cdots\dsum\delete{E_i}\dsum\cdots\dsum E_r\]
Then \(f_i^{k_i}(\varphi)\)~kills \(E_i\) and induces an isomorphism on~\(F_i\), while \(g_i(\varphi)\)~kills \(F_i\) and induces an isomorphism on~\(E_i\).

More generally if \(K\subseteq\{1,\ldots,r\}\), define
\[f_K=\prod_{i\in K}f_i^{k_i}\]
and
\[E_K=\sum_{i\in K}E_i\qquad E_{\delete{K}}=\sum_{i\not\in K}E_i\]
Then \(f_K(\varphi)\)~kills \(E_K\) and induces an isomorphism on~\(E_{\delete{K}}\).
\end{rmk}

\begin{rmk}
In Proposition~I, if \(\pi_i\) and~\(\pi^*_i\) are the projection operators associated with the generalized eigenspace decompositions in \(E\) and~\(E^*\), then
\[(\pi_i)^*=\pi^*_i\]
\end{rmk}
\begin{proof}
From~(13.24), it follows that
\[\sprod{\pi^*_i x^*}{x}=\sprod{\pi^*_i x^*}{\pi_i x}=\sprod{x^*}{\pi_i x}\qedhere\]
\end{proof}
\begin{rmk}
If \(g_i\in\Gamma[t]\) with \(g_i(\varphi)=\pi_i\) (subsection~13.5), then
\[g_i(\varphi^*)=g_i(\varphi)^*=(\pi_i)^*=\pi^*_i\]
Alternatively, the fact that \(g_i(\varphi^*)=\pi^*_i\) follows from the fact that \(g_i\)~depends only on the common minimum polynomial of~\(\varphi^*\) and~\(\varphi\), and the argument above can be run in reverse to obtain~(13.24).\footnote{See problems 7 and~8.}
\end{rmk}

\begin{exer}[1]
The minimum polynomial~\(\mu\) of \(\varphi:E\to E\) is completely reducible if and only if every nonzero subspace of~\(E\) stable under~\(\varphi\) contains an eigenvector of~\(\varphi\).
\end{exer}
\begin{proof}
If \(F\ne 0\) is stable, then by~(13.22)
\[F=\sum_i F\sect E_i\]
where the~\(E_i\) are the generalized eigenspaces of~\(\varphi\). Fix~\(i\) with \(F\sect E_i\ne 0\). If \(\mu\)~is completely reducible, then \(E_i=K((t-\lambda)^k)\) for some \(\lambda\in\Gamma\) and \(k\ge 1\). Choose \(x\in F\sect E_i\) with \(x\ne 0\) and let \(1\le l\le k\) be least with \((\varphi-\lambda\iota)^l x=0\). Then it follows that \((\varphi-\lambda\iota)^{l-1}x\in F\) is an eigenvector.

Conversely, suppose \(f\)~is monic irreducible with \(\deg f\ge 1\) and \(f\divides\mu\). Let \(F=K(f)\). Then \(F\ne 0\) is stable and so contains an eigenvector by assumption. If \(\lambda\)~is the associated eigenvalue, then \((t-\lambda)\divides\mu_f=f\), so \(f=t-\lambda\). It follows that \(\mu\)~is completely reducible.
\end{proof}
\begin{rmk}
This is true in particular if \(\Gamma\)~is algebraically closed (for example \(\Gamma=\C\)).
\end{rmk}

\begin{exer}[2]
If the minimum polynomial~\(\mu\) of \(\varphi:E\to E\) is completely reducible, then there is a basis of~\(E\) with respect to which the matrix of~\(\varphi\) is triangular:
\[\begin{pmatrix}
\lambda_1&&0\\
&\ddots&\\
*&&\lambda_n
\end{pmatrix}\]
\end{exer}
\begin{proof}
By induction on~\(\dim E\).

Since \(E\ne 0\) is stable, there is an eigenvector~\(x_1\) with \(\varphi x_1=\lambda_1 x_1\). Let \(E_1=\gen{x_1}\) and consider \(\proj{\varphi}:E/E_1\to E/E_1\) with minimum polynomial~\(\proj{\mu}\). Since \(\proj{\mu}\divides\mu\), \(\proj{\mu}\)~is completely reducible, and by induction there is a basis \(\proj{x_2},\ldots,\proj{x_n}\) of~\(E/E_1\) with \(\proj{\varphi}(\proj{x_k})\in\gen{\proj{x_2},\ldots,\proj{x_k}}\) for \(2\le k\le n\). It follows that \(x_1,\ldots,x_n\) is a basis of~\(E\) with \(\varphi x_k\in\gen{x_1,\ldots,x_k}\) for \(1\le k\le n\).
\end{proof}
\begin{rmk}
The diagonal entries of this matrix are just the eigenvalues of~\(\varphi\).
\end{rmk}

\begin{exer}[9]
Let \(F\)~be a subspace of~\(E\) stable under \(\varphi:E\to E\) and consider the induced mappings \(\varphi_F:F\to F\) and \(\proj{\varphi}:E/F\to E/F\). Let \(E=\sum_i E_i\) be the generalized eigenspace decomposition of~\(E\) under~\(\varphi\).
\begin{enumerate}
\item[(a)] The generalized eigenspace decomposition of~\(F\) under~\(\varphi_F\) is
\[F=\sum_i F\sect E_i\]
\item[(b)] The generalized eigenspace decomposition of~\(E/F\) under~\(\proj{\varphi}\) is
\[E/F=\sum_i E_i/F\]
Moreover, \(E_i/F\iso E_i/(F\sect E_i)\).
\item[(c)] If \(\pi_i\), \(\pi^F_i\), and \(\proj{\pi}_i\) are the projection operators in \(E\), \(F\), and \(E/F\) associated with the decompositions, then the diagram
\begin{diagram}
F				&\rTo^j	&E			&\rTo^{\rho}&E/F\\
\dTo<{\pi^F_i}	&		&\dTo{\pi_i}&			&\dTo>{\proj{\pi}_i}\\
F				&\rTo_j	&E			&\rTo_{\rho}&E/F
\end{diagram}
commutes, where \(j\) and~\(\rho\) are the canonical injection and projection. Moreover,
\[\pi^F_i=(\pi_i)_F\qquad\text{and}\qquad\proj{\pi}_i=\proj{\pi_i}\]
so \(\pi^F_i\) and \(\proj{\pi}_i\) are unique making the diagram commute.
\item[(d)] If \(g_i\in\Gamma[t]\) with \(g_i(\varphi)=\pi_i\), then
\[g_i(\varphi_F)=\pi^F_i\qquad\text{and}\qquad g_i(\proj{\varphi})=\proj{\pi}_i\]
\end{enumerate}
\end{exer}
\begin{proof}
For~(a), we already know from~(13.22) that
\[F=\sum_i F\sect E_i\tag{1}\]
Let
\[F=\sum_i F_i\tag{2}\]
be the generalized eigenspace decomposition of~\(F\) under~\(\varphi_F\). Since \(\mu_{\varphi_F}\divides\mu_{\varphi}\), we may assume that \(F_i=K_F(f_i^{j_i})\) and \(E_i=K(f_i^{k_i})\) for some~\(f_i\) with \(j_i\le k_i\). Then
\[F_i=K_F(f_i^{j_i})=F\sect K(f_i^{j_i})\subseteq F\sect K(f_i^{k_i})=F\sect E_i\tag{3}\]
It follows from (1), (2), and~(3) that \(F_i=F\sect E_i\), establishing~(a).

For~(b), it follows from~(1) that
\[E/F=\sum_i E_i/F\tag{4}\]
is a direct sum decomposition of~\(E/F\), and it is immediate that \(E_i/F\)~is stable under~\(\proj{\varphi}\) since \(E_i\)~is stable under~\(\varphi\). Let
\[E/F=\sum_i(E/F)_i\tag{5}\]
be the generalized eigenspace decomposition of~\(E/F\) under~\(\proj{\varphi}\). Since \(\mu_{\proj{\varphi}}\divides\mu_{\varphi}\), we may assume that \((E/F)_i=\proj{K}(f_i^{m_i})\) with \(m_i\le k_i\). Now
\[E_i/F=K(f_i^{k_i})/F\subseteq\proj{K}(f_i^{k_i})\]
so if \(\mu_i\)~denotes the minimum polynomial of~\(\proj{\varphi}_{E_i/F}\), then \(\mu_i\divides f_i^{k_i}\). On the other hand, \(\mu_i\divides\mu_{\proj{\varphi}}\), so we must have \(\mu_i\divides f_i^{m_i}\) and therefore
\[E_i/F\subseteq\proj{K}(f_i^{m_i})=(E/F)_i\tag{6}\]
It now follows from (4), (5), and~(6) that \((E/F)_i=E_i/F\). Finally, the restriction to~\(E_i\) of the canonical projection induces the isomorphism \(E_i/F_i\iso E_i/F\).

For~(c), commutativity of the left square in the diagram follows from~(a) and commutativity of the right square follows from~(b). The rest of~(c) follows from the diagram.

Finally, (d)~follows immediately from~(c).
\end{proof}

\begin{exer}[10]
If the minimum polynomial~\(\mu\) of \(\varphi:E\to E\) is completely reducible, then \(\mu\divides\chi\) where \(\chi\)~is the characteristic polynomial of~\(\varphi\).
\end{exer}
\begin{proof}
First consider the case \(\mu=(t-\lambda)^k\). By problem~2, there is a basis of~\(E\) with respect to which the matrix of~\(\varphi\) has the form
\[\begin{pmatrix}
\lambda&&0\\
&\ddots&\\
*&&\lambda
\end{pmatrix}\]
It follows immediately that \((\varphi-\lambda\iota)^n=0\) where \(n=\dim E\), so \(\mu\divides(t-\lambda)^n\) and therefore \(k\le n\). Moreover, \(\chi=(-1)^n(t-\lambda)^n\), so \(\mu\divides\chi\).

In the general case, take the prime decomposition
\[\mu=(t-\lambda_1)^{k_1}\cdots(t-\lambda_r)^{k_r}\]
and let \(E=\sum_i E_i\) be the corresponding generalized eigenspace decomposition. Let \(\varphi_i:E_i\to E_i\) be the restriction with minimum polynomial \(\mu_i=(t-\lambda_i)^{k_i}\) and characteristic polynomial \(\chi_i\). Then \(\mu_i\divides\chi_i\) by the previous case, so
\[\mu=\mu_1\cdots\mu_r\divides\chi_1\cdots\chi_r=\chi\qedhere\]
\end{proof}
\begin{rmk}
This is just a special case of the Cayley-Hamilton theorem.\footnote{See Theorem~II in \S~5.}
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
In Proposition~I, the vector~\(a\) is intuitively ``hard to kill'', and therefore generates a cyclic subspace that is as large as possible.\footnote{See Proposition~V.}
\end{rmk}

\begin{rmk}
A vector space~\(E\) is cyclic with generator \(a\in E\) under \(\varphi:E\to E\) if and only if \(E\)~is the smallest subspace of~\(E\) containing~\(a\) and stable under~\(\varphi\).
\end{rmk}

\begin{rmk}
To prove Proposition~III, just recall from the proof of Proposition~II that \(\sigma_a\)~induces an isomorphism \(\proj{\sigma_a}:\Gamma[t]/I_{\mu}\iso E\) defined by \(\proj{\sigma_a}(\proj{f})=\sigma_a(f)\). Since \(1,\proj{t},\ldots,\proj{t}^{m-1}\) is a basis of \(\Gamma[t]/I_{\mu}\) and \(\sigma_a(t^i)=\varphi^i(a)\), it follows that
\[a,\varphi(a),\ldots,\varphi^{m-1}(a)\]
is a basis of~\(E\).
\end{rmk}

\begin{rmk}
In the proof of the corollary to Proposition~IV, to see that \(\mu(\varphi)=0\), first observe that
\[\mu(\varphi)(a_0)=\varphi^n(a_0)-\sum_{\nu=0}^{n-1}\alpha_{\nu}\varphi^{\nu}(a_0)=\varphi(a_{n-1})-\sum_{\nu=0}^{n-1}\alpha_{\nu}a_{\nu}=0\]
Now if \(\mu(\varphi)(a_{\nu})=0\) for some \(0\le\nu\le n-2\), then
\[\mu(\varphi)(a_{\nu+1})=\mu(\varphi)(\varphi(a_{\nu}))=\varphi(\mu(\varphi)(a_{\nu}))=\varphi(0)=0\]
By induction, \(\mu(\varphi)(a_{\nu})=0\) for all \(0\le\nu\le n-1\), so \(\mu(\varphi)=0\).
\end{rmk}

\begin{rmk}
If \(E\) and~\(E^*\) are dual spaces and \(\varphi:E\to E\) and \(\varphi^*:E^*\from E^*\) are dual transformations, then \(E\)~is cyclic with respect to~\(\varphi\) if and only if \(E^*\)~is cyclic with respect to~\(\varphi^*\).
\end{rmk}
\begin{proof}
By Theorem~I, if \(E\)~is cyclic then
\[\dim E^*=\dim E=\deg\mu_{\varphi}=\deg\mu_{\varphi^*}\]
so \(E^*\)~is cyclic. The converse follows by symmetry.
\end{proof}

\subsection*{\S~4}
\begin{rmk}
If \(E=\sum_i E_i\) is the generalized eigenspace decomposition of~\(E\) and \(E_i=\sum_j E_{ij}\) is a cyclic decomposition of~\(E_i\), then \(E=\sum_{i,j}E_{ij}\) is an irreducible decomposition of~\(E\). Conversely, an irreducible decomposition of~\(E\) yields an irreducible (and hence cyclic) decomposition of each~\(E_i\).\footnote{See the remarks at the end of subsection~13.17.}
\end{rmk}

\begin{rmk}
In subsection~13.14, observe that
\begin{align*}
\sum_{\nu=0}^{p-1}\alpha_{\nu}f^{i-1}(\varphi)\varphi^{\nu}e+f(\varphi)^{i-1}\varphi^pe&=f(\varphi)^{i-1}\sum_{\nu=0}^p\alpha_{\nu}\varphi^{\nu}e\\
	&=f(\varphi)^{i-1}f(\varphi)e\\
	&=f(\varphi)^ie
\end{align*}
\end{rmk}

\begin{rmk}
In subsection~13.17, observe from the formula for~\(r(\psi^j)\) that we can first determine~\(N_k\) from~\(r(\psi^{k-1})\), then determine~\(N_{k-1}\) from \(r(\psi^{k-2})\) and~\(N_k\), then determine successively \(N_{k-2},\ldots,N_1\).
\end{rmk}

\begin{rmk}
Proposition~II shows that two linear transformations are conjugate if and only if they have the same Jordan canonical matrix. The matrix is therefore a particularly simple representative for the conjugacy class.

The proof of case~II can be simplified by noting that \(\dim E_i=\dim F_i\) follows from the fact that \(r(f_i(\varphi_i)^j)=r(f_i(\psi_i)^j)\) for all \(j\ge 1\).
\end{rmk}

\begin{rmk}
In the proof of Corollary~II to Proposition~II, it follows from the results of subsection~13.20 that the common characteristic polynomial of the induced restrictions of \(\varphi\) and~\(\psi\) to \(E_i\) and~\(F_i\) is just \(f_i^{m_i}\).\footnote{See the remarks in \S~5 below.}
\end{rmk}

\begin{exer}[5]
If \(\varphi:E\to E\) stabilizes~\(E_1\), and \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced transformations, then \(E\)~is cyclic if and only if (i) \(E_1\)~is cyclic, (ii) \(E/E_1\)~is cyclic, and (iii) \(\mu=\mu_1\proj{\mu}\), where \(\mu=\mu_{\varphi}\), \(\mu_1=\mu_{\varphi_1}\), and \(\proj{\mu}=\mu_{\proj{\varphi}}\).
\end{exer}
\begin{proof}
If \(E\)~is cyclic, then \(\sigma_a:\Gamma[t]\to E\) is surjective for some \(a\in E\). It follows that \(\pi\sigma_a:\Gamma[t]\to E/E_1\) is also surjective, where \(\pi:E\to E/E_1\) is the canonical projection. But
\[(\pi\sigma_a)(f)=\pi(f(\varphi)(a))=f(\proj{\varphi})(\proj{a})=\sigma_{\proj{a}}(f)\]
so \(\sigma_{\proj{a}}=\pi\sigma_a\) is surjective, and therefore \(E/E_1\)~is cyclic. Now
\[\deg\mu_1\le\dim E_1=\dim E-\dim E/E_1=\deg\mu-\deg\proj{\mu}\le\deg\mu_1\]
since \(\mu\divides\mu_1\proj{\mu}\). It follows that \(\dim E_1=\deg\mu_1\) so \(E_1\)~is cyclic, and \(\deg\mu=\deg\mu_1\proj{\mu}\) so \(\mu=\mu_1\proj{\mu}\).

Conversely, if \(E_1\) and~\(E/E_1\) are cyclic and \(\mu=\mu_1\proj{\mu}\), then
\[\dim E=\dim E_1+\dim E/E_1=\deg\mu_1+\deg\proj{\mu}=\deg\mu_1\proj{\mu}=\deg\mu\]
so \(E\)~is cyclic.
\end{proof}

\begin{rmk}
If \(\varphi=\varphi_1\dsum\varphi_2\) with \(\mu_2=\mu_{\varphi_2}\), then \(\proj{\mu}=\mu_2\), so if \(E\)~is cyclic it follows that \(\mu=\mu_1\mu_2\). Since also \(\mu=\mu_1\meet\mu_2\), this implies that \(\mu_1\) and~\(\mu_2\) are relatively prime.
\end{rmk}

\begin{exer}[6]
Let \(E=F_1\dsum\cdots\dsum F_s\) and \(\varphi=\varphi_1\dsum\cdots\dsum\varphi_s\) where \(\varphi_j:F_j\to F_j\) with \(\mu=\mu_{\varphi}\) and \(\mu_j=\mu_{\varphi_j}\).
\begin{enumerate}
\item[(a)] \(E\)~is cyclic if and only if each~\(F_j\) is cyclic and the~\(\mu_j\) are relatively prime.
\item[(b)] If \(E\)~is cyclic, then each \(F_j\)~is a sum of generalized eigenspaces for~\(\varphi\).
\item[(c)] If \(E\)~is cyclic and
\[a=a_1+\cdots+a_s\qquad a_j\in F_j\]
is any vector in~\(E\), then \(a\)~generates \(E\) if and only if each~\(a_j\) generates \(F_j\).
\end{enumerate}
\end{exer}
\begin{proof}
For~(a), if \(E\)~is cyclic then by problem~5 each~\(F_j\) is cyclic and (by induction on~\(s\))
\[\mu_1\meet\cdots\meet\mu_s=\mu=\mu_1\cdots\mu_s\]
so the \(\mu_j\)~are relatively prime. Conversely, if each~\(F_j\) is cyclic and the~\(\mu_j\) are relatively prime, then
\begin{align*}
\dim E&=\dim F_1+\cdots+\dim F_s\\
	&=\deg\mu_1+\cdots+\deg\mu_s\\
	&=\deg\mu_1\cdots\mu_s\\
	&=\deg\mu_1\meet\cdots\meet\mu_s\\
	&=\deg\mu
\end{align*}
so \(E\)~is cyclic.

For~(b), since \(\mu=\mu_1\cdots\mu_s\) and the~\(\mu_j\) are relatively prime, the generalized eigenspaces of~\(\varphi_j\) in~\(F_j\) are generalized eigenspaces of~\(\varphi\) in~\(E\). It follows that the projection operator associated with~\(F_j\) is a polynomial \(g_j(\varphi)\) in~\(\varphi\).

For~(c), if \(a\)~generates \(E\) and \(x\in F_j\), then there is \(f\in\Gamma[t]\) with
\[x=f(\varphi)a=f(\varphi_1)a_1+\cdots+f(\varphi_s)a_s=f(\varphi_j)a_j\]
so \(a_j\)~generates \(F_j\). Conversely, if each~\(a_j\) generates \(F_j\) and
\[x=x_1+\cdots+x_s\qquad x_j\in F_j\]
is a vector in~\(E\), there are \(f_j\in\Gamma[t]\) with \(x_j=f_j(\varphi_j)a_j\). Define \(f=\sum f_jg_j\). Then
\[f(\varphi)a=\sum f_j(\varphi)g_j(\varphi)a=\sum f_j(\varphi)a_j=\sum x_j=x\]
so \(a\)~generates \(E\).
\end{proof}

\begin{exer}[7]
If \(\varphi:E\to E\) stabilizes~\(F\), and \(\varphi_F:F\to F\) and \(\proj{\varphi}:E/F\to E/F\) are the induced transformations, then \(E\)~is irreducible if and only if (i) \(E/F\)~is irreducible, (ii) \(F\)~is irreducible, and (iii) \(\mu_{\varphi_F}=f^k\), \(\mu_{\proj{\varphi}}=f^l\), and \(\mu_{\varphi}=f^{k+l}\) where \(f\)~is irreducible.
\end{exer}
\begin{proof}
By Theorem~I and problem~5.
\end{proof}

\begin{exer}[8]
Let \(E\)~be irreducible with respect to \(\varphi\), with \(\mu_{\varphi}=f^k\) (\(f\)~irreducible).
\begin{enumerate}
\item[(a)] The \(k\)~subspaces \(K(f),\ldots,K(f^k)\) are the only nontrivial stable subspaces of~\(E\).
\item[(b)] \(\im f(\varphi)^m=K(f^{k-m})\) for \(0\le m\le k\).
\end{enumerate}
\end{exer}
\begin{proof}
We may assume that \(f\)~is monic with \(\deg f=p\). For~(a), we know that
\[0=K(1)\subset K(f)\subset\cdots\subset K(f^k)=E\]
are stable subspaces of~\(E\). If \(F\ne 0\) is a stable subspace of~\(E\), then \(\mu_F=f^m\) for some \(1\le m\le k\) and hence \(F\subseteq K(f^m)\). Since \(F\) and~\(K(f^m)\) are both cyclic,
\[\dim F=pm=\dim K(f^m)\]
so \(F=K(f^m)\).

For~(b), \(\im f(\varphi)^m\) is stable and
\begin{align*}
\dim\im f(\varphi)^m&=\dim E-\dim\ker f(\varphi)^m\\
	&=pk-pm\\
	&=p(k-m)\\
	&=\dim\ker f(\varphi)^{k-m}
\end{align*}
so we must have \(\im f(\varphi)^m=K(f^{k-m})\) by~(a).
\end{proof}

\subsection*{\S~5}
\begin{rmk}
The proof of Lemma~I is essentially the same as the proof that the product of cyclic groups of orders \(m\) and~\(n\) is cyclic if and only if \(m\) and~\(n\) are relatively prime.

In the proof, if \(Y_i\)~is the cyclic subspace generated by~\(y_i\), then \(Y_i=Y\sect F_i\). The contradiction is that \(Y=Y_1\dsum Y_2\) is a cyclic subspace but the orders of \(y_1\) and~\(y_2\) under~\(f_1(\varphi)\) are not relatively prime.\footnote{See also problem~6 in \S~4 above.}
\end{rmk}

\begin{rmk}
The minimum polynomial \(\mu\) and the characteristic polynomial \(\chi\) of a linear transformation \(\varphi:E\to E\) have the same prime factors.
\end{rmk}
\begin{proof}
Let \(E=\sum_i F_i\) be a cyclic decomposition of~\(E\) and \(\varphi_i:F_i\to F_i\) the induced restriction of~\(\varphi\) with minimum polynomial \(\mu_i\) and characteristic polynomial \(\chi_i\). Then
\[\mu=\bigmeet\mu_i\]
and
\[\chi=\prod\chi_i=\pm\prod\mu_i\]
by the cyclic case of the Cayley-Hamilton theorem (Lemma~II). It follows that the prime factors of \(\mu\) and~\(\chi\) are the prime factors of the~\(\mu_i\).\footnote{It also follows from this proof that \(\mu\divides\chi\).}
\end{proof}

\begin{rmk}
If \(\varphi:E\to E\), let
\[\chi=f_1^{m_1}\cdots f_r^{m_r}\qquad\text{and}\qquad\mu=f_1^{k_1}\cdots f_r^{k_r}\quad(k_i\le m_i)\]
be the prime decompositions of the characteristic and minimum polynomials, by the Cayley-Hamilton theorem. If
\[E=E_1\dsum\cdots\dsum E_r\]
is the generalized eigenspace decomposition of~\(E\) and \(\varphi_i:E_i\to E_i\) the induced restriction of~\(\varphi\) with minimum polynomial \(\mu_i\) and characteristic polynomial \(\chi_i\), then
\[\chi_i=f_i^{m_i}\qquad\text{and}\qquad\mu_i=f_i^{k_i}\]
\end{rmk}
\begin{proof}
We already know that \(\mu_i=f_i^{k_i}\), and it follows from the previous remark that \(f_i\)~does not divide~\(\chi_j\) if \(i\ne j\). On the other hand \(\chi=\chi_1\cdots\chi_r\), so we must have \(\chi_i=f_i^{m_i}\).
\end{proof}
\begin{rmk}
This result shows that the algebraic multiplicity of an eigenvalue~\(\lambda\) (the number of times it occurs as a root of the characteristic polynomial) is the dimension of the generalized eigenspace corresponding to~\(\lambda\), which is greater than or equal to the geometric multiplicity (the dimension of the eigenspace).
\end{rmk}

\begin{rmk}
In subsection~13.20, we see that
\[\mu_{\varphi}(t)=\mu_{\varphi-\lambda\iota}(t-\lambda)\]
Substituting \(t+\lambda\) for~\(t\), we obtain
\[\mu_{\varphi-\lambda\iota}(t)=\mu_{\varphi}(t+\lambda)\]
In particular, it follows that \(\mu_{\varphi}(\lambda)\)~is the constant term of~\(\mu_{\varphi-\lambda\iota}\), and \(\mu_{\varphi-\lambda\iota}(-\lambda)\)~is the constant term of~\(\mu_{\varphi}\).
\end{rmk}

\begin{rmk}
In subsections 13.21 and~13.22, we have
\[C^2(\varphi)=\Gamma(\varphi)\subseteq C(\varphi)=C(\Gamma(\varphi))=C^3(\varphi)\]
\end{rmk}

\begin{exer}[1]
If \(\varphi:E\to E\), let \(\mu=f_1^{k_1}\cdots f_r^{k_r}\) be the prime decomposition of the minimum polynomial with \(p_i=\deg f_i\) and \(E=\sum_i E_i\) the generalized eigenspace decomposition. Denote by \(N_{ij}\) the number of irreducible subspaces of~\(E_i\) of dimension \(p_ij\) for \(1\le j\le k_i\) and set
\[l_i=\sum_{j=1}^{k_i}jN_{ij}\]
Then the characteristic polynomial of~\(\varphi\) is \(\chi=f_1^{l_1}\cdots f_r^{l_r}\).
\end{exer}
\begin{proof}
By a remark above, we know that the characteristic polynomial of the induced restriction \(\varphi_i:E_i\to E_i\) is \(\chi_i=f_i^{m_i}\) where
\[p_im_i=\deg\chi_i=\dim E_i=\sum_{j=1}^{k_i}p_ijN_{ij}=p_i l_i\]
so \(m_i=l_i\) and
\[\chi=\chi_1\cdots\chi_r=f_1^{l_1}\cdots f_r^{l_r}\qedhere\]
\end{proof}

\begin{exer}[2]
\(E\)~is cyclic with respect to~\(\varphi\) if and only if \(\chi_{\varphi}=\pm\mu_{\varphi}\).
\end{exer}
\begin{proof}
The forward direction follows from Lemma~II, and the reverse from
\[\dim E=\deg\chi_{\varphi}=\deg\mu_{\varphi}\qedhere\]
\end{proof}

\begin{exer}[3]
If \(\varphi:E\to E\), and \(E=\sum_j F_j\) is a decomposition of~\(E\) as a direct sum of stable subspaces \(F_j\), then each~\(F_j\) is a sum of generalized eigenspaces for~\(\varphi\) if and only if each~\(F_j\) is stable under~\(C(\varphi)\).
\end{exer}
\begin{proof}
If each~\(F_j\) is a sum of generalized eigenspaces, then by Theorem~I the projection operators~\(\rho_j\) are in~\(\Gamma(\varphi)\). If \(\psi\in C(\varphi)\), then \(\psi\)~commutes with~\(\rho_j\), so
\[\psi F_j=\psi\rho_j F_j=\rho_j\psi F_j\subseteq F_j\]
Conversely, if each~\(F_j\) is stable under~\(C(\varphi)\) and \(\psi\in C(\varphi)\), then for \(x=\sum_j x_j\) with \(x_j\in F_j\), we have
\[\rho_j\psi x=\psi x_j=\psi\rho_j x\]
so \(\rho_j\)~commutes with~\(\psi\). Since \(\psi\)~was arbitrary, \(\rho_j\in C^2(\varphi)\). But \(C^2(\varphi)=\Gamma(\varphi)\) by Theorem~III, so \(\rho_j\in\Gamma(\varphi)\) and each~\(F_j\) is a sum of generalized eigenspaces by Theorem~I.
\end{proof}

\begin{exer}[4]
Let \(\varphi:E\to E\) and let \(P\)~be the set of projection operators in~\(C(\varphi)\).
\begin{enumerate}
\item[(a)] \(P=\{0,\iota\}\) if and only if \(E\)~is irreducible.
\item[(b)] \(P\subseteq C^2(\varphi)\) if and only if \(E\)~is cyclic.
\end{enumerate}
\end{exer}
\begin{proof}
By Proposition~I, \(P\)~consists of the projection operators for stable direct sum decompositions of~\(E\). For~(a), the condition is therefore equivalent to the existence of only trivial stable decompositions. For~(b), since \(C^2(\varphi)=\Gamma(\varphi)\) by Theorem~III, the condition is equivalent to the existence of only generalized eigenspace decompositions by Theorem~I, which is equivalent to \(E\)~being cyclic by problem~6 in \S~4 above.
\end{proof}

\begin{exer}[5]
Let \(\varphi:E\to E\).
\begin{enumerate}
\item[(a)] \(C^3(\varphi)=C(\varphi)\)
\item[(b)] \(C^2(\varphi)=C(\varphi)\) if and only if \(E\)~is cyclic.
\end{enumerate}
\end{exer}
\begin{proof}
By a remark above and problem~4.
\end{proof}

\subsection*{\S~6}
\begin{rmk}
If \(\varphi:E\to E\) is nilpotent of degree~\(k\) and \(E\)~is cyclic with respect to~\(\varphi\), then \(a\) generates \(E\) if and only if \(k\)~is least with \(\varphi^k a=0\), which is true if and only if \(\varphi^{k-1}a\ne 0\).
\end{rmk}
\begin{proof}
If \(a\) generates \(E\), then the vectors
\[a,\varphi a,\ldots,\varphi^{k-1}a\tag{1}\]
form a basis of~\(E\) and are therefore nonzero, while \(\varphi^k a=0\). Conversely, if \(k\)~is least with \(\varphi^k a=0\) and the vectors (1) are linearly dependent, there are scalars \(\alpha_0,\ldots,\alpha_{k-1}\) not all zero with
\[\alpha_0a+\alpha_1\varphi a+\cdots+\alpha_{k-1}\varphi^{k-1}a=0\tag{2}\]
Let \(j\)~be least with \(\alpha_j\ne 0\). Applying \(\varphi^{k-j-1}\) to~(2) yields
\[\alpha_j\varphi^{k-1}a=0\]
which is a contradiction.
\end{proof}

\begin{rmk}
Semisimplicity of self-adjoint, skew, and isometric transformations in real and complex inner product spaces ultimately follows from the fact that if a transformation stabilizes a subspace, then the dual transformation stabilizes the orthogonal complement of the subspace.
\end{rmk}

\begin{rmk}
In the proof of Theorem~I, Proposition~I is used as a lemma. On the other hand, Proposition~I follows as a corollary of Theorem~I by the remarks at the end of subsection~13.23.
\end{rmk}

\begin{rmk}
In the proof of Theorem~IV, in the reduction of the infinite case to the finite case, let \(S\)~be a finite set of generators for~\(A\). Since \(S\)~is finite, there is a finite set \(T\subseteq\{\varphi_{\alpha}\}\) such that \(S\)~is contained in the subalgebra \(A'\subseteq A\) generated by~\(T\). By the finite case, \(T\)~is semisimple, so \(A'\)~is semisimple. It follows from Theorem~V that \(S\)~consists of semisimple transformations. By the finite case again, \(S\)~is semisimple, so \(A\)~is semisimple, so \(\{\varphi_{\alpha}\}\)~is semisimple.
\end{rmk}

\begin{exer}[1]
If \(\varphi:E\to E\) is nilpotent and \(N_{\lambda}\)~is the number of subspaces of dimension~\(\lambda\) in an irreducible decomposition of~\(E\), then
\[\dim\ker\varphi=\sum_{\lambda}N_{\lambda}\]
\end{exer}
\begin{proof}
From subsection~13.17,
\[r(\varphi)=\sum_{\lambda}(\lambda-1)N_{\lambda}=\dim E-\sum_{\lambda}N_{\lambda}\]
so
\[\sum_{\lambda}N_{\lambda}=\dim E-\dim\im\varphi=\dim\ker\varphi\qedhere\]
\end{proof}

\begin{exer}[3]
Let \(\varphi:E\to E\) and \(\varphi^*:E^*\from E^*\) be dual. If \(\varphi\)~is nilpotent of degree~\(k\) and \(E\)~is cyclic with respect to~\(\varphi\), then \(\varphi^*\)~is nilpotent of degree~\(k\) and \(E^*\)~is cyclic with respect to~\(\varphi^*\). If \(a\) generates \(E\), then \(a^*\) generates \(E^*\) if and only if
\[\sprod{a^*}{\varphi^{k-1}a}\ne0\]
\end{exer}
\begin{proof}
We know \(\mu_{\varphi^*}=\mu_{\varphi}=t^k\), so \(\varphi^*\)~is nilpotent of degree~\(k\), and
\[\dim E^*=\dim E=k\]
so \(E^*\)~is cyclic with respect to~\(\varphi^*\).\footnote{See the remark in \S~4 above.}

By a remark above, \(a^*\) generates \(E^*\) if and only if \((\varphi^*)^{k-1}a^*\ne0\). Since
\[\sprod{(\varphi^*)^{k-1}a^*}{\varphi^j a}=\sprod{a^*}{\varphi^{k+j-1}a}=0\tag{1}\]
for all \(j\ge 1\) and the \((\varphi^j a)_{j\ge 0}\) form a basis of~\(E\), it follows that \((\varphi^*)^{k-1}a^*=0\) if and only if (1)~holds for \(j=0\).
\end{proof}

\begin{exer}[4]
A linear transformation is diagonalizable if and only if it is semi\-simple and its minimum polynomial is completely reducible, or equivalently if its minimum polynomial is the product of distinct monic linear factors.
\end{exer}
\begin{proof}
A linear transformation is diagonalizable if and only if its generalized eigenspace decomposition is an eigenspace decomposition, which is true if and only if the minimum polynomial has the indicated form. The rest follows from Theorem~I.
\end{proof}

\begin{exer}[5]
If \(\varphi,\psi:E\to E\) are commuting diagonalizable transformations, they are simultaneously diagonalizable (that is, there exists a basis of~\(E\) which diagonalizes both \(\varphi\) and~\(\psi\)).
\end{exer}
\begin{proof}
Let
\[\sum_i E_i=E=\sum_j F_j\]
be the generalized eigenspace decompositions for \(\varphi\) and~\(\psi\) respectively, which are eigenspace decompositions by diagonalizability of \(\varphi\) and~\(\psi\). It follows from commutativity of \(\varphi\) and~\(\psi\) and the results of subsection~13.21 that there is an induced decomposition
\[E=\sum_{i,j}E_i\sect F_j\]
A basis of~\(E\) obtained by joining bases of the subspaces \(E_i\sect F_j\) diagonalizes both \(\varphi\) and~\(\psi\).
\end{proof}

\begin{rmk}
It follows from this problem and the previous problem that if \(\varphi\) and~\(\psi\) are commuting semisimple transformations over the field of complex numbers (which is algebraically closed), then \(\varphi+\psi\) and~\(\varphi\psi\) are semisimple.
\end{rmk}

\begin{exer}[6]
Let \(E\)~be a complex vector space and \(\varphi:E\to E\). Let \(E=\sum_i E_i\) be the generalized eigenspace decomposition of~\(E\) and \(\pi_i:E\to E\) the corresponding projection operators. Assume that the minimum polynomial of the induced transformation \(\varphi_i:E_i\to E_i\) is \(\mu_i=(t-\lambda_i)^{k_i}\). Then the semisimple part of~\(\varphi\) is
\[\varphi_S=\sum_i\lambda_i\pi_i\]
\end{exer}
\begin{proof}
Let \(\iota_i:E_i\to E_i\) be the identity map and write
\[\varphi_i=(\varphi_i-\lambda_i\iota_i)+\lambda_i\iota_i\]
Note that \(\varphi_i-\lambda_i\iota_i\) is nilpotent, \(\lambda_i\iota_i\)~is semisimple, and the two transformations commute. Since
\[\varphi=\sum_i\varphi_i=\sum_i(\varphi_i-\lambda_i\iota_i)+\sum_i\lambda_i\iota_i\]
and direct sums preserve nilpotency, semisimplicity, and commutativity, the result now follows from the uniqueness of the Jordan-Chevalley decomposition (Theorem II).
\end{proof}

\begin{exer}[7]
Let \(E\)~be an \(n\)-dimensional complex vector space and \(\varphi:E\to E\) have eigenvalues \(\lambda_1,\ldots,\lambda_n\) (not necessarily distinct). If \(f\in\Gamma[t]\), then \(f(\varphi)\)~has the eigenvalues \(f(\lambda_1),\ldots,f(\lambda_n)\).
\end{exer}
\begin{proof}
Fix a basis of~\(E\) with
\[M(\varphi)=\begin{pmatrix}
\lambda_1&&0\\
&\ddots&\\
*&&\lambda_n
\end{pmatrix}\]
By direct computation,
\[M(f(\varphi))=f(M(\varphi))=\begin{pmatrix}
f(\lambda_1)&&0\\
&\ddots&\\
*&&f(\lambda_n)
\end{pmatrix}\]
from which the result follows.
\end{proof}

\begin{exer}[8] Consider the transformations \(\varphi,\psi:\R^2\to\R^2\) defined by
\[\varphi(x,y)=(y,0)\qquad\text{and}\qquad\psi(x,y)=(y,x)\]
The only 1-dimensional subspace stable under~\(\varphi\) is the line \(y=0\), while the only 1-dimensional subspaces stable under~\(\psi\) are the lines \(y=\pm x\), so the set \(\{\varphi,\psi\}\) is semisimple but \(\varphi\)~is not. Note that \(\varphi\) and~\(\psi\) do not commute since, for example
\[\varphi\psi(1,0)=(1,0)\ne(0,0)=\psi\varphi(1,0)\]
\end{exer}
\begin{rmk}
Conversely, let \(\varphi\)~be reflection in the line \(y=0\) along the line \(x=0\), and let \(\psi\)~be reflection in the line \(y=0\) along the line \(y=x\). Then \(\varphi\) and~\(\psi\) are semisimple but the set \(\{\varphi,\psi\}\) is not.
\end{rmk}

\subsection*{\S~7}
\begin{rmk}
In Theorems I and~II, the definition of \emph{homothetic} must be loosened slightly to allow for possibly negative scalars.

In the proof of Theorem~I, to see that \(\tau\)~is a \emph{proper} rotation, let \(E=\sum_i F_i\) be an irreducible decomposition. The minimum polynomial \(\mu_i\) of the restriction \(\varphi_i:F_i\to F_i\) must be \(\mu_i=t^2+\alpha t+\beta\), so the Jordan canonical matrix of~\(\varphi_i\) is just
\[\begin{pmatrix}
0&1\\
-\beta&-\alpha
\end{pmatrix}\]
and therefore \(\det\varphi_i=\beta>0\). It follows that \(\det\varphi>0\) and \(\det\tau>0\).
\end{rmk}

\begin{exer}[2]
If \(E\)~is an inner product space, then \(\varphi:E\to E\) is normal if and only if \(\adj{\varphi}=f(\varphi)\) for some \(f\in\R[t]\).
\end{exer}
\begin{proof}
If \(\adj{\varphi}=f(\varphi)\), then \(\adj{\varphi}\varphi=\varphi\adj{\varphi}\), so \(\varphi\)~is normal. Conversely, if \(\varphi\)~is normal let
\[E=E_1\dsum\cdots\dsum E_r\tag{1}\]
be the orthogonal generalized eigenspace decomposition of~\(E\) and \(\varphi_i:E_i\to E_i\) the induced homothetic restriction (Theorem~I). Write \(\varphi_i=\lambda_i\tau_i\) where \(\lambda_i\in\R\) and \(\tau_i\)~is a rotation. We know \(\tau_i^{-1}=f_i(\tau_i)\) for some \(f_i\in\R[t]\).\footnote{See problem~12 in \S~1 above.} By subsection~8.5, we have the direct sum
\[\adj{\varphi}=\sum\adj{\varphi_i}=\sum\lambda_i\adj{\tau_i}=\sum\lambda_i\tau_i^{-1}=\sum\lambda_i f_i(\tau_i)\tag{2}\]
Define \(g_i\in\R[t]\) by
\[g_i(t)=\begin{cases}
0&\text{if }\lambda_i=0\\
\lambda_if_i(\lambda_i^{-1}t)&\text{if }\lambda_i\ne0
\end{cases}\]
Let \(\pi_i=h_i(\varphi)\) be the \(i\)-th projection operator for~(1), where \(h_i\in\R[t]\). It follows from~(2) that
\[\adj{\varphi}=\sum g_i(\varphi)h_i(\varphi)\]
Setting \(f=\sum g_ih_i\), we have \(\adj{\varphi}=f(\varphi)\).
\end{proof}
\begin{rmk}
In the case of a unitary space, \(\varphi\) and~\(\adj{\varphi}\) are both diagonalizable by the spectral theorem\footnote{See subsection~11.10.} and hence simultaneously diagonalizable.\footnote{See problem~5 in \S~6 above.} By Lagrange interpolation, there is a polynomial \(f\in\C[t]\) with \(f(M(\varphi))=M(\adj{\varphi})\), so \(f(\varphi)=\adj{\varphi}\).
\end{rmk}

% Multilinear algebra
\newpage
\part*{Multilinear Algebra}
\section*{Chapter~1}
\subsection*{\S~1}
\begin{rmk}
If \(\lambda^1\lambda^4-\lambda^2\lambda^3=0\), we want \(\xi^1,\xi^2,\eta^1,\eta^2\) with
\[\lambda^1=\xi^1\eta^1\qquad\lambda^2=\xi^1\eta^2\qquad\lambda^3=\xi^2\eta^1\qquad\lambda^4=\xi^2\eta^2\]
If \(\lambda^1=0\), then \(\lambda^2\lambda^3=\lambda^1\lambda^4=0\), so \(\lambda^2=0\) or \(\lambda^3=0\).
\begin{itemize}[itemsep=0pt]
\item If \(\lambda^2=0\), we take \(\xi^1=0\), \(\xi^2=1\), \(\eta^1=\lambda^3\), and \(\eta^2=\lambda^4\).
\item If \(\lambda^3=0\), we take \(\xi^1=\lambda^2\), \(\xi^2=\lambda^4\), \(\eta^1=0\), and \(\eta^2=1\).
\end{itemize}
If \(\lambda^1\ne 0\) and \(\lambda^2=0\), then \(\lambda^4=0\) and we take \(\xi^1=\lambda^1\), \(\xi^2=\lambda^3\), \(\eta^1=1\), and \(\eta^2=0\). If \(\lambda^1\ne 0\) and \(\lambda^2\ne 0\), we take \(\xi^1=1\), \(\xi^2=\lambda^3/\lambda^1=\lambda^4/\lambda^2\), \(\eta^1=\lambda^1\), and \(\eta^2=\lambda^2\).
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In the construction of the first induced bilinear map~\(\widetilde{\varphi}\), note that for each \(y\in F\) the linear map \(\varphi(-,y):E\to G\) sends \(E_1\) into~\(G_1\) and hence induces a linear map \(\proj{\varphi}(-,y):E/E_1\to G/G_1\) by \(\proj{\varphi}(\rho x,y)=\pi\varphi(x,y)\):
\begin{diagram}
E			&\rTo^{\varphi(-,y)}		&G\\
\dTo<{\rho}	&							&\dTo>{\pi}\\
E/E_1		&\rTo_{\proj{\varphi}(-,y)}	&G/G_1
\end{diagram}
Since \(\proj{\varphi}(-,y)\) depends linearly on~\(y\), \(\proj{\varphi}\)~is bilinear and we define \(\widetilde{\varphi}=\proj{\varphi}\).

In the construction of the second induced bilinear map~\(\widetilde{\varphi}\), note that the linear map \(y\mapsto\proj{\varphi}(-,y)\) kills~\(F_1\), so it factors through~\(\sigma\):
\begin{diagram}[nohug]
F				&\rTo^{y\mapsto\proj{\varphi}(-,y)}			&L(E/E_1;G/G_1)\\
\dTo<{\sigma}	&\ruTo>{\sigma y\mapsto\proj{\varphi}(-,y)}	&\\
F/F_1			&											&
\end{diagram}
This allows us to define the bilinear map \(\widetilde{\varphi}(\rho x,\sigma y)=\proj{\varphi}(\rho x,y)=\pi\varphi(x,y)\).
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
The claim in problem~5(b) is false because it implies, for example, that any two inner products in~\(\R^2\) agree on orthogonality, which is false. The claim holds if and only if \(\psi\)~preserves linear relations satisfied by~\(\varphi\).
\end{rmk}

\begin{exer}[7]
If \(E,E^*\) are finite-dimensional dual spaces and \(\Phi:E^*\times E\to\Gamma\) is a bilinear function such that
\[\Phi(\tau^{*-1}x^*,\tau x)=\Phi(x^*,x)\]
for every pair of dual automorphisms, then there is \(\lambda\in\Gamma\) such that
\[\Phi(x^*,x)=\lambda\sprod{x^*}{x}\]
\end{exer}
\begin{proof}
Let \(\varphi:E\to E\) be the linear transformation with \(\Phi(x^*,x)=\sprod{x^*}{\varphi x}\). If \(\tau\)~is an automorphism of~\(E\), then since \((\tau^{-1})^*=(\tau^*)^{-1}\),
\[\sprod{x^*}{\tau^{-1}\varphi\tau(x)}=\sprod{\tau^{*-1}x^*}{\varphi\tau(x)}=\Phi(\tau^{*-1}x^*,\tau x)=\Phi(x^*,x)=\sprod{x^*}{\varphi x}\]
It follows that \(\tau^{-1}\varphi\tau=\varphi\). Since \(\tau\)~was arbitrary, \(\varphi=\lambda\iota\) for some \(\lambda\in\Gamma\), from which the result follows.
\end{proof}

\subsection*{\S~4}
\begin{rmk}
The tensor product \(E\tprod F\) is a universal (initial) object in the category of ``vector spaces with bilinear maps of \(E\times F\) into them''. In this category, the objects are bilinear maps \(E\times F\to G\), and the arrows are linear maps \(G\to H\) which respect the bilinear maps:
\begin{diagram}
E\times F	&\rTo	&H\\
\dTo		&\ruTo	&\\
G			&		&
\end{diagram}
Every object \(E\times F\to G\) in this category can be obtained from the tensor product \(\tprod:E\times F\to E\tprod F\) in a unique way. This is why \(\tprod\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(E\tprod F\) satisfy only those relations required to make \(E\tprod F\) into a vector space and to make \(\tprod\)~bilinear. By category theoretic abstract nonsense, \(E\tprod F\)~is unique up to isomorphism.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Lemma~1.5.1 generalizes the consequence of linear independence to tensor products other than scalar multiplication.

We provide an alternative proof. For a linear function \(g\in L(F)\), consider the bilinear map \(E\times F\to E\) defined by \((x,y)\mapsto g(y)x\). By the universal property of the tensor product, there is a linear map \(h:E\tprod F\to E\) with \(h(x\tprod y)=g(y)x\). Now
\[0=h(\sum a_i\tprod b_i)=\sum h(a_i\tprod b_i)=\sum g(b_i)a_i\]
By linear independence of the~\(a_i\), it follows that \(g(b_i)=0\) for all~\(i\). Since \(g\)~was arbitrary, it follows that \(b_i=0\) for all~\(i\).

Note \(h=\iota\tprod g:E\tprod F\to E\tprod\Gamma\) (see \S~16).
\end{rmk}

\begin{rmk}
Lemma~1.5.2 generalizes the existence and uniqueness of a representation relative to a basis.
\end{rmk}

\begin{rmk}
If \(z=\sum_{i=1}^n x_i\tprod y_i\) with \(x_i\in E\) and \(y_i\in F\), and \(\{x_1,\ldots,x_m\}\) is a maximal linearly independent subset of \(\{x_1,\ldots,x_n\}\) of size \(m\le n\), then
\[z=\sum_{i=1}^m x_i\tprod y_i'\qquad\text{where}\qquad y_i'-y_i\in\gen{y_{m+1},\ldots,y_n}\]
\end{rmk}
\begin{proof}
Write
\[x_j=\sum_{i=1}^m\lambda_{ji}x_i\qquad j=m+1,\ldots,n\qquad(\lambda_{ji}\in\Gamma)\]
Then
\begin{align*}
z&=\sum_{i=1}^m x_i\tprod y_i+\sum_{j=m+1}^n\Bigl(\sum_{i=1}^m\lambda_{ji}x_i\Bigr)\tprod y_j\\
	&=\sum_{i=1}^m x_i\tprod y_i+\sum_{i=1}^m x_i\tprod\Bigl(\sum_{j=m+1}^n\lambda_{ji} y_j\Bigr)\\
	&=\sum_{i=1}^m x_i\tprod\Bigl(y_i+\sum_{j=m+1}^n\lambda_{ji} y_j\Bigr)
\end{align*}
Now take \(y_i'=y_i+\sum_{j=m+1}^n\lambda_{ji} y_j\).
\end{proof}
\begin{rmk}
Lemma~1.5.3 follows from this remark, which makes clear that a tensor representation of minimal length must consist of linearly independent vectors.
\end{rmk}

\subsection*{\S~8}
\begin{rmk}
In the proof of Proposition~1.8.1, note that if \(f\)~is injective, it has a left inverse \(g:G\to E\tprod F\) with \(g\after f=\iota\). If \(\psi:E\times F\to K\) is bilinear, there is a linear map \(h:E\tprod F\to K\) with
\[\psi=h\after\tprod=h\after\iota\after\tprod=h\after g\after f\after\tprod=h\after g\after\varphi\]
So \(\psi\)~factors through~\(\varphi\). Since \(\psi\)~was arbitrary, \(\varphi\)~satisfies \(\medtprod_2\).
\end{rmk}

\begin{exer}[3]
If \(S\) and~\(T\) are sets, then \(C(S\times T)\iso C(S)\tprod C(T)\).
\end{exer}
\begin{proof}
By the universal property of the free space, there is a unique linear map \(\varphi:C(S\times T)\to C(S)\tprod C(T)\) with \(\varphi(s,t)=s\tprod t\) for \(s\in S\) and \(t\in T\):
\begin{diagram}[nohug]
S\times T	&\rTo^{i\times i}	&C(S)\times C(T)\\
\dTo<i		&					&\dTo>{\tprod}\\
C(S\times T)&\rTo>{\varphi}		&C(S)\tprod C(T)
\end{diagram}
Observe that \(\varphi\)~is injective since
\[0=\varphi\bigl(\,\sum_i\lambda_i(s_i,t_i)\bigr)=\sum_i\lambda_i\,s_i\tprod t_i\]
implies \(\lambda_i=0\) by linear independence of the distinct \(s_i\) in~\(C(S)\) and \(t_i\) in~\(C(T)\) (1.5.1). Also \(\varphi\)~is surjective since
\[\bigl(\,\sum_i\lambda_i s_i\bigr)\tprod\bigl(\,\sum_j\mu_j t_j\bigr)=\sum_{i,j}\lambda_i\mu_j\,s_i\tprod t_j=\sum_{i,j}\lambda_i\mu_j\,\varphi(s_i,t_j)\]
and the elements on the left generate \(C(S)\tprod C(T)\).
\end{proof}

\begin{exer}[4]
If \(a\tprod b\ne 0\), then \(a\tprod b=a'\tprod b'\) if and only if \(a'=\lambda a\) and \(b'=\lambda^{-1}b\) for some \(\lambda\ne 0\).
\end{exer}
\begin{proof}
For the forward direction, note all the vectors are nonzero by bilinearity of~\(\tprod\). Now
\[a\tprod b+a'\tprod(-b')=0\]
so we must have \(a'=\lambda a\) for some \(\lambda\ne 0\) (1.5.1). It follows that
\[a\tprod(b-\lambda b')=0\]
so \(b-\lambda b'=0\) and \(b'=\lambda^{-1}b\).

The reverse direction follows from bilinearity of~\(\tprod\).
\end{proof}

\subsection*{\S~10}
\begin{rmk}
To obtain the tensor product of \(E/E_1\) and~\(F/F_1\), just take the tensor product of \(E\) and~\(F\) and then kill off all the products by elements of~\(E_1\) and by elements of~\(F_1\).
\end{rmk}

\subsection*{\S~11}
\begin{rmk}
The tensor product operation is bilinear on \emph{spaces}:
\[\bigl(\bigdsum_{\alpha}E_{\alpha}\bigr)\tprod\bigl(\bigdsum_{\beta}F_{\beta}\bigr)=\bigdsum_{\alpha,\beta}E_{\alpha}\tprod F_{\beta}\]
In particular, \(E\tprod 0=0=0\tprod E\).
\end{rmk}

\subsection*{\S~12}
\begin{rmk}
In the proof of~(1.2), the idea is that \(E\iso\bigdsum_{\alpha}E_{\alpha}\) and \(F\iso\bigdsum_{\beta}F_{\beta}\), so
\[E\tprod F\iso\bigl(\bigdsum_{\alpha}E_{\alpha}\bigr)\tprod\bigl(\bigdsum_{\beta}F_{\beta}\bigr)=\bigdsum_{\alpha,\beta}E_{\alpha}\tprod F_{\beta}\]
by the result for external direct sums in the previous subsection. Observe that \(h=f\tprod g\) (see \S~16).
\end{rmk}

\subsection*{\S~15}
\begin{rmk}
By~(1.7), \emph{the intersection of tensor products is the tensor product of the intersections}. Observe that (1.4) and~(1.5) are special cases of~(1.7).

In the proof of~(1.4), \(u_{\beta}=v_{\beta}\) follows immediately from Lemma~1.5.2. In the proof of~(1.5), if \(z\in(E_1\tprod F)\sect(E\tprod F_1)\), then in particular \(z=x+y\) with \(x\in E_1\tprod F_1\) and \(y\in E_1\tprod F'\). Now \(y=z-x\in E\tprod F_1\), so
\[y\in(E\tprod F_1)\sect(E\tprod F')=E\tprod(F_1\sect F')=E\tprod 0=0\]
by~(1.4). Hence \(y=0\) and \(z=x\in E_1\tprod F_1\). Note that this argument makes~(1.6) superfluous.
\end{rmk}

\begin{rmk}
If \(\sum_{i=1}^r x_i\tprod y_i=\sum_{j=1}^s x_j'\tprod y_j'\) and the~\(x_i\) are linearly independent, then the~\(y_i\) are in the span of the~\(y_j'\).
\end{rmk}
\begin{proof}
By induction on~\(s\). If the vectors in \(\{x_i\}\union\{x_j'\}\) are linearly independent, then \(y_i=0\) for all~\(i\) (1.5.1), so the result holds trivially. Otherwise, since the~\(x_i\) are linearly independent, we may assume (relabeling if necessary) that
\[x_s'=\sum_{i=1}^r\lambda_i x_i+\sum_{j=1}^{s-1}\mu_j x_j'\qquad(\lambda_i,\mu_j\in\Gamma)\]
By bilinearity of the tensor product, it follows that
\[\sum_{i=1}^r x_i\tprod(y_i-\lambda_i y_s')=\sum_{j=1}^{s-1}x_j'\tprod(y_j'+\mu_j y_s')\]
By induction, the \(y_i-\lambda_iy_s'\) are in the span of the \(y_j'+\mu_jy_s'\), so the~\(y_i\) are in the span of the~\(y_j'\).
\end{proof}

\begin{rmk}
If \(\sum_{i=1}^r x_i\tprod y_i=\sum_{j=1}^s x_j'\tprod y_j'\) and the \(x_i\) and~\(y_i\) are respectively linearly independent, then \(r\le s\).
\end{rmk}
\begin{proof}
By the previous remark and the elementary fact that the size of a linearly independent set is at most the size of a spanning set in a subspace.
\end{proof}

\begin{exer}[1]
If \(\sum_{i=1}^r x_i\tprod y_i=\sum_{j=1}^s x_j'\tprod y_j'\) and the \(x_i\), \(y_i\), \(x_j'\), and \(y_j'\) are each respectively linearly independent, then \(r=s\).
\end{exer}
\begin{proof}
By the previous remark, \(r\le s\) and \(s\le r\).
\end{proof}
\begin{rmk}
It follows from this result and the proof of Lemma~1.5.3 that the tensor representations of minimal length are precisely the representations by linearly independent vectors. These representations are \emph{not} unique, as already seen in problem~1.8.4.
\end{rmk}

\begin{exer}[2]
A bilinear mapping \(\varphi:E\times F\to G\) satisfies~\(\medtprod_2\) if and only if the vectors~\(\varphi(x_{\alpha},y_{\beta})\) are linearly independent whenever the vectors \(x_{\alpha}\in E\) and \(y_{\beta}\in F\) are linearly independent.
\end{exer}
\begin{proof}
If \(f:E\tprod F\to G\) is the induced linear map with \(\varphi=f\after\tprod\), then \(\varphi\)~satisfies \(\medtprod_2\) if and only if \(f\)~is injective (1.8.1)---that is, if and only if \(f\)~preserves linear independence. But \(f\)~preserves linear independence if and only if \(\varphi\)~does, since \(\tprod\)~does (1.5.1).
\end{proof}

\begin{exer}[3]
If \(A\ne 0\) is a finite-dimensional algebra forming a tensor product under the algebra multiplication, then \(\dim A=1\).
\end{exer}
\begin{proof}
By~(1.3) \(\dim A=(\dim A)^2\), and \(\dim A\ne 0\), so \(\dim A=1\).
\end{proof}

\begin{exer}[5]
If \(E,E^*\) and \(F,F^*\) are pairs of dual spaces of finite dimension and \(\beta:E\times F\to B(E^*,F^*)\) is the bilinear map given by
\[\beta_{x,y}(x^*,y^*)=\sprod{x^*}{x}\sprod{y^*}{y}\]
then \((B(E^*,F^*),\beta)\)~is the tensor product of \(E\) and~\(F\).
\end{exer}
\begin{proof}
Let \(x_1,\ldots,x_n\) be a basis in~\(E\) with dual basis \(x^{*1},\ldots,x^{*n}\) in~\(E^*\) and let \(y_1,\ldots,y_m\) be a basis in~\(F\) with dual basis \(y^{*1},\ldots,y^{*m}\) in~\(F^*\). Let \(\varphi^{*kl}:E^*\times F^*\to\Gamma\) be the basis function in~\(B(E^*,F^*)\) defined by
\[\varphi^{*kl}(x^{*i},y^{*j})=\delta^k_i\delta^l_j\]
Then
\[\beta_{x_k,y_l}(x^{*i},y^{*j})=\sprod{x^{*i}}{x_k}\sprod{y^{*j}}{y_l}=\delta^k_i\delta^l_j=\varphi^{*kl}(x^{*i},y^{*j})\]
so \(\beta_{x_k,y_l}=\varphi^{*kl}\). It follows that \(\im\beta=B(E^*,F^*)\), so \(\beta\)~satisfies~\(\medtprod_1\).

If \(\varphi:E\times F\to G\) is bilinear, define \(f:B(E^*,F^*)\to G\) by \(f(\varphi^{*kl})=\varphi(x_k,y_l)\). Then \(\varphi(x_k,y_l)=f(\beta_{x_k,y_l})\), so \(\varphi=f\beta\). It follows that \(\beta\)~satisfies \(\medtprod_2\).
\end{proof}
\begin{rmk}
This result allows us to view an element \(\sum x_i\tprod y_i\) of the tensor product as a bilinear function: given linear scalar substitutions \(x_i\to\lambda_i\) and \(y_i\to\mu_i\) as inputs, it produces the scalar \(\sum\lambda_i\mu_i\) as output. This is analogous to viewing a vector as a linear function. Indeed, if we identify~\(x\) with~\(\sprod{-}{x}\) and \(y\) with~\(\sprod{-}{y}\), then we can identify \(x\tprod y\) with \(\sprod{-}{x}\sprod{-}{y}\).
\end{rmk}

\subsection*{\S~16}
\begin{rmk}
The tensor product operation is a bifunctor in the category of vector spaces. It maps the objects \(E\) and~\(F\) to the object \(E\tprod F\) and the arrows \(\varphi:E\to E'\) and \(\psi:F\to F'\) to the arrow \(\varphi\tprod\psi:E\tprod F\to E'\tprod F'\).
\end{rmk}

\begin{rmk}
In the proof of Proposition~1.16.1, taking \(x=a\) and \(y\)~arbitrary in~(1.9), it follows from a remark in \S~5 above and Lemma~1.5.1 that the \(\psi_i(y)\)~are linearly dependent. But the linear dependence relation is independent of the choice of~\(y\), so the~\(\psi_i\) are linearly dependent, contrary to the assumption. Here we are essentially ``lifting'' the remark from vectors to maps.
\end{rmk}

\begin{rmk}
Corollary~III to Proposition~1.16.1 is not established in subsection~1.27, where it is also assumed that \(E'\) and~\(F'\) are finite-dimensional. However, it is true. In fact, for fixed bases in \(E,E',F,F'\), the tensor products of the induced basis maps in \(L(E;E')\) and \(L(F;F')\) are the basis maps in \(L(E\tprod F,E'\tprod F')\) induced by the tensor products of the basis vectors.\footnote{See problem~1.19.4.}
\end{rmk}

\subsection*{\S~17}
\begin{rmk}
In the example presented in this subsection, intuitively the left null space of a linear function in the image of~\(\beta\) is ``large'' in the sense of having finite codimension in an infinite dimensional space, while this is not true for all linear functions in the codomain of~\(\beta\).\footnote{See also the remark in chapter~II, \S~1 of~\cite{greub1} above.}
\end{rmk}

\subsection*{\S~19}
\begin{rmk}
The proof of~(1.12) is captured in the following commutative diagram:
\begin{diagram}[nohug]
E\tprod F						&														&\rTo^{\varphi\tprod\psi}	&		&E'\tprod F'\\
								&\rdLine_{\pi}											&							&\ruTo	&\\
\dTo<{\pi_1\tprod\pi_2}			&														&\rdLine\ruLine				&		&\uTo>{\chi}\\
								&\ruLine_{\ \proj{\varphi}\tprod\proj{\psi}}			&							&\rdTo	&\\
\proj{E}\tprod\proj{F}			&														&\rTo_g						&		&\proj{E\tprod F}
\end{diagram}
\end{rmk}

\begin{rmk}
The ``only if'' part of the claim in problem~1(a) requires \(E\ne 0\) and \(F\ne 0\). For example, if \(E\ne 0\) and \(F=0\) and \(\varphi=0\) and \(\psi=0\), then \(E\tprod F=0\) and \(\varphi\tprod\psi=0\) is injective despite the fact that \(\varphi\)~is not.
\end{rmk}

\begin{exer}[2]
If \(\varphi:E\to E\) and \(\psi:F\to F\) are linear with \(\dim E=n\) and \(\dim F=m\), then
\[\tr(\varphi\tprod\psi)=\tr\varphi\mult\tr\psi\]
and
\[\det(\varphi\tprod\psi)=(\det\varphi)^m\mult(\det\psi)^n\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) and \(f_1,\ldots,f_m\) be a basis of~\(F\). Write
\begin{align*}
\varphi e_i&=\sum_{k=1}^n\alpha^k_ie_k\\
\psi f_j&=\sum_{l=1}^m\beta^l_jf_l
\end{align*}
Then \(e_1\tprod f_1,\ldots,e_n\tprod f_m\) is a basis of~\(E\tprod F\) and
\[(\varphi\tprod\psi)(e_i\tprod f_j)=\varphi e_i\tprod\psi f_j=\sum_{k,l}\alpha^k_i\beta^l_je_k\tprod f_l\]
It follows that
\[\tr(\varphi\tprod\psi)=\sum_{i,j}\alpha^i_i\beta^j_j=\bigl(\sum_i\alpha^i_i\bigr)\bigl(\sum_j\beta^j_j\bigr)=\tr\varphi\mult\tr\psi\]
For the determinant, observe that
\[\varphi\tprod\psi=(\varphi\tprod\iota_F)\after(\iota_E\tprod\psi)\]
so
\[\det(\varphi\tprod\psi)=\det(\varphi\tprod\iota_F)\mult\det(\iota_E\tprod\psi)\]
But \(M(\varphi\tprod\iota_F)=(\alpha^k_i\delta^l_j)\) is block diagonal with \(m\)~blocks each equal to~\(M(\varphi)\), so \(\det(\varphi\tprod\iota_F)=(\det\varphi)^m\). Similarly \(\det(\iota_E\tprod\psi)=(\det\psi)^n\).
\end{proof}

\begin{exer}[3]
If \(\alpha,\beta:E\to E\) are linear with \(\dim E=n\), and \(\Phi:L(E;E)\to L(E;E)\) is defined by
\[\Phi\sigma=\alpha\after\sigma\after\beta\]
then
\[\tr\Phi=\tr\alpha\mult\tr\beta\]
and
\[\det\Phi=\det(\alpha\after\beta)^n\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) and let \(\psi_{ij}:E\to E\) be the induced basis transformation defined by \(\psi_{ij}e_k=\delta^i_ke_j\). The isomorphism \(\Psi:E\tprod E\to L(E;E)\) defined by \(\Psi(e_i\tprod e_j)=\psi_{ij}\) induces an isomorphism
\[\widehat{\Psi}:L(E\tprod E;E\tprod E)\iso L(L(E;E);L(E;E))\]
by \(\widehat{\Psi}(\sigma\tprod\tau)=\Psi\after(\sigma\tprod\tau)\after\Psi^{-1}\):
\begin{diagram}
E\tprod E					&\rTo^{\Psi}	&L(E;E)\\
\dTo<{\sigma\tprod\tau}		&				&\dTo>{\widehat{\Psi}(\sigma\tprod\tau)}\\
E\tprod E					&\rTo_{\Psi}	&L(E;E)
\end{diagram}
It is easy to verify that
\[\widehat{\Psi}(\beta^*\tprod\alpha)=\Phi\]
where \(\beta^*\)~is the transpose of~\(\beta\). Therefore by the previous exercise,
\[\tr\Phi=\tr(\beta^*\tprod\alpha)=\tr(\beta^*)\mult\tr\alpha=\tr\alpha\mult\tr\beta\]
and
\[\det\Phi=\det(\beta^*\tprod\alpha)=(\det\beta^*)^n(\det\alpha)^n=\det(\alpha\after\beta)^n\qedhere\]
\end{proof}
\begin{rmk}
It is more natural to do this problem using the composition algebra (see \S~26). Indeed, if \(\alpha=a^*\tprod b\), \(\beta=c^*\tprod d\), and \(\sigma=x^*\tprod y\), then
\begin{align*}
\alpha\after\sigma\after\beta&=(a^*\tprod b)\after(x^*\tprod y)\after(c^*\tprod d)\\
	&=(a^*\tprod b)\after\bigl(\sprod{x^*}{d}(c^*\tprod y)\bigr)\\
	&=\sprod{x^*}{d}\sprod{a^*}{y}(c^*\tprod b)\\
	&=\bigl(\sprod{x^*}{d}c^*\bigr)\tprod\bigl(\sprod{a^*}{y}b\bigr)\\
	&=\beta^*x^*\tprod\alpha y\\
	&=(\beta^*\tprod\alpha)\sigma
\end{align*}
so \(\Phi=\beta^*\tprod\alpha\). Here \(\beta^*=d\tprod c^*\).\footnote{See problem~1.30.1.} The general case follows by multilinearity. Alternately, we could use the isomorphism~\(\Omega\) from Proposition~1.29.1.
\end{rmk}

\subsection*{\S~20}
\begin{rmk}
A \emph{linear} map is universal if and only if it is an isomorphism. In fact, it satisfies~\(\medtprod_1\) if and only if it is surjective, and it satisfies~\(\medtprod_2\) if and only if it is injective (has a left inverse). In this sense, a tensor product map is a multilinear analogue of an isomorphism.
\end{rmk}

\begin{rmk}
To see that there is a unique isomorphism
\[f:(\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\to\tprods{E}{1}{p+q}\]
with
\[f((\tprods{x}{1}{p})\tprod(\tprods{x}{p+1}{p+q}))=\tprods{x}{1}{p+q}\tag{1}\]
observe that for each \(p\)-tuple \((x_1,\ldots,x_p)\in\timess{E}{1}{p}\), there is a \(q\)-linear map \(\varphi_{x_1\ldots x_p}:\timess{E}{p+1}{p+q}\to\tprods{E}{1}{p+q}\) given by
\[\varphi_{x_1\ldots x_p}(x_{p+1},\ldots,x_{p+q})=\tprods{x}{1}{p+q}\]
By the universal property of \(\tprods{E}{p+1}{p+q}\), it follows that there is a linear map \(f_{x_1\ldots x_p}:\tprods{E}{p+1}{p+q}\to\tprods{E}{1}{p+q}\) with
\[f_{x_1\ldots x_p}(\tprods{x}{p+1}{p+q})=\tprods{x}{1}{p+q}\]
Now the mapping \((x_1,\ldots,x_p)\mapsto f_{x_1\ldots x_p}\) is \(p\)-linear, so by the universal property of \(\tprods{E}{1}{p}\), there is a linear map
\[\widehat{f}:\tprods{E}{1}{p}\to L(\tprods{E}{p+1}{p+q};\tprods{E}{1}{p+q})\]
with \(\widehat{f}(\tprods{x}{1}{p})=f_{x_1\ldots x_p}\). Define
\[\varphi:(\tprods{E}{1}{p})\times(\tprods{E}{p+1}{p+q})\to\tprods{E}{1}{p+q}\]
by \(\varphi(x,y)=\widehat{f}(x)(y)\). Then \(\varphi\)~is bilinear, so by the universal property of \((\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\), there is a linear map~\(f\) satisfying \(f\after\tprod=\varphi\), which implies~(1).

By the universal property of \(\tprods{E}{1}{p+q}\), the \((p+q)\)-linear map
\[\psi:\timess{E}{1}{p+q}\to(\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\]
defined by
\[\psi(x_1,\ldots,x_{p+q})=(\tprods{x}{1}{p})\tprod(\tprods{x}{p+1}{p+q})\]
induces a linear map
\[g:\tprods{E}{1}{p+q}\to(\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\]
with
\[g(\tprods{x}{1}{p+q})=(\tprods{x}{1}{p})\tprod(\tprods{x}{p+1}{p+q})\]
By universal properties again, \(f\after g=\iota\) and \(g\after f=\iota\) so \(f\)~is an isomorphism, and \(f\)~is uniquely determined by~(1).
\end{rmk}

\begin{rmk}
If the vectors~\(a^i_{\nu}\) are linearly independent in~\(E_i\) (\(i=1,\ldots,p\)), then their tensor products \(a^1_{\nu_1}\tprod\cdots\tprod a^p_{\nu_p}\) are linearly independent in \(\tprods{E}{1}{p}\). Indeed, if \(p=2\), this follows from Lemma~1.5.1. If \(p>2\), we use the natural isomorphism
\[\tprods{E}{1}{p}\iso(\tprods{E}{1}{p-1})\tprod E_p\]
If
\[\sum_{\nu_1,\ldots,\nu_p}\lambda_{\nu_1\ldots\nu_p}a^1_{\nu_1}\tprod\cdots\tprod a^p_{\nu_p}=0\]
then
\[\sum_{\nu_1,\ldots,\nu_p}(\lambda_{\nu_1\ldots\nu_p}a^1_{\nu_1}\tprod\cdots\tprod a^{p-1}_{\nu_{p-1}})\tprod a^p_{\nu_p}=0\]
Since the~\(a^p_{\nu_p}\) are linearly independent in~\(E_p\), it follows from Lemma~1.5.1 that
\[\lambda_{\nu_1\ldots\nu_p}a^1_{\nu_1}\tprod\cdots\tprod a^{p-1}_{\nu_{p-1}}=0\]
for all \(\nu_1,\ldots,\nu_p\). But \(a^1_{\nu_1}\tprod\cdots\tprod a^{p-1}_{\nu_{p-1}}\ne 0\) since \(a^i_{\nu}\ne 0\) for all \(i,\nu\) (see problem~1), so \(\lambda_{\nu_1\ldots\nu_p}=0\) for all \(\nu_1,\ldots,\nu_p\).

If the vectors~\(a^i_{\nu}\) span~\(E_i\) (\(i=1,\ldots,p\)), then clearly their tensor products span \(\tprods{E}{1}{p}\). It follows that if the vectors~\(a^i_{\nu}\) form a basis of~\(E_i\) (\(i=1,\ldots,p\)), then their tensor products form a basis of \(\tprods{E}{1}{p}\).
\end{rmk}

\begin{rmk}
To see that for \(\varphi_i:E_i\to F_i\) the map \((\varphi_1,\ldots,\varphi_p)\mapsto\tprods{\varphi}{1}{p}\) induces an injection
\[L(E_1;F_1)\tprod\cdots\tprod L(E_p;F_p)\to L(\tprods{E}{1}{p};\tprods{F}{1}{p})\]
proceed by induction on \(p\ge 2\). For \(p=2\), this is just Proposition~1.16.1. For \(p>2\), if we make the appropriate identifications we have
\begin{align*}
L(E_1;F_1)\tprod\cdots\tprod L(E_p;F_p)&=\bigl(L(E_1;F_1)\tprod\cdots\tprod L(E_{p-1};F_{p-1})\bigr)\tprod L(E_p;F_p)\\
	&\subseteq L(\tprods{E}{1}{p-1};\tprods{F}{1}{p-1})\tprod L(E_p;F_p)\\
	&\subseteq L((\tprods{E}{1}{p-1})\tprod E_p;(\tprods{F}{1}{p-1})\tprod F_p)\\
	&=L(\tprods{E}{1}{p};\tprods{F}{1}{p})
\end{align*}
\end{rmk}

\begin{exer}[1]
In \(\tprods{E}{1}{p}\):
\begin{enumerate}
\item[(a)] \(\tprods{x}{1}{p}=0\) if and only if at least one \(x_i=0\).
\item[(b)] If \(\tprods{x}{1}{p}\ne 0\), then
\[\tprods{x}{1}{p}=\tprods{y}{1}{p}\]
if and only if \(y_i=\lambda_i x_i\) with \(\lambda_1\cdots\lambda_p=1\).
\end{enumerate}
\end{exer}
\begin{proof}
The reverse directions follow from multilinearity of the tensor product. The forward directions follow by induction on~\(p\) using the natural isomorphism
\[\tprods{E}{1}{p}\iso(\tprods{E}{1}{p-1})\tprod E_p\]
\begin{enumerate}
\item[(a)] If \(p=2\), the result follows from Lemma~1.5.1. If \(p>2\), then \((\tprods{x}{1}{p-1})\tprod x_p=0\), so by Lemma~1.5.1 either \(\tprods{x}{1}{p-1}=0\) and \(x_i=0\) for some \(i=1,\ldots,p-1\) by induction, or else \(x_p=0\).
\item[(b)] If \(p=2\), the result follows from problem~1.8.4. If \(p>2\), then
\[(\tprods{x}{1}{p-1})\tprod x_p=(\tprods{y}{1}{p-1})\tprod y_p\]
so by the same problem,
\[\tprods{y}{1}{p-1}=\lambda_p^{-1}\tprods{x}{1}{p-1}\qquad\text{and}\qquad y_p=\lambda_p x_p\]
for some \(\lambda_p\ne 0\). By induction, we may write \(y_1=\mu_1\lambda_p^{-1}x_1\) and \(y_i=\mu_i x_i\) for \(i=2,\ldots,p-1\) with \(\mu_1\cdots\mu_{p-1}=1\). Setting \(\lambda_1=\mu_1\lambda_p^{-1}\) and \(\lambda_i=\mu_i\) for \(i=2,\ldots,p-1\), it follows that \(y_i=\lambda_i x_i\) for \(i=1,\ldots,p\) and \(\lambda_1\cdots\lambda_p=1\).\qedhere
\end{enumerate}
\end{proof}

\subsection*{\S~21}
\begin{rmk}
For \(p,q\ge 1\), let
\[\varphi_i:\prod_{j=1}^p E^i_j\to E^i_{p+1}\qquad(i=1,\ldots,q)\]
be a family of \(q\) \(p\)-linear maps. Then there is a unique \(p\)-linear map
\[\varphi=\tprods{\varphi}{1}{q}:\prod_{j=1}^p\Biggl(\bigtprod_{i=1}^q E^i_j\Biggr)\to\bigtprod_{i=1}^q E^i_{p+1}\]
with
\[\varphi(x^1_1\tprod\cdots\tprod x^q_1\,,\,\ldots\,,\,x^1_p\tprod\cdots\tprod x^q_p)=\varphi_1(x^1_1,\ldots,x^1_p)\tprod\cdots\tprod\varphi_q(x^q_1,\ldots,x^q_p)\]
Moreover,
\[\ker_j\varphi=\sum_{i=1}^q E^1_j\tprod\cdots\tprod\ker_j\varphi_i\tprod\cdots\tprod E^q_j\qquad(j=1,\ldots,p)\]
In particular, if each \(\varphi_i\)~is nondegenerate, then \(\varphi\)~is nondegenerate.
\end{rmk}

\subsection*{\S~22}
\begin{rmk}
If \(\Phi\)~is bilinear in \(E\times E'\) and \(\Psi\)~is bilinear in \(F\times F'\), then nondegeneracy of \(\Phi\tprod\Psi\) does not imply nondegeneracy of \(\Phi\) and~\(\Psi\) unless all the spaces are nonzero, contrary to what the book says. For example, if \(E\ne 0\) and \(E'=0\), \(F=F'=0\), and \(\Phi=0\) and \(\Psi=0\), then \(\Phi\)~is degenerate, but \(E\tprod F=0\) and \(E'\tprod F'=0\), so \(\Phi\tprod\Psi=0\) is nondegenerate.\footnote{This is essentially the same error as in problem~1.19.1(a) above.}
\end{rmk}

\subsection*{\S~26}
\begin{rmk}
Let \(S:(E^*\tprod E)\tprod(E^*\tprod E)\to(E^*\tprod E^*)\tprod(E\tprod E)\) be the linear isomorphism defined by
\[(x^*\tprod x)\tprod(y^*\tprod y)\mapsto(x^*\tprod y^*)\tprod(y\tprod x)\]
Let \(f:(E^*\tprod E^*)\tprod(E\tprod E)\to\Gamma\tprod(E^*\tprod E)\) be the linear map corresponding to the tensor product of the bilinear maps \(\sprod{-}{-}:E^*\cross E\to\Gamma\) and \(\tprod:E^*\cross E\to E^*\tprod E\). Let \(g:\Gamma\tprod(E^*\tprod E)\to E^*\tprod E\) be the linear map defined by \(g(\lambda\tprod z)=\lambda z\). Then the bilinear map~\({\after}\) corresponding to \(gfS\) satisfies
\[(x^*\tprod x)\after(y^*\tprod y)=\sprod{x^*}{y}(y^*\tprod x)\]
\end{rmk}

\begin{rmk}
If \(\dim E<\infty\), one way to discover the composition algebra is (see \S~28)
\[L(E;E)\iso L(E\tprod\Gamma;\Gamma\tprod E)\iso L(E;\Gamma)\tprod L(\Gamma;E)\iso L(E)\tprod E\]
\end{rmk}

\begin{rmk}
If \(\dim E<\infty\), the fact that
\[\sprod{T(x^*\tprod x)}{T(y^*\tprod y)}=\sprod{x^*\tprod x}{y^*\tprod y}\]
follows from~(1.29) in \S~28. This fact implies that \(T\)~is an isometry, so \(T^*=T^{-1}\).
\end{rmk}

\begin{exer}[1]
For the bilinear map
\[\gamma:L(E,E';E'')\times L(F,F';F'')\to L(E\tprod F,E'\tprod F';E''\tprod F'')\]
with
\[\gamma(\varphi,\psi):(x\tprod y,x'\tprod y')\mapsto\varphi(x,x')\tprod\psi(y,y')\]
the pair \((\im\gamma,\gamma)\) is the tensor product of \(L(E,E';E'')\) and \(L(F,F';F'')\).
\end{exer}
\begin{proof}
By transfer of Corollary~II of Proposition~1.16.1, using the isomorphism between linear and bilinear maps induced by the tensor product.

In detail, consider the following commutative diagram:
\begin{diagram}[nohug]
L(E\tprod E';E'')\tprod L(F\tprod F';F'')	&\rTo^f			&L((E\tprod E')\tprod(F\tprod F');E''\tprod F'')\\
											&				&\dTo>{\iso}\\
\uTo<{\iso}									&				&L((E\tprod F)\tprod(E'\tprod F');E''\tprod F'')\\
											&				&\dTo>{\iso}\\
L(E,E';E'')\tprod L(F,F';F'')				&\rTo^g			&L(E\tprod F,E'\tprod F';E''\tprod F'')\\
\uTo<{\tprod}								&\ruTo>{\gamma}	&\\
L(E,E';E'')\times L(F,F';F'')				&				&
\end{diagram}
Note \(g\)~is injective since \(f\)~is injective (1.16.1), so \(\gamma\)~satisfies \(\medtprod_2\) (1.8.1). Since \(\gamma\)~also satisfies~\(\medtprod_1\), it follows that \(\gamma\)~is the tensor product.
\end{proof}

\begin{exer}[2]
If \(E,E^*\) and \(F,F^*\) are pairs of dual spaces with \(E_1\subseteq E\) and \(F_1\subseteq F\) subspaces, then a scalar product is induced between
\[(E^*\tprod F^*)/(\oc{E_1}\tprod F^*+E^*\tprod\oc{F_1})\qquad\text{and}\qquad E_1\tprod F_1\]
by the scalar product between \(E^*\tprod F^*\) and \(E\tprod F\). In particular,
\[\oc{(E_1\tprod F_1)}=\oc{E_1}\tprod F^*+E^*\tprod\oc{F_1}\]
\end{exer}
\begin{proof}
A scalar product is induced between \(E^*/\oc{E_1}\) and~\(E_1\) by \(\sprod{\proj{x^*}}{x}=\sprod{x^*}{x}\), and between \(F^*/\oc{F_1}\) and~\(F_1\) by \(\sprod{\proj{y^*}}{y}=\sprod{y^*}{y}\). Therefore a scalar product is induced between \((E^*/\oc{E_1})\tprod(F^*/\oc{F_1})\) and \(E_1\tprod F_1\) by
\[\sprod{\proj{x^*}\tprod\proj{y^*}}{x\tprod y}=\sprod{x^*}{x}\sprod{y^*}{y}=\sprod{x^*\tprod y^*}{x\tprod y}\]
where the scalar product on the right is between \(E^*\tprod F^*\) and \(E\tprod F\). However,
\[(E^*\tprod F^*)/(\oc{E_1}\tprod F^*+E^*\tprod\oc{F_1})\iso(E^*/\oc{E_1})\tprod(F^*/\oc{F_1})\]
where \(\proj{x^*\tprod y^*}\mapsto\proj{x^*}\tprod\proj{y^*}\), so there is a scalar product
\[\sprod{\proj{x^*\tprod y^*}}{x\tprod y}=\sprod{x^*\tprod y^*}{x\tprod y}\]
as required.
\end{proof}

\begin{exer}[3]
If \(E,E^*\) are dual spaces and \(\varphi:E\to E\) and \(\varphi^*:E^*\from E^*\) are dual transformations, then \(\varphi\tprod\varphi^*\)~is self-dual.
\end{exer}
\begin{proof}
Recall that \(E\tprod E^*\) is self-dual under the scalar product
\[\sprod{x\tprod x^*}{y\tprod y^*}=\sprod{x^*}{y}\sprod{y^*}{x}\]
Now
\begin{align*}
\sprod{(\varphi\tprod\varphi^*)(x\tprod x^*)}{y\tprod y^*}&=\sprod{\varphi x\tprod\varphi^* x^*}{y\tprod y^*}\\
	&=\sprod{\varphi^* x^*}{y}\sprod{y^*}{\varphi x}\\
	&=\sprod{x^*}{\varphi y}\sprod{\varphi^* y^*}{x}\\
	&=\sprod{x\tprod x^*}{\varphi y\tprod\varphi^* y^*}\\
	&=\sprod{x\tprod x^*}{(\varphi\tprod\varphi^*)(y\tprod y^*)}
\end{align*}
so \((\varphi\tprod\varphi^*)^*=\varphi\tprod\varphi^*\).
\end{proof}

\subsection*{\S~28}
\textbf{In this subsection, all vector spaces are finite-dimensional.}

\begin{rmk}
If \(\varphi:E\to F\) is linear, then \(T^{-1}(\varphi)\) is decomposable if and only if \(\varphi\)~has rank at most~\(1\).
\end{rmk}
\begin{proof}
If \(T^{-1}(\varphi)=a^*\tprod b\), then \(\im\varphi=\gen{b}\). Conversely if \(\im\varphi=\gen{b}\), there is a linear function \(f\in L(E)\) with \(\varphi(x)=f(x)b\) for all \(x\in E\). If \(a^*\in E^*\) corresponds to~\(f\), then \(T^{-1}(\varphi)=a^*\tprod b\).
\end{proof}

\begin{rmk}
If \(\varphi:E\to F\) is linear and
\[T^{-1}(\varphi)=\sum_{i=1}^r a^{*i}\tprod b_i\qquad(a^{*i}\in E^*,b_i\in F)\]
where the~\(a^{*i}\) and the~\(b_i\) are linearly independent, then \(\varphi\)~has rank~\(r\). Thus the rank of~\(\varphi\) is equal to the tensor rank of~\(T^{-1}(\varphi)\).
\end{rmk}
\begin{proof}
It is sufficient to prove that \(\im\varphi=\gen{b_i}\). Clearly \(\im\varphi\subseteq\gen{b_i}\). Conversely if \(y=\sum_{i=1}^r\beta_ib_i\) and \(a_j\in E\) with \(\sprod{a^{*i}}{a_j}=\delta_{ij}\) for all \(1\le i,j\le r\), then for the vector \(x=\sum_{j=1}^r\beta_j a_j\) we have \(\varphi(x)=y\).
\end{proof}

\begin{rmk}
Fix dual bases \(e_i,e^{*i}\) and \(f_j,f^{*j}\) of \(E,E^*\) and \(F,F^*\). If \(z\in E^*\tprod F\) and \(\varphi=T(z)\), write
\[z=\sum_{\mu,\nu}(z)^{\mu}_{\nu}e^{*\nu}\tprod f_{\mu}\]
Then
\[\varphi(e_{\lambda})=\sum_{\mu,\nu}(z)^{\mu}_{\nu}\sprod{e^{*\nu}}{e_{\lambda}}f_{\mu}=\sum_{\mu}(z)^{\mu}_{\lambda}f_{\mu}\]
so
\[(z)^{\kappa}_{\lambda}=\sprod{f^{*\kappa}}{\varphi(e_{\lambda})}\]
This shows that the components of the tensor~\(z\) are precisely the matrix entries of the map~\(\varphi\), with respect to the chosen bases.
\end{rmk}

\begin{rmk}
By~(1.30), if we view \(a^*\tprod a\) as a linear transformation, then its trace is just \(\sprod{a^*}{a}\). This extends by linearity to a natural (coordinate-free) definition of the trace which yields a slick proof that \(\tr(\varphi\tprod\psi)=\tr\varphi\mult\tr\psi\). Indeed, if \(\varphi=a^*\tprod a\) and \(\psi=b^*\tprod b\), then we can write \(\varphi\tprod\psi=(a^*\tprod b^*)\tprod(a\tprod b)\) since
\begin{align*}
(\varphi\tprod\psi)(x\tprod y)&=\varphi x\tprod\psi y\\
	&=(\sprod{a^*}{x}a)\tprod(\sprod{b^*}{y}b)\\
	&=\sprod{a^*}{x}\sprod{b^*}{y}(a\tprod b)\\
	&=\sprod{a^*\tprod b^*}{x\tprod y}(a\tprod b)
\end{align*}
Therefore
\[\tr(\varphi\tprod\psi)=\sprod{a^*\tprod b^*}{a\tprod b}=\sprod{a^*}{a}\sprod{b^*}{b}=\tr\varphi\mult\tr\psi\]
The general case follows by multilinearity.
\end{rmk}

\subsection*{\S~29}
\textbf{In this subsection, all vector spaces are finite-dimensional.}

\begin{rmk}
In the proof of Proposition~1.29.1, note that if \(\alpha=a^*\tprod b\) and \(\beta=c^*\tprod d\), then
\[F(\alpha\tprod\beta)(x^*\tprod y)=\sprod{a^*\tprod b}{x^*\tprod y}(c^*\tprod d)=\sprod{x^*}{b}\sprod{a^*}{y}(c^*\tprod d)\]
Comparing with the calculation after problem~1.19.3 above, we immediately see that \(Q\), which swaps \(b\) and~\(d\), satisfies
\[F(Q(\alpha\tprod\beta))=\beta^*\tprod\alpha=\Omega(\alpha\tprod\beta)\]
These results show that we can view \(L(A;A)\) as a tensor product in multiple ways.
\end{rmk}

\subsection*{\S~30}
\textbf{In this subsection, all vector spaces are finite-dimensional.}

\begin{exer}[1]
Let \(E,E^*\) and \(F,F^*\) be pairs of dual spaces. If \(a^*\in E^*\) and \(b\in F\), then
\[(a^*\tprod b)^*=b\tprod a^*\]
\end{exer}
\begin{proof}
Recall \(a^*\tprod b:E\to F\) is defined by \(x\mapsto\sprod{a^*}{x}b\) and \(b\tprod a^*:F^*\to E^*\) is defined by \(y^*\mapsto\sprod{y^*}{b}a^*\). For \(y^*\in F^*\) and \(x\in E\),
\[\sprod{(b\tprod a^*)y^*}{x}=\sprod{y^*}{b}\sprod{a^*}{x}=\sprod{y^*}{(a^*\tprod b)x}\qedhere\]
\end{proof}

\begin{exer}[2]
If \(E,F\ne 0\) are Euclidean spaces with \(\varphi:E\to E\) and \(\psi:F\to F\), then \(\varphi\tprod\psi:E\tprod F\to E\tprod F\) is a rotation if and only if \(\varphi=\lambda\tau_E\) and \(\psi=\lambda^{-1}\tau_F\) where \(\tau_E\) and~\(\tau_F\) are rotations and \(\lambda\ne 0\).
\end{exer}
\begin{proof}
If \(\varphi=\lambda\tau_E\) and \(\psi=\lambda^{-1}\tau_F\), then \(\varphi\tprod\psi=\tau_E\tprod\tau_F\), and
\begin{align*}
\adj{\tau_E\tprod\tau_F}\after(\tau_E\tprod\tau_F)&=(\adj{\tau_E}\tprod\adj{\tau_F})\after(\tau_E\tprod\tau_F)\\
	&=(\adj{\tau_E}\after\tau_E)\tprod(\adj{\tau_F}\after\tau_F)\\
	&=\iota_E\tprod\iota_F\\
	&=\iota_{E\tprod F}
\end{align*}
so \(\adj{\varphi\tprod\psi}=(\varphi\tprod\psi)^{-1}\) and \(\varphi\tprod\psi\)~is a rotation.

Conversely if \(\varphi\tprod\psi\)~is a rotation, then
\[\adj{\varphi}\tprod\adj{\psi}=\adj{\varphi\tprod\psi}=(\varphi\tprod\psi)^{-1}=\varphi^{-1}\tprod\psi^{-1}\]
It follows that \(\adj{\varphi}=\mu\varphi^{-1}\) and \(\adj{\psi}=\mu^{-1}\psi^{-1}\) for some \(\mu\ne 0\).\footnote{See problem~1.8.4.} We may assume \(\mu>0\) (otherwise consider \(-\varphi\) and~\(-\psi\)). Set \(\lambda=\sqrt{\mu}>0\) and define \(\tau_E=\lambda^{-1}\varphi\) and \(\tau_F=\lambda\psi\). Then \(\adj{\tau_E}\after\tau_E=\iota_E\), so \(\adj{\tau_E}=\tau_E^{-1}\) and \(\tau_E\)~is a rotation, and similarly for~\(\tau_F\).
\end{proof}
\begin{rmk}
The proofs of the other parts of this problem in the book are similar.
\end{rmk}

\begin{exer}[3]
Let \(E\)~be a real vector space with \(\dim E=n\) and \(\varphi,\psi:E\to E\) be regular transformations.
\begin{enumerate}
\item[(a)] If \(n\)~is even, then \(\varphi\tprod\psi\) preserves orientation.
\item[(b)] If \(n\)~is odd, then \(\varphi\tprod\psi\) preserves orientation if and only if \(\varphi\) and~\(\psi\) either both preserve orientation or both reverse orientation.
\end{enumerate}
\end{exer}
\begin{proof}
This follows from \(\det(\varphi\tprod\psi)=(\det\varphi\mult\det\psi)^n\).\footnote{See problem~1.19.2.}
\end{proof}

\newpage
\section*{Chapter~2}
\begin{rmk}
In the sections of this chapter, the tensor product operation is seen to be a bifunctor in various subcategories of the category of vector spaces.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
The multiplication in the canonical tensor product \(A\tprod B\) of algebras \(A\) and~\(B\) is just the tensor product of the multiplications in \(A\) and~\(B\), as defined in subsection~1.21.
\end{rmk}

\begin{rmk}
Let \(A\) and~\(B\) be nonzero associative unital algebras. There are injective unital algebra homomorphisms
\begin{diagram}
A&\rTo^{i_1}&A\tprod B&\lTo^{i_2}&B
\end{diagram}
given by \(i_1(a)=a\tprod 1\) and \(i_2(b)=1\tprod b\), whose images commute:
\[i_1(a)i_2(b)=(a\tprod 1)(1\tprod b)=a\tprod b=(1\tprod b)(a\tprod 1)=i_2(b)i_1(a)\]
Moreover, given any unital algebra homomorphisms \(f:A\to C\) and \(g:B\to C\) whose images commute in an associative unital algebra~\(C\), there is a unique unital algebra homomorphism \(h:A\tprod B\to C\) with \(h\after i_1=f\) and \(h\after i_2=g\):
\begin{diagram}[nohug]
A	&\rTo^{i_1}	&A\tprod B	&\lTo^{i_2}	&B\\
	&\rdTo<f	&\dDashto<h	&\ldTo>g	&\\
	&			&C			&			&
\end{diagram}
Indeed, if such an~\(h\) exists it must satisfy
\[h(a\tprod b)=h((a\tprod 1)(1\tprod b))=h(a\tprod 1)h(1\tprod b)=f(a)g(b)\]
and this uniquely determines it by linearity. On the other hand, the mapping \((a,b)\mapsto f(a)g(b)\) is bilinear, so induces a linear map~\(h\) with \(h(a\tprod b)=f(a)g(b)\). This map is a homomorphism since it is linear and
\begin{align*}
h((a_1\tprod b_1)(a_2\tprod b_2))&=h(a_1a_2\tprod b_1b_2)\\
	&=f(a_1a_2)g(b_1b_2)\\
	&=f(a_1)f(a_2)g(b_1)g(b_2)\\
	&=f(a_1)g(b_1)f(a_2)g(b_2)\\
	&=h(a_1\tprod b_1)h(a_2\tprod b_2)
\end{align*}
Moreover,
\[h(a\tprod 1)=f(a)g(1)=f(a)\qquad\text{and}\qquad h(1\tprod b)=f(1)g(b)=g(b)\]
since \(f\) and~\(g\) are unital, so the diagram above commutes and \(h\)~is also unital.

This argument shows that \(A\tprod B\) is a universal (initial) object in the category of ``associative unital algebras with unital algebra homomorphisms of \(A\) and~\(B\) into them whose images commute''. By category theoretic abstract nonsense, \(A\tprod B\)~is therefore unique up to isomorphism. If \(A\) and~\(B\) are also \emph{commutative}, then \(A\tprod B\)~is just the coproduct of \(A\) and~\(B\) in the subcategory of algebras that are also commutative.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
If \(\varphi_1:A_1\to B_1\) and \(\varphi_2:A_2\to B_2\) are algebra homomorphisms and \(\varphi=\varphi_1\tprod\varphi_2\) is the tensor product of the underlying \emph{linear maps}, then
\begin{align*}
\varphi((x_1\tprod x_2)(y_1\tprod y_2))&=\varphi(x_1y_1\tprod x_2y_2)\\
	&=\varphi_1(x_1y_1)\tprod\varphi_2(x_2y_2)\\
	&=(\varphi_1x_1\varphi_1y_1)\tprod(\varphi_2x_2\varphi_2y_2)\\
	&=(\varphi_1x_1\tprod\varphi_2 x_2)(\varphi_1y_1\tprod\varphi_2 y_2)\\
	&=\varphi(x_1\tprod x_2)\varphi(y_1\tprod y_2)
\end{align*}
Since \(\varphi\)~is linear, it follows that \(\varphi\)~is an algebra homomorphism. This proof avoids the use of structure maps.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
The canonical tensor product \(A\tprod B\) of \(G\)-graded algebras \(A=\sum_{\alpha\in G}A_{\alpha}\) and \(B=\sum_{\beta\in G}B_{\beta}\) is a \emph{graded algebra} under the simple gradation in~(2.15). In fact, if \(x_i\in A_{\alpha_i}\) and \(y_i\in B_{\beta_i}\) for \(i=1,2\), then
\[(x_1\tprod y_1)(x_2\tprod y_2)=x_1x_2\tprod y_1y_2\]
Since \(A\) and~\(B\) are graded algebras, \(x_1x_2\in A_{\alpha_1+\alpha_2}\) and \(y_1y_2\in B_{\beta_1+\beta_2}\), so by~(2.15) \(x_1x_2\tprod y_1y_2\) is homogeneous with
\[\deg(x_1x_2\tprod y_1y_2)=\alpha_1+\alpha_2+\beta_1+\beta_2=\deg(x_1\tprod y_1)+\deg(x_2\tprod y_2)\]
The result now follows from bilinearity of the algebra multiplication.
\end{rmk}

\begin{rmk}
If \(x\) and~\(y\) are distinct indeterminates, then
\[\Gamma[x]\tprod\Gamma[y]\iso\Gamma[x,y]\]
is an isomorphism of graded algebras.
\end{rmk}

\subsection*{\S~8}
\begin{rmk}
Let \(A=\sum_p A_p\) and \(B=\sum_q B_q\) be skew graded algebras, so \(A\stprod B\)~is also a skew graded algebra. Define the skew flip operator \(\widehat{f}:A\stprod B\to B\stprod A\) by
\[\widehat{f}(a\tprod b)=(-1)^{pq}b\tprod a\qquad(a\in A_p,\ b\in B_q)\]
Then \(\widehat{f}\)~is an algebra isomorphism. Indeed, it is a linear isomorphism and if \(a_1\in A_p\), \(a_2\in A_q\), \(b_1\in B_r\), and \(b_2\in B_s\), then
\begin{align*}
\widehat{f}[(a_1\tprod b_1)(a_2\tprod b_2)]&=(-1)^{qr}\widehat{f}(a_1a_2\tprod b_1b_2)\\
	&=(-1)^{qr+(p+q)(r+s)}b_1b_2\tprod a_1a_2\\
	&=(-1)^{pr+qs}(b_1\tprod a_1)(b_2\tprod a_2)\\
	&=\widehat{f}(a_1\tprod b_1)\widehat{f}(a_2\tprod b_2)
\end{align*}
The result now follows by multilinearity.

Note the regular flip operator \(a\tprod b\mapsto b\tprod a\) is \emph{not} an algebra isomorphism from \(A\stprod B\) to \(B\stprod A\). This shows that \(\widehat{f}\)~is the appropriate flip operator to use when working with skew products of skew graded algebras.\footnote{See for example subsection~6.18.}
\end{rmk}

\newpage
\section*{Chapter~3}
In this chapter, we see three ways of defining a tensor:
\begin{itemize}[itemsep=0pt]
\item As an object in a tensor algebra characterized by a universal property.
\item As a multilinear function.
\item As a multidimensional array of scalars which transforms according to a certain rule under change of basis.
\end{itemize}
The first way is the most elegant but perhaps the hardest to learn; the second, easier to get started with but messier in the long run; the third, quick and dirty.

\subsection*{\S~3}
\begin{rmk}
The tensor algebra~\(\medtprod E\) is a universal (initial) object in the category of ``associative unital algebras with linear maps of~\(E\) into them''. In this category, the objects are linear maps \(E\to A\), for associative unital algebras~\(A\), and the arrows are unital algebra homomorphisms \(A\to B\) which preserve the units and respect the linear maps from~\(E\):
\begin{diagram}
E	&\rTo	&B\\
\dTo&\ruTo	&\\
A	&		&
\end{diagram}
Every object in this category can be obtained from the tensor algebra~\(\medtprod E\) in a unique way. This is why \(\medtprod E\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(\medtprod E\) satisfy only those properties that are required to make~\(\medtprod E\) into an associative unital algebra containing~\(E\). By category theoretic abstract nonsense, \(\medtprod E\)~is unique up to isomorphism (\S~4).
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
We have a functor from the category of vector spaces into the category of associative unital algebras, which sends vector spaces \(E\) and~\(F\) to the tensor algebras \(\medtprod E\) and~\(\medtprod F\), and which sends a linear map \(\varphi:E\to F\) to the unital algebra homomorphism \(\varphi_{\tprod}:\medtprod E\to\medtprod F\).
\end{rmk}

\begin{rmk}
For a linear map \(\varphi:E\to F\), the following diagram commutes:
\begin{diagram}
E			&\rTo^{\varphi}			&F\\
\dTo<i		&						&\dTo>j\\
\medtprod E	&\rTo_{\varphi_{\tprod}}&\medtprod F
\end{diagram}
This shows that the canonical injection \(i:E\to\medtprod E\) is a natural transformation from the identity functor to the tensor algebra functor (in the category of vector spaces).
\end{rmk}

\begin{rmk}
By a remark from \S~21 of chapter~1 above,
\[\ker\varphi_{\tprod}=\sum_{p=0}^{\infty}\sum_{i=1}^p E\tprod\cdots\tprod\ker\varphi\tprod\cdots\tprod E\]
where \(\ker\varphi\)~is in position \(i\)~of~\(p\) on the right.
\end{rmk}

\begin{rmk}
For vector spaces \(E\) and~\(F\), the canonical injections
\begin{diagram}
E&\rTo^{i_1}&E\dsum F&\lTo^{i_2}&F
\end{diagram}
induce the homomorphisms
\begin{diagram}
\medtprod E&\rTo^{(i_1)_{\tprod}}&\medtprod(E\dsum F)&\lTo^{(i_2)_{\tprod}}&\medtprod F
\end{diagram}
which in turn induce a linear map \(f:(\medtprod E)\tprod(\medtprod F)\to\medtprod(E\dsum F)\) homogeneous of degree zero with
\[f(u\tprod v)=(i_1)_{\tprod}(u)\tprod(i_2)_{\tprod}(v)\qquad(u\in\medtprod E,v\in\medtprod F)\]
However \(f\)~is \emph{not} an algebra homomorphism if \(E\ne 0\) and \(F\ne 0\). Indeed, if \(x\in E\) with \(x\ne 0\) and \(y\in F\) with \(y\ne 0\), then \(x\) and~\(y\) are linearly independent in~\(E\dsum F\), so \(x\tprod y\) and \(y\tprod x\) are linearly independent in \(\medtprod(E\dsum F)\). If \(f\)~were a homomorphism, we would have
\[y\tprod x=f(1\tprod y)\tprod f(x\tprod 1)=f((1\tprod y)(x\tprod 1))=f(x\tprod y)=x\tprod y\]
which is a contradiction.

In the other direction, the sum of the injections
\begin{diagram}
E&\rTo&\medtprod E&\rTo&(\medtprod E)\tprod(\medtprod F)
\end{diagram}
given by \(x\mapsto x\tprod 1\) and
\begin{diagram}
F&\rTo&\medtprod F&\rTo&(\medtprod E)\tprod(\medtprod F)
\end{diagram}
given by \(y\mapsto 1\tprod y\) induces a homomorphism \(h:\medtprod(E\dsum F)\to(\medtprod E)\tprod(\medtprod F)\). If
\[u=\tprods{x}{1}{p}\in\medtprod^p E\qquad\text{and}\qquad v=\tprods{y}{1}{q}\in\medtprod^q F\]
then
\begin{align*}
(h\after f)(u\tprod v)&=h(\tprods{x}{1}{p}\tprod\tprods{y}{1}{q})\\
	&=h(x_1)\cdots h(x_p)h(y_1)\cdots h(y_q)\\
	&=(x_1\tprod 1)\cdots(x_p\tprod 1)(1\tprod y_1)\cdots(1\tprod y_q)\\
	&=(\tprods{x}{1}{p})\tprod(\tprods{y}{1}{q})\\
	&=u\tprod v
\end{align*}
By multilinearity, it follows that \(h\after f=\iota\), so \(f\)~is injective and \(h\)~is surjective. However  for \(x\in E\) and \(y\in F\),
\[h(y\tprod x)=(1\tprod y)(x\tprod 1)=x\tprod y=(x\tprod 1)(1\tprod y)=h(x\tprod y)\]
so \(h\)~is not injective and \(f\)~is not surjective if \(E\ne 0\) and \(F\ne 0\).

Intuitively it makes sense that \((\medtprod E)\tprod(\medtprod F)\) can be viewed as a subspace of~\(\medtprod(E\dsum F)\) since elements in the former can be represented in the latter, but not a subalgebra since factors from~\(E\) commute with factors from~\(F\) in products in the former but not in the latter.

Similar remarks apply to \((\medtprod E)\stprod(\medtprod F)\) and~\(\medtprod(E\dsum F)\).
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
For a linear map \(\varphi:E\to E\), the following diagram commutes:
\begin{diagram}
E			&\rTo^{\varphi}					&E\\
\dTo<i		&								&\dTo>i\\
\medtprod E	&\rTo_{\theta_{\tprod}(\varphi)}&\medtprod E
\end{diagram}
\end{rmk}

\begin{rmk}
There is a connection between the derivation and the trace of a linear transformation. If \(E\)~is an \(n\)-dimensional vector space and \(\varphi:E\to E\) is linear, let \(\Delta:E^n\to\Gamma\) be a nonzero determinant function in~\(E\). Recall
\[\tr\varphi\mult\Delta(x_1,\ldots,x_n)=\sum_{i=1}^n\Delta(x_1,\ldots,\varphi x_i,\ldots,x_n)\]
By the universal property of the tensor product~\(\medtprod^n E\), there is an induced linear function \(\Delta_{\tprod}:\medtprod^n E\to\Gamma\) with \(\Delta_{\tprod}(x_1\tprod\cdots\tprod x_n)=\Delta(x_1,\ldots,x_n)\).
Now
\begin{align*}
\tr\varphi\mult\Delta_{\tprod}(x_1\tprod\cdots\tprod x_n)&=\sum_{i=1}^n \Delta_{\tprod}(x_1\tprod\cdots\tprod\varphi x_i\tprod\cdots\tprod x_n)\\
	&=\Delta_{\tprod}(\sum_{i=1}^n x_1\tprod\cdots\tprod\varphi x_i\tprod\cdots\tprod x_n)\\
	&=\Delta_{\tprod}(\theta_{\circled{n}}(\varphi)(x_1\tprod\cdots\tprod x_n))
\end{align*}
It follows that
\[\Delta_{\tprod}\after(\tr\varphi\mult\iota)=\tr\varphi\mult\Delta_{\tprod}=\Delta_{\tprod}\after\theta_{\circled{n}}(\varphi)\]
Since \(\theta_{\circled{n}}\)~is linear by~(3.7), it follows that the trace is also linear. If \(\psi:E\to E\) is another linear transformation, then it follows from~(3.8) that
\[\tr(\varphi\psi)-\tr(\psi\varphi)=\tr(\varphi\psi-\psi\varphi)=\tr\varphi\mult\tr\psi-\tr\psi\mult\tr\varphi=0\]
so \(\tr(\varphi\psi)=\tr(\psi\varphi)\).\footnote{This is what problem~3.7.3 was supposed to be.}
\end{rmk}

\subsection*{\S~7}
\begin{exer}[1]
If \(u_1=a_1\tprod b_1\ne 0\) and \(u_2=a_2\tprod b_2\) are decomposable tensors, then \(u_1+u_2\) is decomposable if and only if \(a_2=\lambda a_1\) or \(b_2=\mu b_1\) for some \(\lambda,\mu\in\Gamma\).
\end{exer}
\begin{proof}
By the remark after problem~1.15.1 above.
\end{proof}

\subsection*{\S~11}
\begin{rmk}
Define
\[M^p_q(E^*,E)=\underbrace{E^*\times\cdots\times E^*}_p\times\underbrace{E\times\cdots\times E}_q\]
Consider the \((p+q)\)-linear map \(\medtprod^p_q:M^p_q(E^*,E)\to\medtprod^p_q(E^*,E)\) defined by
\[\medtprod^p_q(x^*_1,\ldots,x^*_p\,;\,x_1,\ldots,x_q)=(\tprods{x^*}{1}{p})\tprod(\tprods{x}{1}{q})\]
It is easy to verify that \(\medtprod^p_q\)~satisfies the following universal property:
\begin{quote}
If \(\varphi:M^p_q(E^*,E)\to F\) is any \((p+q)\)-linear map, then there is a unique linear map \(f:\medtprod^p_q(E^*,E)\to F\) with \(f\after\medtprod^p_q=\varphi\):
\begin{diagram}[nohug]
M^p_q(E^*,E)		&\rTo^{\varphi}	&F\\
\dTo<{\medtprod^p_q}&\ruTo>f		&\\
\medtprod^p_q(E^*,E)&				&
\end{diagram}
\end{quote}
By category theoretic abstract nonsense, this property characterizes \(\medtprod^p_q(E^*,E)\) uniquely up to isomorphism.
\end{rmk}

\begin{rmk}
In the finite-dimensional case, the following are familiar types of mixed tensors in \(E^*,E\):
\begin{center}
\begin{tabular}{|c|l|}
\hline
\textbf{Bidegree}&\textbf{Type}\\
\hline
(0,0)&Scalar\\
(0,1)&Vector\\
(1,0)&Linear function\\
(1,1)&Linear transformation\\
\hline
\end{tabular}
\end{center}
\end{rmk}

\subsection*{\S~12}
\begin{rmk}
The mixed tensor algebra~\(\medtprod(E^*,E)\) can be constructed in two ways:
\begin{itemize}
\item Top down, as \(\medtprod(E^*,E)=(\medtprod E^*)\tprod(\medtprod E)\).
\item Bottom up, as
\[\medtprod(E^*,E)=\sum_{p,q}\medtprod^p_q(E^*,E)\]
\end{itemize}
An element of this algebra is a finite sum of decomposable homogeneous mixed tensors. As an example of multiplication, if \(x^*\tprod x\) and \(y^*\tprod y\) are (1,1)-tensors, then
\[(x^*\tprod x)(y^*\tprod y)=x^*\tprod y^*\tprod x\tprod y\]
is a (2,2)-tensor.
\end{rmk}

\begin{rmk}
By a remark in \S~2 of chapter~2 above, \(\medtprod(E^*,E)\)~satisfies a universal property as an associative unital algebra and is unique up to isomorphism.
\end{rmk}

\begin{rmk}
By a remark in \S~5 of chapter~2 above, \(\medtprod(E^*,E)\)~is a \emph{graded algebra} under the simple gradation in~(2.15).
\end{rmk}

\begin{rmk}
By~(1.25), \(\medtprod(E^*,E)\) is dual to itself and to \(\medtprod(E,E^*)\).
\end{rmk}

\subsection*{\S~14}
\begin{rmk}
Let \(\alpha^{\circled{p}}\) denote the restriction of~\(\alpha^{\tprod}\) to \(\medtprod^p E^*\) and \(\alpha_{\circled{q}}\) the restriction of~\(\alpha_{\tprod}\) to \(\medtprod^q E\). Then \(\alpha^{\circled{p}}\) and \(\alpha_{\circled{q}}\) are automorphisms, and
\[T_{\alpha}=(\alpha^{\circled{p}})^{-1}\tprod\alpha_{\circled{q}}\]
is an automorphism of~\(\medtprod^p_q(E^*,E)\) which extends to \((\alpha^{\tprod})^{-1}\tprod\alpha_{\tprod}\) on~\(\medtprod(E^*,E)\).
\end{rmk}

\begin{rmk}
The mapping \(T:GL(E)\to GL(\medtprod^p_q(E^*,E))\) given by \(\alpha\mapsto T_{\alpha}\) is a group homomorphism. The map~\(T_{\alpha}\) is tensorial if \(\alpha\)~is in the center of~\(GL(E)\).
\end{rmk}

\begin{rmk}
We have \(T_{\alpha}^*=T_{\alpha^{-1}}\). Indeed, since \((\alpha^{\tprod})^{-1}=(\alpha^{-1})^{\tprod}\),
\begin{align*}
\sprod{y^*\tprod y}{T_{\alpha}(x^*\tprod x)}&=\sprod{y^*\tprod y}{(\alpha^{\tprod})^{-1}x^*\tprod\alpha_{\tprod}x}\\
	&=\sprod{y^*}{\alpha_{\tprod}x}\sprod{(\alpha^{\tprod})^{-1}x^*}{y}\\
	&=\sprod{\alpha^{\tprod}y^*}{x}\sprod{x^*}{(\alpha^{-1})_{\tprod}y}\\
	&=\sprod{\alpha^{\tprod}y^*\tprod(\alpha^{-1})_{\tprod}y}{x^*\tprod x}\\
	&=\sprod{T_{\alpha^{-1}}(y^*\tprod y)}{x^*\tprod x}
\end{align*}
\end{rmk}

\begin{rmk}
Writing \(\medtprod^p_q(E^*,E)\tprod\medtprod^p_q(E^*,E)=\medtprod^p_q(E^*\tprod E^*,E\tprod E)\) (see problem~10), we have \(T_{\alpha}\tprod T_{\beta}=T_{\alpha\tprod\beta}\). Writing \(\medtprod^p_q(E^*,E)\tprod\medtprod^r_s(E^*,E)=\medtprod^{p+r}_{q+s}(E^*,E)\), we have \(T_{\alpha}\tprod T_{\alpha}=T_{\alpha}\).
\end{rmk}

\begin{rmk}
If \(e_i,e^{*i}\) and \(\bar{e}_i,\bar{e}^{*i}\) are pairs of dual bases in \(E,E^*\) and \(\alpha\)~is the change of basis transformation \(e_i\mapsto\bar{e}_i\) in~\(E\), then \(\alpha^{*-1}\)~is the corresponding change of basis transformation \(e^{*i}\mapsto\bar{e}^{*i}\) in~\(E^*\).\footnote{See the remark in chapter~III, \S~3 of~\cite{greub1} above.} Therefore in \(\medtprod^p_q(E^*,E)\),
\[T_{\alpha}(e^{\nu_1\cdots\nu_p}_{\mu_1\cdots\mu_q})=\bar{e}^{\nu_1\cdots\nu_p}_{\mu_1\cdots\mu_q}\]
In other words, \(T_{\alpha}\)~is the induced change of basis transformation in \(\medtprod^p_q(E^*,E)\). It follows that a mapping is tensorial if and only if its matrix is invariant under this type of change of basis.
\end{rmk}

\begin{rmk}
For \(\alpha=\lambda\iota\) with \(\lambda\ne0\), \(\alpha^{*-1}=\lambda^{-1}\iota\), so \(T_{\alpha}=\lambda^{q-p}\iota\) on \(\medtprod^p_q(E^*,E)\).
\end{rmk}

\begin{rmk}
If \(E\)~is finite-dimensional and we identify \(\medtprod^1_1(E^*,E)\) with \(L(E;E)\) in the natural way, then for any automorphism \(\alpha\) of~\(E\),
\[T_{\alpha}(a^*\tprod b)=\alpha\after(a^*\tprod b)\after\alpha^{-1}\]
\end{rmk}

\begin{exer}[1]
If \(e_{\nu},e^{*\nu}\) and \(\bar{e}_{\nu},\bar{e}^{*\nu}\) are pairs of dual bases in \(E,E^*\) related by
\[\bar{e}_{\nu}=\sum_{\lambda}\alpha_{\nu}^{\lambda}e_{\lambda}\qquad\text{and}\qquad\bar{e}^{*\nu}=\sum_{\lambda}\beta^{\nu}_{\lambda}e^{*\lambda}\]
then the components of a mixed tensor \(w\in\medtprod^p_q(E^*,E)\) are related by
\[\bar{\zeta}^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}=\sum_{(\kappa),(\lambda)}\alpha_{\nu_1}^{\lambda_1}\cdots\alpha_{\nu_p}^{\lambda_p}\beta^{\mu_1}_{\kappa_1}\cdots\beta^{\mu_q}_{\kappa_q}\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}\]
\end{exer}
\begin{proof}
In this proof, we use the Einstein summation convention to simplify the notation. Since \(e_{\kappa}=\beta_{\kappa}^{\mu}\bar{e}_{\mu}\) and \(e^{*\lambda}=\alpha^{\lambda}_{\nu}\bar{e}^{*\nu}\), we have
\begin{align*}
w&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}e^{\lambda_1\cdots\lambda_p}_{\kappa_1\cdots\kappa_q}\\
	&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}e^{*\lambda_1}\tprod\cdots\tprod e^{*\lambda_p}\tprod e_{\kappa_1}\tprod\cdots\tprod e_{\kappa_q}\\
	&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}(\alpha^{\lambda_1}_{\nu_1}\bar{e}^{*\nu_1})\tprod\cdots\tprod(\alpha^{\lambda_p}_{\nu_p}\bar{e}^{*\nu_p})\tprod(\beta_{\kappa_1}^{\mu_1}\bar{e}_{\mu_1})\tprod\cdots\tprod(\beta_{\kappa_q}^{\mu_q}\bar{e}_{\mu_q})\\
	&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}\alpha^{\lambda_1}_{\nu_1}\cdots\alpha^{\lambda_p}_{\nu_p}\beta_{\kappa_1}^{\mu_1}\cdots\beta_{\kappa_q}^{\mu_q}\bar{e}^{\nu_1\cdots\nu_p}_{\mu_1\cdots\mu_q}
\end{align*}
Therefore
\[\bar{\zeta}^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}=\alpha_{\nu_1}^{\lambda_1}\cdots\alpha_{\nu_p}^{\lambda_p}\beta^{\mu_1}_{\kappa_1}\cdots\beta^{\mu_q}_{\kappa_q}\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}\qedhere\]
\end{proof}
\begin{rmk}
This relationship serves as the basis of an alternative definition of a \emph{tensor} as a multidimensional array of scalars which transforms according to this rule under change of basis.
\end{rmk}

\begin{exer}[3]
For \(u^*\in\medtprod^p E^*\) and \(u\in\medtprod^p E\),
\[\sprod{u^*}{u}=(C^1_1)^p(u^*\tprod u)\]
\end{exer}
\begin{proof}
By induction on~\(p\). For \(p=0\) the result is trivial, and for \(p=1\) it follows from the definition of~\(C^1_1\). For \(p>1\), write \(u^*=u^{*1}\tprod\tilde{u}^*\) and \(u=u_1\tprod\tilde{u}\). Then
\begin{align*}
(C^1_1)^p(u^*\tprod u)&=(C^1_1)^{p-1}(C^1_1(u^*\tprod u))\\
	&=\sprod{u^{*1}}{u_1}(C^1_1)^{p-1}(\tilde{u}^*\tprod\tilde{u})\\
	&=\sprod{u^{*1}}{u_1}\sprod{\tilde{u}^*}{\tilde{u}}\\
	&=\sprod{u^*}{u}\qedhere
\end{align*}
\end{proof}

\begin{exer}[4]
If the mappings
\[\Phi:\medtprod^p_q(E^*,E)\to\medtprod^r_s(E^*,E)\quad\text{and}\quad\Phi^*:\medtprod^q_p(E^*,E)\from\medtprod^s_r(E^*,E)\]
are dual and \(\Phi\)~is tensorial, then \(\Phi^*\)~is tensorial.
\end{exer}
\begin{proof}
If \(\alpha\)~is an automorphism of~\(E\), then by a remark above
\[\Phi^*\after T_{\alpha}=\Phi^*\after T_{\alpha^{-1}}^*=(T_{\alpha^{-1}}\after\Phi)^*=(\Phi\after T_{\alpha^{-1}})^*=T_{\alpha^{-1}}^*\after\Phi^*=T_{\alpha}\after\Phi^*\qedhere\]
\end{proof}

\begin{exer}[5]
The sum, composite, and tensor product of tensorial mappings are tensorial.
\end{exer}
\begin{proof}
If \(\Phi\) and~\(\Psi\) are tensorial, then it is obvious that \(\Phi+\Psi\) and \(\Phi\after\Psi\) are also tensorial. By a remark above,
\begin{align*}
(\Phi\tprod\Psi)\after T_{\alpha}&=(\Phi\tprod\Psi)\after(T_{\alpha}\tprod T_{\alpha})\\
	&=(\Phi\after T_{\alpha})\tprod(\Psi\after T_{\alpha})\\
	&=(T_{\alpha}\after\Phi)\tprod(T_{\alpha}\after\Psi)\\
	&=(T_{\alpha}\tprod T_{\alpha})\after(\Phi\tprod\Psi)\\
	&=T_{\alpha}\after(\Phi\tprod\Psi)
\end{align*}
so \(\Phi\tprod\Psi\) is tensorial.
\end{proof}

\begin{exer}[6]
Let \(\Gamma\)~have characteristic zero. If \(\Phi:\medtprod^p_q(E^*,E)\to\medtprod^r_s(E^*,E)\) is a nonzero tensorial mapping, then \(r-p=s-q\).
\end{exer}
\begin{proof}
By a remark above with \(\lambda=2\), \(2^{q-p}\Phi=2^{s-r}\Phi\) since \(\Phi\)~is tensorial. It follows that \(2^{q-p+r-s}=1\) since \(\Phi\ne 0\), so \(q-p+r-s=0\).
\end{proof}

\begin{exer}[7]
If \(E\)~is finite-dimensional and for \(a\in E^*\tprod E\) the linear map~\(\mu(a)\) is defined by \(z\mapsto a\tprod z\) for \(z\in E\), then \(\mu(a)\)~is tensorial if and only if \(a=\lambda t\) where \(t\)~is the unit tensor.
\end{exer}
\begin{proof}
By direct computation, it is easily verified that \(\mu(a)\)~is tensorial if and only if \(T_{\alpha}(a)=a\) for all automorphisms \(\alpha\) of~\(E\). By a remark above, the latter is true if and only if \(a\), when viewed as a transformation of~\(E\), is preserved under conjugation, which is true if and only if it is a scalar transformation. Since the unit tensor corresponds to the identity transformation, the result follows.
\end{proof}

\begin{exer}[8]
If \(E\)~is finite-dimensional, then every tensorial map \(\Phi:E^*\tprod E\to\Gamma\) is of the form \(\Phi=\lambda\mult C\) where \(C\)~is the contraction operator.
\end{exer}
\begin{proof}
This follows from problem~1.3.7 applied to \(\Phi\after\tprod\).
\end{proof}

\begin{exer}[11]
Let \(\Gamma\)~have characteristic zero.
\begin{enumerate}
\item[(a)] If \(u\in\medtprod^p_q(E^*,E)\) is a nonzero invariant tensor, then \(p=q\).
\item[(b)] If \(E\)~has finite dimension, then \(u\in E^*\tprod E\) is invariant if and only if \(u=\lambda t\) where \(t\)~is the unit tensor.
\item[(c)] If \(E\)~has infinite dimension and \(u\in E^*\tprod E\) is invariant, then \(u=0\).
\item[(d)] If \(E\)~has finite dimension, then \(u\)~is invariant if and only if the components of~\(u\) are the same with respect to every pair of dual bases.
\end{enumerate}
\end{exer}
\begin{proof}
For~(a), taking \(\alpha=2\iota\) we have \(T_{\alpha}=2^{q-p}\iota\) by a remark above, so \(2^{q-p}u=u\) since \(u\)~is invariant, which implies \(2^{q-p}=1\) since \(u\ne 0\), which implies \(q-p=0\).

For (b) and~(c), if we view~\(u\) as a linear transformation of~\(E\), then \(u\)~is invariant if and only if it is preserved under conjugation, which is true if and only if it is a scalar transformation. Now (b) follows since the unit tensor corresponds to the identity transformation when \(\dim E<\infty\), and (c) follows since there is no unit tensor when \(\dim E=\infty\).

For~(d), since \(T_{\alpha}\)~is a change of basis transformation by a remark above.
\end{proof}
\begin{rmk}
Intuitively, an invariant tensor represents a geometric quantity whose description does not depend upon the choice of coordinate system.
\end{rmk}

\subsection*{\S~15}
\begin{rmk}
The induced inner products in \(\medtprod^p E\) and \(\medtprod E\) are just the induced scalar products from \S~8 with \(E^*=E\) under the inner product.
\end{rmk}

\subsection*{\S~17}
\begin{rmk}
The metric tensors encode all of the information required to compute inner products, and hence metric properties (length, angle, etc.), in \(E\) and~\(E^*\).
\end{rmk}

\begin{rmk}
Since \(E\)~and the orthonormal basis \(e_{\nu}\) are self-dual under the inner product, the metric tensor \(g=\sum_{\nu}e_{\nu}\tprod e_{\nu}\) is just the unit tensor in~\(E\tprod E\). It follows that \(g\tprod g\) is the unit tensor in \((E\tprod E)\tprod(E\tprod E)\), and hence is the metric tensor in~\(E\tprod E\), and more generally
\[\underbrace{g\tprod\cdots\tprod g}_p\]
is the metric tensor in~\(\medtprod^p E\).\footnote{See problem~2(b) for another proof.}
\end{rmk}

\begin{rmk}
We have
\[\tau_{\tprod}(g)=\sum_{\nu}\tau e_{\nu}\tprod\tau e_{\nu}=\sum_{\nu}e^{*\nu}\tprod e^{*\nu}=g^*\]
so \(\tau_{\tprod}^{-1}(g^*)=g\). This yields
\begin{align*}
\iprod{x^*}{y^*}&=\iprod{\tau^{-1}x^*}{\tau^{-1}y^*}\\
	&=\sprod{g^*}{\tau^{-1}x^*\tprod\tau^{-1}y^*}\\
	&=\sprod{g^*}{\tau_{\tprod}^{-1}(x^*\tprod y^*)}\\
	&=\sprod{x^*\tprod y^*}{\tau_{\tprod}^{-1}g^*}\\
	&=\sprod{x^*\tprod y^*}{g}
\end{align*}
\end{rmk}

\begin{exer}[2]
Let \(E,E^*\) be a pair of dual finite-dimensional Euclidean spaces with metric tensors \(g\in E\tprod E\) and \(g^*\in E^*\tprod E^*\).
\begin{enumerate}
\item[(a)] \(C^1_2(g^*\tprod g)\) is the unit tensor in~\(E^*\tprod E\).
\item[(b)] \(\underbrace{g\tprod\cdots\tprod g}_p\) is the metric tensor of~\(\medtprod^p E\).
\end{enumerate}
\end{exer}
\begin{proof}
Let \(e_{\nu},e^{*\nu}\) be a pair of dual orthonormal bases in \(E,E^*\). We know that \(g=\sum_{\nu}e_{\nu}\tprod e_{\nu}\) and \(g^*=\sum_{\mu}e^{*\mu}\tprod e^{*\mu}\), so
\begin{align*}
C^1_2(g^*\tprod g)&=\sum_{\mu,\nu}C^1_2(e^{*\mu}\tprod e^{*\mu}\tprod e_{\nu}\tprod e_{\nu})\\
	&=\sum_{\mu,\nu}\sprod{e^{*\mu}}{e_{\nu}}e^{*\mu}\tprod e_{\nu}\\
	&=\sum_{\nu}e^{*\nu}\tprod e_{\nu}
\end{align*}
which is the unit tensor in~\(E^*\tprod E\).

Writing \(\medtprod^p(E\tprod E)=(\medtprod^p E)\tprod(\medtprod^p E)\), we have
\begin{align*}
\underbrace{g\tprod\cdots\tprod g}_p&=\bigl(\sum_{\nu_1}e_{\nu_1}\tprod e_{\nu_1}\bigr)\tprod\cdots\tprod\bigl(\sum_{\nu_p}e_{\nu_p}\tprod e_{\nu_p}\bigr)\\
	&=\sum_{(\nu)}(\tprods{e}{\nu_1}{\nu_p})\tprod(\tprods{e}{\nu_1}{\nu_p})
\end{align*}
which is the metric tensor of~\(\medtprod^p E\) since \(\tprods{e}{\nu_1}{\nu_p}\) is an orthonormal basis in~\(\medtprod^p E\).
\end{proof}

\begin{exer}[4]
Let \(E,E^*\) be a pair of dual finite-dimensional Euclidean spaces.
\begin{enumerate}
\item[(a)] If \(\varphi=a^*\tprod b\) is a linear transformation of~\(E\), then \(\adj{\varphi}=\tau b\tprod\tau^{-1}a^*\).
\item[(b)] The inner product induced in~\(L(E;E)\) is given by
\[\iprod{\varphi}{\psi}=\tr(\varphi\after\adj{\psi})\]
\end{enumerate}
\end{exer}
\begin{proof}
For~(a), if \(\varphi=a^*\tprod b\) in~\(E^*\tprod E\) then \(\varphi=\tau^{-1}a^*\tprod b\) in~\(E\tprod E\), so \(\adj{\varphi}=b\tprod\tau^{-1}a^*\) in~\(E\tprod E\) by problem~1.30.1 and \(\adj{\varphi}=\tau b\tprod\tau^{-1}a^*\) in~\(E^*\tprod E\).

For~(b), if \(\varphi=a^*\tprod b\) and \(\psi=c^*\tprod d\), then
\begin{align*}
\varphi\after\adj{\psi}&=(a^*\tprod b)\after(\tau d\tprod\tau^{-1}c^*)\\
	&=\sprod{a^*}{\tau^{-1}c^*}(\tau d\tprod b)\\
	&=\iprod{\tau^{-1}a^*}{\tau^{-1}c^*}(\tau d\tprod b)\\
	&=\iprod{a^*}{c^*}(\tau d\tprod b)
\end{align*}
so
\begin{align*}
\tr(\varphi\after\adj{\psi})&=\iprod{a^*}{c^*}\sprod{\tau d}{b}\\
	&=\iprod{a^*}{c^*}\iprod{b}{d}\\
	&=\iprod{a^*\tprod b}{c^*\tprod d}\\
	&=\iprod{\varphi}{\psi}
\end{align*}
The general case follows by multilinearity.
\end{proof}

\begin{exer}[7]
If \(E,E^*\) is a pair of dual finite-dimensional Euclidean spaces and \(x_{\nu},x^{*\nu}\) a pair of dual bases, then
\[g^*=\sum_{\nu,\mu}\iprod{x_{\nu}}{x_{\mu}}x^{*\nu}\tprod x^{*\mu}\qquad\text{and}\qquad g=\sum_{\nu,\mu}\iprod{x^{*\nu}}{x^{*\mu}}x_{\nu}\tprod x_{\mu}\]
\end{exer}
\begin{proof}
Since \(x_{\nu}\tprod x_{\mu}\) and \(x^{*\nu}\tprod x^{*\mu}\) are dual bases of \(E\tprod E\) and \(E^*\tprod E^*\), we have
\[g^*=\sum_{\nu,\mu}g_{\nu\mu}x^{*\nu}\tprod x^{*\mu}\qquad\text{and}\qquad g=\sum_{\nu,\mu}g^{\nu\mu}x_{\nu}\tprod x_{\mu}\]
where
\[g_{\nu\mu}=\sprod{g^*}{x_{\nu}\tprod x_{\mu}}=\iprod{x_{\nu}}{x_{\mu}}\]
and
\[g^{\nu\mu}=\sprod{x^{*\nu}\tprod x^{*\mu}}{g}=\iprod{x^{*\nu}}{x^{*\mu}}\qedhere\]
\end{proof}
\begin{rmk}
It follows that the components of~\(g^*\) vary in the same way as the basis vectors in~\(E\) under a change of basis, while the components of~\(g\) vary inversely. This is why \(g^*\)~is called ``covariant'' and \(g\)~is called ``contravariant''.
\end{rmk}

\subsection*{\S~18}
\begin{rmk}
The multiplication in the algebra~\(T^{\tdot}(E)\) is defined by
\[\Phi\fprod\Psi=\sum_{p,q}\Phi_p\fprod\Psi_q\]
where \(\Phi=\sum_p\Phi_p\) with \(\Phi_p\in T^p(E)\), \(\Psi=\sum_q\Psi_q\) with \(\Psi_q\in T^q(E)\), and \(\Phi_p\fprod\Psi_q\) is given by~(3.17).\footnote{Compare with the definition in subsection~3.2.} This definition makes~\(T^{\tdot}(E)\) into a \emph{graded algebra} where the elements of~\(T^p(E)\) are homogeneous of degree~\(p\).
\end{rmk}

\begin{rmk}
The function \(\fprods{f}{1}{p}\) (\(f_{\nu}\in T^1(E)\)) is \emph{decomposable} in~\(T^p(E)\).
\end{rmk}

\begin{rmk}
For a given linear map \(\varphi:E\to F\), to see that \(\varphi^*:T^{\tdot}(E)\from T^{\tdot}(F)\) is a homomorphism homogeneous of degree zero, first note that \(\varphi^*\Psi\in T^p(E)\) for \(\Psi\in T^p(F)\) by the definition since \(\varphi\)~is linear and \(\Psi\)~is \(p\)-linear. Moreover,
\[\varphi^*(\lambda\Phi+\mu\Psi)=\lambda\varphi^*\Phi+\mu\varphi^*\Psi\qquad(\Phi,\Psi\in T^p(F))\]
so \(\varphi^*:T^p(E)\from T^p(F)\) is linear. This map extends via direct sum over all~\(p\) to the linear map \(\varphi^*:T^{\tdot}(E)\from T^{\tdot}(F)\) which is homogeneous of degree zero. For \(\Phi\in T^p(F)\) and \(\Psi\in T^q(F)\),
\begin{align*}
(\varphi^*(\Phi\fprod\Psi))(x_1,\ldots,x_{p+q})&=(\Phi\fprod\Psi)(\varphi x_1,\ldots,\varphi x_{p+q})\\
	&=\Phi(\varphi x_1,\ldots,\varphi x_p)\mult\Psi(\varphi x_{p+1},\ldots,\varphi x_{p+q})\\
	&=(\varphi^*\Phi)(x_1,\ldots,x_p)\mult(\varphi^*\Psi)(x_{p+1},\ldots,x_{p+q})\\
	&=((\varphi^*\Phi)\fprod(\varphi^*\Psi))(x_1,\ldots,x_{p+q})
\end{align*}
so
\[\varphi^*(\Phi\fprod\Psi)=(\varphi^*\Phi)\fprod(\varphi^*\Psi)\]
This equation extends to all \(\Phi,\Psi\in T^{\tdot}(F)\) by bilinearity of the multiplication in \(T^{\tdot}(E)\) and~\(T^{\tdot}(F)\) and linearity of~\(\varphi^*\), so \(\varphi^*\)~is a homomorphism. Under the natural identifications \(E^*=T^1(E)\) and \(F^*=T^1(F)\), \(\varphi^*\)~is an extension of the dual map \(\varphi^*:E^*\from F^*\) of~\(\varphi\) and
\[\varphi^*(\fprods{y^*}{1}{p})=\fprods{\varphi^*y^*}{1}{p}\qquad(y^*_{\nu}\in F^*)\]
\end{rmk}

\begin{rmk}
For a linear map \(\varphi:E\to E\), the map \(\theta^T(\varphi):T^{\tdot}(E)\from T^{\tdot}(E)\) is seen to be a derivation homogeneous of degree zero by an argument similar to that in the previous remark. In this case for \(\Phi\in T^p(E)\) and \(\Psi\in T^q(E)\) with \(p,q\ge 1\),
\begin{align*}
(\theta^T(\varphi)(\Phi\fprod\Psi))(x_1,\ldots,x_{p+q})&=\sum_{i=1}^{p+q}(\Phi\fprod\Psi)(x_1,\ldots,\varphi x_i,\ldots,x_{p+q})\\
	&=\sum_{i=1}^{p}\Phi(x_1,\ldots,\varphi x_i,\ldots,x_p)\mult\Psi(x_{p+1},\ldots,x_{p+q})\\
	&\qquad+\sum_{i=p+1}^{p+q}\Phi(x_1,\ldots,x_p)\mult\Psi(x_{p+1},\ldots,\varphi x_i,\ldots,x_{p+q})\\
	&=(\theta^T(\varphi)(\Phi))(x_1,\ldots,x_p)\mult\Psi(x_{p+1},\ldots,x_{p+q})\\
	&\qquad+\Phi(x_1,\ldots,x_p)\mult(\theta^T(\varphi)(\Psi))(x_{p+1},\ldots,x_{p+q})\\
	&=\bigl(\theta^T(\varphi)(\Phi)\fprod\Psi+\Phi\fprod\theta^T(\varphi)(\Psi)\bigr)(x_1,\ldots,x_{p+q})
\end{align*}
so
\[\theta^T(\varphi)(\Phi\fprod\Psi)=\theta^T(\varphi)(\Phi)\fprod\Psi+\Phi\fprod\theta^T(\varphi)(\Psi)\]
Under the identification \(E^*=T^1(E)\), \(\theta^T(\varphi)\)~extends the dual map \(\varphi^*:E^*\from E^*\) and
\[\theta^T(\varphi)(\fprods{x^*}{1}{p})=\sum_{i=1}^p x^*_1\fprod\cdots\fprod\varphi^*x^*_i\fprod\cdots\fprod x^*_p\qquad(x^*_{\nu}\in E^*)\]
\end{rmk}

\subsection*{\S~19}
\begin{rmk}
The definitions of the substitution operators \(i_{\nu}(h)\), \(i_A(h)\), and \(i_S(h)\) in this subsection are stated on~\(T^p(E)\) and understood to extend via direct sum over all~\(p\) to~\(T^{\tdot}(E)\). The operator \(i_{\nu}(h)\)~acts as the identity on~\(T^p(E)\) for \(p<\nu\). The definitions of \(i_A(h)\) and~\(i_S(h)\) are inappropriate for use with the algebras of skew-symmetric and symmetric multilinear functions, respectively. See below.
\end{rmk}

\begin{rmk}
The operator~\(i_A(h)\) is seen to be an antiderivation with respect to the canonical involution in~\(T^{\tdot}(E)\), while \(i_S(h)\)~is seen to be a derivation in~\(T^{\tdot}(E)\).
\end{rmk}

\subsection*{\S~21}
\begin{rmk}
By definition,
\[T_{\tdot}(E)=\sum_p T_p(E)=\sum_p T^p(E^*)=T^{\tdot}(E^*)\]
The multiplication makes~\(T_{\tdot}(E)\) into a \emph{graded algebra} where the elements of~\(T_p(E)\) are homogeneous of degree~\(p\).
\end{rmk}

\begin{rmk}
For a linear map \(\varphi:E\to F\), the dual map \(\varphi^*:E^*\from F^*\) induces the homomorphism \(\varphi_*=(\varphi^*)^*:T^{\tdot}(E^*)\to T^{\tdot}(F^*)\), homogeneous of degree zero. Under the natural identifications \(E=T_1(E)\) and \(F=T_1(F)\), \(\varphi_*\)~extends \(\varphi\) and
\[\varphi_*(\fprods{x}{1}{p})=\fprods{\varphi x}{1}{p}\qquad(x_{\nu}\in E)\]
\end{rmk}

\begin{rmk}
For a linear map \(\varphi:E\to E\), the dual map \(\varphi^*:E^*\from E^*\) induces the derivation \(\theta_T(\varphi)=\theta^T(\varphi^*):T^{\tdot}(E^*)\to T^{\tdot}(E^*)\) which is homogeneous of degree zero. Under the identification \(E=T_1(E)\), \(\theta_T(\varphi)\)~extends \(\varphi\) and
\[\theta_T(\varphi)(\fprods{x}{1}{p})=\sum_{i=1}^p x_1\fprod\cdots\fprod\varphi x_i\fprod\cdots\fprod x_p\qquad(x_{\nu}\in E)\]
\end{rmk}

\subsection*{\S~22}
\begin{rmk}
The scalar product between \(T^p(E)\) and~\(T_p(E)\) is given by
\[\sprod{\fprods{f}{1}{p}}{\fprods{x}{1}{p}}=\sprod{f_1}{x_1}\cdots\sprod{f_p}{x_p}\qquad(f_{\nu}\in E^*, x_{\nu}\in E)\]
It follows that the isomorphisms \(\medtprod^p E^*\iso T^p(E)\) and \(\medtprod^p E\iso T_p(E)\) preserve scalar products.
\end{rmk}

\begin{rmk}
A scalar product between \(T^{\tdot}(E)\) and~\(T_{\tdot}(E)\) is induced by
\[\sprod{\Phi}{\Psi}=\sum_p\sprod{\Phi_p}{\Psi_p}\]
where \(\Phi=\sum_p\Phi_p\) with \(\Phi_p\in T^p(E)\) and \(\Psi=\sum_p\Psi_p\) with \(\Psi_p\in T_p(E)\).
It follows from the previous remark that the isomorphisms \(\medtprod E^*\iso T^{\tdot}(E)\) and \(\medtprod E\iso T_{\tdot}(E)\) preserve scalar products.
\end{rmk}

\begin{rmk}
For a linear map \(\varphi:E\to F\), the homomorphisms \(\varphi_*:T_{\tdot}(E)\to T_{\tdot}(F)\) and \(\varphi^*:T^{\tdot}(E)\from T^{\tdot}(F)\) are dual. This can be proved by direct computation or by appealing to commutativity of the following diagrams:
\begin{diagram}
\medtprod E^*	&\lTo{\varphi^{\tprod}}	&\medtprod F^*	&&\medtprod E	&\rTo^{\varphi_{\tprod}}	&\medtprod F\\
\dTo<{\iso}		&						&\dTo>{\iso}	&&\dTo<{\iso}	&							&\dTo>{\iso}\\
T^{\tdot}(E)	&\lTo^{\varphi^*}		&T^{\tdot}(F)	&&T_{\tdot}(E)	&\rTo^{\varphi_*}			&T_{\tdot}(F)
\end{diagram}
\end{rmk}

\begin{rmk}
For a linear map \(\varphi:E\to E\), the derivations \(\theta_T(\varphi):T_{\tdot}(E)\to T_{\tdot}(E)\) and \(\theta^T(\varphi):T^{\tdot}(E)\from T^{\tdot}(E)\) are dual, from the following diagrams:
\begin{diagram}
\medtprod E^*	&\lTo{\theta^{\tprod}(\varphi)}	&\medtprod E^*	&&\medtprod E	&\rTo^{\theta_{\tprod}(\varphi)}&\medtprod E\\
\dTo<{\iso}		&								&\dTo>{\iso}	&&\dTo<{\iso}	&								&\dTo>{\iso}\\
T^{\tdot}(E)	&\lTo^{\theta^T(\varphi)}		&T^{\tdot}(E)	&&T_{\tdot}(E)	&\rTo^{\theta_T(\varphi)}		&T_{\tdot}(E)
\end{diagram}
\end{rmk}

\subsection*{\S~23}
\begin{rmk}
We have
\[T^p_q(E)\iso(\medtprod^pE^*)\tprod(\medtprod^q E)=\medtprod^p_q(E^*,E)\]
and
\[T(E)\iso(\medtprod E^*)\tprod(\medtprod E)=\medtprod(E^*,E)\]
It follows from a remark in \S~12 above that \(T(E)\)~is a \emph{graded algebra} under the simple gradation in~(2.15). Also \(T^p_q(E)\)~is dual to~\(T^q_p(E)\), and \(T(E)\)~is dual to itself and to~\(T(E^*)\).
\end{rmk}

\begin{rmk}
The results in this subsection show that it is possible to develop tensor algebra using multilinear function spaces instead of tensor product spaces (at least in the finite-dimensional case). While it is easier to get started this way, it is messier in the long run.
\end{rmk}

\newpage
\section*{Chapter~4}
\subsection*{\S~2}
\begin{rmk}
The factor~\(1/p!\) in the definition of the alternator~\(\pi_A\) on~\(\medtprod^p E\) ensures that \(\pi_A\)~is a projection operator (\(\pi_A^2=\pi_A\)). This factor creeps into the scalar product between dual spaces of skew-symmetric tensors in~(4.8).
\end{rmk}

\begin{rmk}
By (4.4) and~(4.5), we have the following commutative diagram:
\begin{diagram}[nohug]
\medtprod^p E		&\rTo^{\pi_A}	&X^p(E)\\
\dTo<{\pi}			&\ruTo>{\iso}	&\\
\medtprod^p E/N^p(E)&				&
\end{diagram}
Therefore \(\medtprod^p E/N^p(E)\)~can be viewed as a ``top down'' construction of the space of skew-symmetric tensors of degree~\(p\), where we start with tensors of degree~\(p\) and kill off tensors with equal factors. This is the tensor equivalent of the result that a \(p\)-linear function is skew-symmetric if and only if it is alternating. In fact, the proof of (4.4) (including the proof of lemma~(4.1)) is just like the proof of the latter result.\footnote{See Proposition~I in chapter~IV, \S~1 of \cite{greub1} and chapter~5, \S~1 of~\cite{greub2}.}
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
The duality of \(\pi_A:\medtprod^p E\to\medtprod^p E\) and \(\pi^A:\medtprod^p E^*\from\medtprod^p E^*\) induces a duality between the restriction \(\pi_A:\medtprod^p E\to X^p(E)\) and the quotient
\[\proj{\pi^A}:\medtprod^p E^*\from\medtprod^p E^*/N^p(E^*)\]
since
\[N^p(E^*)=\ker\pi^A=\oc{(\im\pi_A)}=\oc{X^p(E)}\]
On the other hand, \(\medtprod^p E^*/N^p(E^*)\iso X^p(E^*)\), so \(\pi_A:\medtprod^p E\to X^p(E)\) is dual to the restriction \(\pi^A:\medtprod^p(E^*)\from X^p(E^*)\).
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
We see from~(4.10) that the skew-symmetric part of a product depends only on the skew-symmetric parts of the factors.
\end{rmk}

\begin{exer}[1]
The map \(\sigma:\medtprod^p E\to\medtprod^p E\) is tensorial. If \(E\)~is finite-dimensional, then \(\sigma\)~is generated by \(\mu(t)\) and~\(C^i_j\), where \(t\)~is the unit tensor.
\end{exer}
\begin{proof}
If \(\alpha\)~is an automorphism of~\(E\) and \(u=\tprods{x}{1}{p}\in\medtprod^p E\), then
\[\sigma T_{\alpha}u=\tprods{\alpha x}{\sigma^{-1}(1)}{\sigma^{-1}(p)}=T_{\alpha}\sigma u\]
so \(\sigma T_{\alpha}=T_{\alpha}\sigma\) and hence \(\sigma\)~is tensorial.

For the second claim, we may assume that \(\sigma=(12\cdots j)\) where \(1<j\). Write \(t=\sum_{\nu}e^{*\nu}\tprod e_{\nu}\) where \(e_{\nu},e^{*\nu}\) is any pair of dual bases in \(E,E^*\). Then
\begin{align*}
C^1_{j+1}(t\tprod u)&=\sum_{\nu}C^1_{j+1}(e^{*\nu}\tprod e_{\nu}\tprod\tprods{x}{1}{p})\\
	&=\sum_{\nu}\sprod{e^{*\nu}}{x_j}e_{\nu}\tprod\tprods{x}{1}{j-1}\tprod\tprods{x}{j+1}{p}\\
	&=x_j\tprod\tprods{x}{1}{j-1}\tprod\tprods{x}{j+1}{p}\\
	&=\sigma u
\end{align*}
so \(C^1_{j+1}\after\mu(t)=\sigma\).
\end{proof}

\subsection*{\S~5}
\begin{rmk}
In the following subsections, an algebra of skew-symmetric tensors is constructed in two ways:
\begin{itemize}
\item Top down, starting with the tensor algebra~\(\medtprod E\) and killing off tensors with equal factors in~\(N(E)\) to form \(\medtprod E/N(E)\).
\item Bottom up, by collecting skew-symmetric tensors in~\(\medtprod E\) to form~\(X(E)\).
\end{itemize}
The two constructions are essentially equivalent by~(4.17), although the scalar product between dual algebras is defined slightly differently in~\S~8---specifically, the factor~\(1/p!\) in~(4.8) is killed off in~(4.18) to obtain~(4.20).
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If we make the appropriate identifications, we have
\[\pi(\medtprod^p E)=\medtprod^p E/(N(E)\sect\medtprod^p E)=\medtprod^p E/N^p(E)=X^p(E)\]
so we already know
\[\medtprod E/N(E)=\sum_p \pi(\medtprod^p E)=\sum_p X^p(E)=X(E)\]
\end{rmk}

\begin{rmk}
To see how (4.16)~follows from~(4.13), write \(u=\pi(a)\) and \(v=\pi(b)\) with \(a\in\medtprod^p E\) and \(b\in\medtprod^q E\). By (4.15), (4.17), and~(4.13),
\[uv=\pi a\mult\pi b=\pi(a\tprod b)=\rho\pi_A(a\tprod b)=(-1)^{pq}\rho\pi_A(b\tprod a)=(-1)^{pq}vu\]
\end{rmk}

\subsection*{\S~7}
\begin{rmk}
\(X(E)\)~is not a subalgebra of~\(\medtprod E\), but can be made into an algebra (see problem~1 of \S~8 below).
\end{rmk}

\begin{rmk}
The inverse of~\(\rho\) is the induced isomorphism \(\overline{\pi_X}:\medtprod E/N(E)\iso X(E)\):
\begin{diagram}[nohug]
\medtprod E			&\rTo^{\pi_X}						&X(E)\\
\dTo<{\pi}			&\ruTo>{\rho^{-1}=\overline{\pi_X}}	&\\
\medtprod E/N(E)	&									&
\end{diagram}
This diagram generalizes the diagram in \S~2 above.
\end{rmk}

\subsection*{\S~8}
\begin{exer}[1]
If we define a product in~\(X(E)\) by
\[u\mult v=\pi_A(u\tprod v)\qquad(u,v\in X(E))\]
then \(X(E)\)~becomes a graded algebra and \(\rho\)~an algebra isomorphism, where the elements of~\(X^p(E)\) are homogeneous of degree~\(p\). Moreover,
\[\pi_A u\mult\pi_A v=\pi_A(u\tprod v)\qquad(u,v\in\medtprod E)\]
\end{exer}
\begin{proof}
Clearly the product makes~\(X(E)\) into a graded algebra. By (4.17) and~(4.15),
\[\rho(u\mult v)=\rho\pi_A(u\tprod v)=\pi(u\tprod v)=\pi u\mult\pi v=\rho u\mult\rho v\]
for all \(u,v\in X(E)\), so \(\rho\)~is an algebra isomorphism. By~(4.10),
\[\pi_A u\mult\pi_A v=\pi_A(\pi_A u\tprod\pi_A v)=\pi_A(u\tprod v)\]
for all \(u,v\in\medtprod E\).
\end{proof}
\begin{rmk}
The proof also shows that the product is uniquely determined by the requirement that \(\rho\)~be an algebra isomorphism.
\end{rmk}

\newpage
\section*{Chapter~5}
\subsection*{\S~1}
\begin{rmk}
For \(\sigma\in S_p\),
\[\sigma(\fprods{f}{1}{p})=\fprods{f}{\sigma^{-1}(1)}{\sigma^{-1}(p)}\qquad(f_{\nu}\in T^1(E))\]
since
\begin{align*}
(\sigma(\fprods{f}{1}{p}))(x_1,\ldots,x_p)&=(\fprods{f}{1}{p})(x_{\sigma(1)},\ldots,x_{\sigma(p)})\\
	&=f_1(x_{\sigma(1)})\mult\cdots\mult f_p(x_{\sigma(p)})\\
	&=f_{\sigma^{-1}(1)}(x_1)\mult\cdots\mult f_{\sigma^{-1}(p)}(x_p)\\
	&=(\fprods{f}{\sigma^{-1}(1)}{\sigma^{-1}(p)})(x_1,\ldots,x_p)
\end{align*}
If \(E\)~is finite-dimensional, it follows that these diagrams commute:
\begin{diagram}
\medtprod^p E^*	&\rTo^{\sigma}	&\medtprod^p E^*&&\medtprod^p E^*	&\rTo^{\pi^A}	&\medtprod^p E^*\\
\dTo<{\iso}		&				&\dTo>{\iso}	&&\dTo<{\iso}		&				&\dTo>{\iso}\\
T^p(E)			&\rTo_{\sigma}	&T^p(E)			&&T^p(E)			&\rTo_A			&T^p(E)
\end{diagram}
As with~\(\pi_A\), the factor~\(1/p!\) in the definition of~\(A\) on~\(T^p(E)\) ensures that \(A\)~is a projection operator (\(A^2=A\)).
\end{rmk}

\begin{exer}[1]
Let \(L_p(E;F)\) denote the space of \(p\)-linear mappings \(E^p\to F\), and \(A_p(E;F)\) the subspace of skew-symmetric mappings. If \(T:L_p(E;F)\to L_p(E;F)\) is linear such that \(T\varphi=\varphi\) for all \(\varphi\in A_p(E;F)\), and \(T(\sigma\varphi)=\sign{\sigma}T\varphi\) for all \(\sigma\in S_p\) and \(\varphi\in L_p(E;F)\), then \(T=A\) (the alternator).
\end{exer}
\begin{proof}
By assumption, \(TA=A\) and
\[(TA)\varphi=T(A\varphi)=\frac{1}{p!}\sum_{\sigma}\sign{\sigma}T(\sigma\varphi)=\frac{1}{p!}\sum_{\sigma}T\varphi=T\varphi\]
so \(TA=T\).
\end{proof}
\begin{rmk}
This result shows that the alternator is the unique ``skew-symmetric'' transformation of \(p\)-linear mappings which fixes skew-symmetric mappings.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
The \(p\)-th exterior power \(\medeprod^p E\) of~\(E\) is a universal (initial) object in the category of ``vector spaces with skew-symmetric \(p\)-linear maps of~\(E\) into them''. In this category, the objects are skew-symmetric \(p\)-linear maps \(E^p\to F\), and the arrows are linear maps \(F\to G\) which respect the \(p\)-linear maps:
\begin{diagram}
E^p		&\rTo	&G\\
\dTo	&\ruTo	&\\
F		&		&
\end{diagram}
Every object \(E^p\to F\) in this category can be obtained from the exterior product \(\medeprod^p:E^p\to\medeprod^p E\) in a unique way. This is why \(\medeprod^p\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(\medeprod^p E\) satisfy only those relations required to make \(\medeprod^p\) skew-symmetric and \(p\)-linear. By category theoretic abstract nonsense, \(\medeprod^p E\)~is unique up to isomorphism.
\end{rmk}

\begin{rmk}
The \(p\)-th exterior power \(\medeprod^p E\) can be constructed in two ways:
\begin{itemize}[itemsep=0pt]
\item Top down, as \(\medeprod^p E=\medtprod^p E/N^p(E)\).
\item Bottom up, as \(\medeprod^p E=X^p(E)\).
\end{itemize}
\end{rmk}

\begin{rmk}
Geometrically, the \(p\)-vector
\[\medeprod^p(x_1,\ldots,x_p)=\eprods{x}{1}{p}\]
represents the oriented (signed) volume of the \(p\)-dimensional parallelepiped determined by \(x_1,\ldots, x_p\). It can also be thought of as a family of \(p\)-dimensional parallelepipeds with the same orientation and volume.
\end{rmk}

\begin{rmk}
If \(E\)~is \(n\)-dimensional and \(\Delta\ne 0\) is a determinant function in~\(E\), then since \(\medeprod^n\)~is skew-symmetric and \(n\)-linear in~\(E\),
\[\eprods{x}{1}{n}=\Delta(x_1,\ldots,x_n)\mult\eprods{e}{1}{n}\]
where \(\Delta(e_1,\ldots,e_n)=1\). Here \(\eprods{e}{1}{n}\) represents the unit volume under~\(\Delta\). On the other hand, since \(\Delta\)~is skew-symmetric and \(n\)-linear, there is a linear function \(\Delta_{\eprod}:\medeprod^n E\to\Gamma\) with
\[\Delta_{\eprod}(\eprods{x}{1}{n})=\Delta(x_1,\ldots,x_n)\]
Note \(\Delta_{\eprod}\)~is inverse to \(\eprods{e}{1}{n}\mult\iota\), so it is an isomorphism. Also
\[\Delta_{\eprod}(\eprods{x}{1}{n})=\Delta_{\tprod}(\tprods{x}{1}{n})\]
These results show that the universality of determinant functions is a special case of the universality of the exterior product.
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The projection \(\pi:\medtprod E\to\medeprod E\) constructed in this subsection and the canonical projection \(\rho:\medtprod E\to\medtprod E/N(E)\) are connected by this commutative diagram:
\begin{diagram}[nohug]
\medtprod E		&\rTo^{\pi}	&\medeprod E\\
\dTo<{\rho}		&\ruTo>{f}	&\\
\medtprod E/N(E)&			&
\end{diagram}
For \(u,v\in\medeprod E\), if we write \(f^{-1}u=\rho\tilde{u}\) and \(f^{-1}v=\rho\tilde{v}\) for \(\tilde{u},\tilde{v}\in\medtprod E\), then \(u=\pi\tilde{u}\) and \(v=\pi\tilde{v}\) and
\[u\eprod v=f(\rho\tilde{u}\mult\rho\tilde{v})=f(\rho(\tilde{u}\tprod\tilde{v}))=\pi(\tilde{u}\tprod\tilde{v})\]
This multiplication makes~\(\medeprod E\) into a \emph{graded algebra} where the elements of~\(\medeprod^p E\) are homogeneous of degree~\(p\). It also makes~\(\pi\) into an algebra homomorphism.
\end{rmk}

\begin{rmk}
The factor~\(1/k!\) in the definition of~\(u^k\) kills off coefficients arising in
\[\underbrace{\eprods{u}{}{}}_k\]
For example if \(u=e_1\eprod e_2+e_3\eprod e_4\) where \(e_i\in E\), then
\[u\eprod u=2e_1\eprod e_2\eprod e_3\eprod e_4\]
so
\[u^2=\frac{1}{2}u\eprod u=e_1\eprod e_2\eprod e_3\eprod e_4\]
\end{rmk}

\begin{rmk}
If \(u\in\medeprod E\), then
\[u^{p+q}=\frac{1}{(p+q)!}\underbrace{\eprods{u}{}{}}_p\eprod\underbrace{\eprods{u}{}{}}_q=\frac{p!\,q!}{(p+q)!}u^p\eprod u^q\qquad(p,q\ge 0)\]
In particular,
\[u\eprod u^k=u^k\eprod u=(k+1)u^{k+1}\qquad(k\ge 0)\]
\end{rmk}

\begin{rmk}
If \(u\in\medeprod^p E\) and \(v\in\medeprod^q E\) with \(pq\)~even, then
\begin{align*}
(u+v)^k&=\frac{1}{k!}\underbrace{\eprods{(u+v)}{}{}}_k\\
	&=\frac{1}{k!}\sum_{i+j=k}\frac{k!}{i!j!}\underbrace{\eprods{u}{}{}}_i\eprod\underbrace{\eprods{v}{}{}}_j\\
	&=\sum_{i+j=k}\frac{1}{i!j!}(i!\,u^i)\eprod(j!\,v^j)\\
	&=\sum_{i+j=k}u^i\eprod v^j
\end{align*}
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
The exterior algebra \(\medeprod E\) is a universal (initial) object in the category of ``associative unital algebras with linear maps of~\(E\) into them whose squares are zero''. In this category, the objects are linear maps \(\varphi:E\to A\) for associative unital algebras~\(A\) with \(\varphi^2=0\), and the arrows are unital algebra homomorphisms \(A\to B\) which preserve the units and respect the linear maps:
\begin{diagram}
E	&\rTo	&B\\
\dTo&\ruTo	&\\
A	&		&
\end{diagram}
Every object in this category can be obtained from the exterior algebra \(\medeprod E\) in a unique way. This is why \(\medeprod E\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(\medeprod E\) satisfy only those properties that are required to make \(\medeprod E\) into an associative unital algebra containing~\(E\) with skew-symmetric multiplication in~\(E\). By category theoretic abstract nonsense, \(\medeprod E\)~is unique up to isomorphism.
\end{rmk}

\begin{rmk}
The exterior algebra~\(\medeprod E\) can be constructed in two ways:
\begin{itemize}[itemsep=0pt]
\item Top down, as \(\medeprod E=\medtprod E/N(E)\).
\item Bottom up, as \(\medeprod E=\sum_p\medeprod^p E\), where \(\medeprod^p E\)~is constructed either top down or bottom up as remarked above.
\end{itemize}
\end{rmk}

\begin{rmk}
It is not necessary to establish~(5.9), since the skew-symmetry of~\(\alpha\) follows from the result in subsection~5.1.
\end{rmk}

\subsection*{\S~7}
\begin{rmk}
If \(\nu_1<\cdots<\nu_p\) and \(\mu_1<\cdots<\mu_p\), then
\[\det(\delta^{\nu_i}_{\mu_j})=\begin{cases}
1&\text{if }(\nu_1,\ldots,\nu_p)=(\mu_1,\ldots,\mu_p)\\
0&\text{otherwise}
\end{cases}\]
\end{rmk}
\begin{proof}
If \((\nu_1,\ldots,\nu_p)=(\mu_1,\ldots,\mu_p)\), then \(\delta^{\nu_i}_{\mu_i}=1\) for all~\(i\) and \(\delta^{\nu_i}_{\mu_j}=0\) for all \(i\ne j\) since \(\nu_i\ne\nu_j=\mu_j\), so \(\det(\delta^{\nu_i}_{\mu_j})=1\). Conversely if \(k\)~is least with \(\nu_k\ne\mu_k\), say \(\nu_k<\mu_k\), then \(\delta^{\nu_k}_{\mu_j}=0\) for all~\(j\), so \(\det(\delta^{\nu_i}_{\mu_j})=0\).
\end{proof}

\begin{rmk}
If \(E\)~is \(n\)-dimensional, then \(\medeprod^p E=0\) for \(p>n\). Geometrically, this just reflects the fact that you can't fit a \(p\)-dimensional parallelepiped into the \(n\)-dimensional space.
\end{rmk}

\begin{rmk}
Let \(E,E^*\) be a pair of dual \(n\)-dimensional vector spaces with dual bases \(e_1,\ldots,e_n\) and \(e^{*1},\ldots,e^{*n}\). For \(p\le n\), write
\[x_i=\sum_{\nu=1}^n\xi_i^{\nu}e_{\nu}\qquad(i=1,\ldots,p)\]
and
\[\eprods{x}{1}{p}=\sum_{<}\xi^{\nu_1\cdots\nu_p}\eprods{e}{\nu_1}{\nu_p}\]
By~(5.13),
\[\xi^{\nu_1\cdots\nu_p}=\sprod{e^{*\nu_1}\eprod\cdots\eprod e^{*\nu_p}}{\eprods{x}{1}{p}}=\det\sprod{e^{*\nu_j}}{x_i}=\det(\xi_i^{\nu_j})\]
This shows that the components of a decomposable \(p\)-vector are  just the \(p\times p\) minor determinants of the matrix of components of the \(p\) factors.
\end{rmk}

\begin{rmk}
Let \(E,E^*\) be a pair of dual \(n\)-dimensional vector spaces and \(e^*\)~a basis vector of~\(\medeprod^n E^*\). Let \(\Delta\)~be the determinant function in~\(E\) corresponding to~\(e^*\),\footnote{See subsection~5.33.} so that
\[\Delta(x_1,\ldots,x_n)=\sprod{e^*}{\eprods{x}{1}{n}}\]
By a Laplace expansion~(5.11), for any \(x^*\in E^*\) and \(x\in E\) we have
\begin{align*}
0&=\sprod{x^*\eprod e^*}{x\eprod\eprods{x}{1}{n}}\\
	&=\sprod{x^*}{x}\sprod{e^*}{\eprods{x}{1}{n}}-\sum_{i=1}^n(-1)^{i-1}\sprod{x^*}{x_i}\sprod{e^*}{x\eprod x_1\eprod\cdots\eprod\delete{x_i}\eprod\cdots\eprod x_n}
\end{align*}
so
\[\sprod{x^*}{\Delta(x_1,\ldots,x_n)x}=\sprod{x^*}{\sum_{i=1}^n(-1)^{i-1}\Delta(x,x_1,\ldots,\delete{x_i},\ldots,x_n)x_i}\]
By nondegeneracy of the scalar product, it follows that\footnote{This is~(4.6) in~\cite{greub1}. See also the remark in chapter~IV, \S~1 of~\cite{greub1} above.}
\[\Delta(x_1,\ldots,x_n)x=\sum_{i=1}^n(-1)^{i-1}\Delta(x,x_1,\ldots,\delete{x_i},\ldots,x_n)x_i\]
\end{rmk}

\begin{rmk}
If \(E\)~is a Euclidean space and \(x_1,\ldots,x_p\in E\), then
\[\norm{\eprods{x}{1}{p}}=\sqrt{\det\iprod{x_i}{x_j}}\]
is the volume of the \(p\)-dimensional parallelepiped determined by \(x_1,\ldots,x_p\).
\end{rmk}

\begin{exer}[1]
Let \(E,E^*\) be a pair of finite-dimensional dual vector spaces and \(A\subseteq L(E^*;E)\) be the subspace of linear maps \(\varphi:E^*\to E\) for which \(\varphi^*=-\varphi\). If \(\varphi:E\times E\to A\) is the bilinear map defined by
\[\varphi_{a,b}(x^*)=\sprod{x^*}{a}b-\sprod{x^*}{b}a\]
then \((A,\varphi)\) is a second exterior power of~\(E\).
\end{exer}
\begin{proof}
Identifying \(E\tprod E\) with \(L(E^*;E)\) under the natural isomorphism
\[a\tprod b\mapsto\bigl(x^*\mapsto\sprod{x^*}{a}b\,\bigr)\]
\(A\)~is just the subspace of skew-symmetric tensors in~\(E\tprod E\), and
\[\varphi_{a,b}=a\tprod b-b\tprod a\]
is (up to a factor of~\(2\)) just the skew-symmetric part of \(a\tprod b\).
\end{proof}

\begin{exer}[4 - Lagrange]
\[
\renewcommand{\arraystretch}{1.5}
\begin{vmatrix}
\sum_{\nu=1}^n\xi_1^{\nu}\eta^1_{\nu}&\cdots&\sum_{\nu=1}^n\xi_1^{\nu}\eta^p_{\nu}\\
\vdots&\ddots&\vdots\\
\sum_{\nu=1}^n\xi_p^{\nu}\eta^1_{\nu}&\cdots&\sum_{\nu=1}^n\xi_p^{\nu}\eta^p_{\nu}
\end{vmatrix}
=
\sum_{<}
\begin{vmatrix}
\xi_1^{\nu_1}&\cdots&\xi_1^{\nu_p}\\
\vdots&\ddots&\vdots\\
\xi_p^{\nu_1}&\cdots&\xi_p^{\nu_p}
\end{vmatrix}
\begin{vmatrix}
\eta^1_{\nu_1}&\cdots&\eta^1_{\nu_p}\\
\vdots&\ddots&\vdots\\
\eta^p_{\nu_1}&\cdots&\eta^p_{\nu_p}
\end{vmatrix}
\tag{1}\]
\end{exer}
\begin{proof}
Let \(L\)~denote the left-hand side of~(1). Let \(E,E^*\) be a pair of dual vector spaces with dual bases \(e_1,\ldots,e_n\) and \(e^{*1},\ldots,e^{*n}\). Define
\[x_i=\sum_{\nu=1}^n\xi_i^{\nu}e_{\nu}\qquad\text{and}\qquad y^{*j}=\sum_{\mu=1}^n\eta^j_{\mu}e^{*\mu}\qquad(i,j=1,\ldots,p)\]
Since
\[\sprod{y^{*j}}{x_i}=\sum_{\nu=1}^n\xi_i^{\nu}\eta^j_{\nu}\]
it follows from (5.10) and~(5.13) that
\begin{align*}
L&=\sprod{y^{*1}\eprod\cdots\eprod y^{*p}}{x_1\eprod\cdots\eprod x_p}\\
	&=\sum_{(\nu),(\mu)}\xi_1^{\nu_1}\cdots\,\xi_p^{\nu_p}\eta^1_{\mu_1}\cdots\,\eta^p_{\mu_p}\sprod{e^{*\mu_1}\eprod\cdots\eprod e^{*\mu_p}}{e_{\nu_1}\eprod\cdots\eprod e_{\nu_p}}\\
	&=\sum_{(\nu),(\mu)}\xi_1^{\nu_1}\cdots\,\xi_p^{\nu_p}\eta^1_{\mu_1}\cdots\,\eta^p_{\mu_p}\det(\delta_{\nu_i}^{\mu_j})
\end{align*}
Now
\[\det(\delta_{\nu_i}^{\mu_j})=\sum_{\sigma}\sign{\sigma}\delta_{\nu_{\sigma(1)}}^{\mu_1}\cdots\,\delta_{\nu_{\sigma(p)}}^{\mu_p}\]
so \(\det(\delta_{\nu_i}^{\mu_j})=\sign{\tau}\) if the~\(\nu_i\) are distinct and \(\mu_i=\nu_{\tau(i)}\) for some permutation~\(\tau\), otherwise \(\det(\delta_{\nu_i}^{\mu_j})=0\). The~\(\nu_i\) are distinct if and only if \(\nu_{\sigma(1)}<\cdots<\nu_{\sigma(p)}\) for some permutation~\(\sigma\), so we have
\begin{align*}
L&=\sum_{\nu_1<\cdots<\nu_p}\sum_{\sigma}\xi_1^{\nu_{\sigma(1)}}\cdots\,\xi_p^{\nu_{\sigma(p)}}\sum_{\tau}\sign{\tau}\eta^1_{\nu_{\tau\sigma(1)}}\cdots\,\eta^p_{\nu_{\tau\sigma(p)}}\\
	&=\sum_{\nu_1<\cdots<\nu_p}\sum_{\sigma}\sign{\sigma}\xi_1^{\nu_{\sigma(1)}}\cdots\,\xi_p^{\nu_{\sigma(p)}}\sum_{\tau}\sign{\tau\sigma}\eta^1_{\nu_{\tau\sigma(1)}}\cdots\,\eta^p_{\nu_{\tau\sigma(p)}}
\end{align*}
Since \(\tau\sigma\)~runs over all permutations as~\(\tau\) does, we have
\[L=\sum_{<}\Bigl(\sum_{\sigma}\sign{\sigma}\xi_1^{\nu_{\sigma(1)}}\cdots\,\xi_p^{\nu_{\sigma(p)}}\Bigr)\Bigl(\sum_{\varrho}\sign{\varrho}\eta^1_{\nu_{\varrho(1)}}\cdots\,\eta^p_{\nu_{\varrho(p)}}\Bigr)\]
which is just~(1).
\end{proof}

\begin{exer}[6]
Let \((\medeprod E,\eprod)\) be an exterior algebra over~\(E\). If we define
\[u\eprodf v=\frac{(p+q)!}{p!q!}u\eprod v\qquad(u\in\medeprod^p E,v\in\medeprod^q E)\]
and extend the definition to~\(\medeprod E\) through bilinearity, then the resulting algebra \((\medeprodf E,\eprodf)\) is also an exterior algebra over~\(E\).
\end{exer}
\begin{proof}
Define \(f:\medeprod E\to\medeprodf E\) by \(f(u)=p!u\) for \(u\in\medeprod^p E\). Then clearly \(f\)~is a linear isomorphism homogeneous of degree zero, and
\[f(u\eprod v)=(p+q)!\,u\eprod v=\frac{(p+q)!}{p!q!}(p!u)\eprod(q!v)=f(u)\eprodf f(v)\]
for \(u\in\medeprod^p E\) and \(v\in\medeprod^q E\), so \(f\)~is a graded algebra isomorphism. It follows that \((\medeprodf E,\eprodf)\) satisfies the universal property of an exterior algebra over~\(E\).
\end{proof}
\begin{rmk}
If \(E,E^*\) are dual spaces, then \(\medeprodf E,\medeprodf E^*\) are dual graded algebras under the scalar product induced by~\(f\), which is just the scalar product in~(5.10) with \(\eprod\) replaced by~\(\eprodf\).
\end{rmk}

\subsection*{\S~8}
\begin{rmk}
We have a functor from the category of vector spaces into the category of associative unital algebras, which sends vector spaces \(E\) and~\(F\) to the exterior algebras \(\medeprod E\) and~\(\medeprod F\), and which sends a linear map \(\varphi:E\to F\) to the unital algebra homomorphism \(\varphi_{\eprod}:\medeprod E\to\medeprod F\).
\end{rmk}

\begin{rmk}
For a linear map \(\varphi:E\to F\), the following diagram commutes:
\begin{diagram}
\medtprod E	&\rTo^{\varphi_{\tprod}}&\medtprod F\\
\dTo<{\pi_E}&						&\dTo>{\pi_F}\\
\medeprod E	&\rTo_{\varphi_{\eprod}}&\medeprod F
\end{diagram}
This shows that the projection \(\pi_E:\medtprod E\to\medeprod E\) is a natural transformation from the tensor algebra functor to the exterior algebra functor.
\end{rmk}

\begin{rmk}
Let \(E\)~be an \(n\)-dimensional space and \(\Delta\)~a determinant function in~\(E\). If \(\varphi:E\to E\) is linear, then
\begin{align*}
\Delta_{\eprod}\after\varphi_{\eprod}(\eprods{x}{1}{n})&=\Delta_{\eprod}(\eprods{\varphi x}{1}{n})\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_n)\\
	&=\det\varphi\mult\Delta(x_1,\ldots,x_n)\\
	&=\det\varphi\mult\Delta_{\eprod}(\eprods{x}{1}{n})\\
	&=\Delta_{\eprod}\after(\det\varphi\mult\iota)(\eprods{x}{1}{n})
\end{align*}
for all \(x_1,\ldots,x_n\in E\). Since \(\Delta_{\eprod}\)~is injective, it follows that
\[\medeprod^n\varphi=\det\varphi\mult\iota\]
where \(\medeprod^n\varphi\)~is the restriction of~\(\varphi_{\eprod}\) to~\(\medeprod^n E\). Equivalently
\[\tr(\medeprod^n\varphi)=\det\varphi\]
This provides another natural (coordinate-free) definition of the determinant of a linear map.
\end{rmk}

\begin{rmk}
Let \(E,E^*\) be a pair of dual \(n\)-dimensional spaces with dual bases \(e_i,e^{*i}\) and \(\varphi:E\to E\) linear. For \(0\le p\le n\), if the induced basis of~\(\medeprod^p E\) is ordered lexically, then for
\[1\le\nu_1<\cdots<\nu_p\le n\qquad\text{and}\qquad 1\le\mu_1<\cdots<\mu_p\le n\]
the \((\nu,\mu)\) entry of the \(\binom{n}{p}\times\binom{n}{p}\) matrix of~\(\medeprod^p\varphi\) with respect to the basis is
\[\sprod{e^{*\mu_1}\eprod\cdots\eprod e^{*\mu_p}}{\eprods{\varphi e}{\nu_1}{\nu_p}}=\det\sprod{e^{*\mu_j}}{\varphi e_{\nu_i}}\]
Since \(\sprod{e^{*\mu_j}}{\varphi e_{\nu_i}}\) is the \((\nu_i,\mu_j)\) entry of the matrix of~\(\varphi\) with respect to the~\(e_i\), this shows that the matrix of~\(\medeprod^p\varphi\) consists of the \(p\times p\) minor determinants of the matrix of~\(\varphi\). In particular the \(1\times 1\) matrix of~\(\medeprod^n\varphi\) consists of the single element~\(\det\varphi\), consistent with the previous remark.

It follows from the determinantal characterization of the rank of a linear map\footnote{See subsection~4.16 of~\cite{greub1}.} that the rank of~\(\varphi\) is the largest~\(r\) for which \(\medeprod^r\varphi\ne 0\). Intuitively, the rank is the highest-dimensional volume that \(\varphi\) ``preserves'' in the sense of mapping to another volume of the same dimension and not lower dimension.
\end{rmk}

\subsection*{\S~10}
\begin{rmk}
For a linear map \(\varphi:E\to E\), the following diagram commutes:
\begin{diagram}
\medtprod E	&\rTo^{\theta_{\tprod}(\varphi)}&\medtprod E\\
\dTo<{\pi_E}&								&\dTo>{\pi_E}\\
\medeprod E	&\rTo_{\theta_{\eprod}(\varphi)}&\medeprod E
\end{diagram}
\end{rmk}

\begin{rmk}
Let \(E\)~be an \(n\)-dimensional space and \(\Delta\) a determinant function in~\(E\). If \(\varphi:E\to E\) is linear, then
\begin{align*}
\Delta_{\eprod}\after\theta_{\eprod}(\varphi)(\eprods{x}{1}{n})&=\sum_{i=1}^n\Delta_{\eprod}(x_1\eprod\cdots\eprod\varphi x_i\eprod\cdots\eprod x_n)\\
	&=\sum_{i=1}^n\Delta(x_1,\ldots,\varphi x_i,\ldots,x_n)\\
	&=\tr\varphi\mult\Delta(x_1,\ldots,x_n)\\
	&=\tr\varphi\mult\Delta_{\eprod}(\eprods{x}{1}{n})\\
	&=\Delta_{\eprod}\after(\tr\varphi\mult\iota)(\eprods{x}{1}{n})
\end{align*}
for all \(x_1,\ldots,x_n\in E\). Since \(\Delta_{\eprod}\)~is injective, it follows that
\[\theta_{\eprod}^{(n)}(\varphi)=\tr\varphi\mult\iota\]
where \(\theta_{\eprod}^{(n)}(\varphi)\)~is the restriction of~\(\theta_{\eprod}(\varphi)\) to~\(\medeprod^n E\). Equivalently
\[\tr\theta_{\eprod}^{(n)}(\varphi)=\tr\varphi\]
This is also equivalent to the result in subsection~3.6 above, by the following commutative diagram:
\begin{diagram}[nohug]
				&										&E^n				&						&\\
				&										&\dTo<{\tprod^n}	&\rdTo>{\Delta}			&\\
\medtprod^n E	&\rTo^{\theta_{\circled{n}}(\varphi)}	&\medtprod^n E		&\rTo>{\Delta_{\tprod}}	&\Gamma\\
\dTo<{\pi}		&										&\dTo<{\pi}			&\ruTo>{\Delta_{\eprod}}&\\
\medeprod^n E	&\rTo_{\theta_{\eprod}^{(n)}(\varphi)}	&\medeprod^n E		&				&
\end{diagram}
This provides another natural (coordinate-free) definition of the trace.
\end{rmk}

\subsection*{\S~11}
\begin{rmk}
If \(\omega=\iota_{\eprod}\) and \(\varphi:E\to E\) is linear, then \(\Omega(\varphi)=\theta_{\eprod}(\varphi)\). This shows that the construction in \S~10 is just a special case of the construction in this subsection.
\end{rmk}

\subsection*{\S~12}
\begin{rmk}
From~(5.36), we see that the following diagram commutes:
\begin{diagram}
\medeprod E				&\rTo^{\Omega_E(\varphi)}	&\medeprod E\\
\dTo<{\alpha_{\eprod}}	&							&\dTo>{\alpha_{\eprod}}\\
\medeprod F				&\rTo_{\Omega_F(\psi)}		&\medeprod F
\end{diagram}
\end{rmk}

\begin{exer}[2]
Let \(a\in\medeprod^k E\) where \(k\)~is odd. Define \(\theta:\medeprod E\to\medeprod E\) by
\[\theta u=\begin{cases}
a\eprod u&\text{if \(u\in\medeprod^p E\), \(p\)~odd}\\
0&\text{if \(u\in\medeprod^p E\), \(p\)~even}
\end{cases}\]
Then \(\theta\)~is a derivation homogeneous of degree~\(k\) in~\(\medeprod E\).
\end{exer}
\begin{proof}
It is clear that \(\theta\)~is linear and homogeneous of degree~\(k\). We must prove
\[\theta(u\eprod v)=\theta u\eprod v+u\eprod\theta v\qquad(u,v\in\medeprod E)\tag{1}\]
By multilinearity, we may assume \(u\in\medeprod^p E\) and \(v\in\medeprod^q E\), so \(u\eprod v\in\medeprod^{p+q}E\). We proceed by cases on the parities of \(p\) and~\(q\).

If \(p\) and~\(q\) are even, then \(p+q\)~is even and both sides of~(1) are zero.

If \(p\)~is even and \(q\)~is odd, then \(p+q\)~is odd and
\begin{align*}
\theta u\eprod v+u\eprod\theta v&=0\eprod v+u\eprod a\eprod v\\
	&=a\eprod u\eprod v\\
	&=\theta(u\eprod v)
\end{align*}

If \(p\)~is odd and \(q\)~is even, then the proof is similar to the previous case.

If \(p\) and~\(q\) are odd, then \(p+q\)~is even and \(pk\)~is odd, so
\begin{align*}
\theta u\eprod v+u\eprod\theta v&=a\eprod u\eprod v+u\eprod a\eprod v\\
	&=a\eprod u\eprod v-a\eprod u\eprod v\\
	&=0\\
	&=\theta(u\eprod v)\qedhere
\end{align*}
\end{proof}
\begin{rmk}
It follows from Proposition~5.27.2 that every derivation of odd degree in~\(\medeprod E\) has this form.
\end{rmk}

\begin{exer}[6]
Let \(\Omega\)~be an antiderivation of degree~\(-1\) in~\(\medeprod E\) (with respect to the canonical involution). If \(\widetilde{\Omega}\) is defined by \(\widetilde{\Omega}u=p^{-1}\mult\Omega u\) for \(u\in\medeprod^p E\) , where \(p^{-1}=0\) for \(p=0\), then \(\widetilde{\Omega}\)~is an antiderivation in~\(\medeprodf E\).\footnote{See problem~5.7.6 for the definition of~\(\medeprodf E\).}
\end{exer}
\begin{proof}
Clearly \(\widetilde{\Omega}\)~is linear. We must prove
\[\widetilde{\Omega}(u\eprodf v)=\widetilde{\Omega}u\eprodf v+(-1)^pu\eprodf\widetilde{\Omega}v\qquad(u\in\medeprod^p E,v\in\medeprod^q E)\]
We have
\begin{align*}
\widetilde{\Omega}(u\eprodf v)&=\frac{1}{p+q}\Omega(u\eprodf v)\\
	&=\frac{(p+q-1)!}{p!q!}\Omega(u\eprod v)\\
	&=\frac{(p+q-1)!}{p!q!}\bigl(\Omega u\eprod v+(-1)^pu\eprod\Omega v\bigr)
\end{align*}
On the other hand
\[\widetilde{\Omega}u\eprodf v=\frac{1}{p}\Omega u\eprodf v=\frac{(p+q-1)!}{p!q!}\Omega u\eprod v\]
and
\[u\eprodf\widetilde{\Omega}v=\frac{1}{q}u\eprodf\Omega v=\frac{(p+q-1)!}{p!q!}u\eprod\Omega v\]
so the result follows.
\end{proof}
\begin{rmk}
As an example, if \(E,E^*\) are dual spaces, then the insertion operator~\(i(h)\) on~\(\medeprod E^*\) with \(h\in E\) must be redefined as in this problem to be an antiderivation on~\(\medeprodf E^*\).\footnote{See subsection~5.14.}
\end{rmk}

\begin{exer}[8]
If \(\psi:E\to E\) is a linear transformation with \(\dim E=n\), then there exist linear transformations \(\psi_i^{(n)}:\medeprod^n E\to\medeprod^n E\) unique such that
\[(\psi-\lambda\iota)_{\eprod}^{(n)}=\sum_{i=0}^n\psi_i^{(n)}\lambda^{n-i}\qquad\lambda\in\Gamma\]
Moreover, \(\tr\psi_i^{(n)}=\alpha_i\) where \(\alpha_i\)~is the \(i\)-th characteristic coefficient of~\(\psi\).
\end{exer}
\begin{proof}
By an argument similar to that given in subsection~4.19 of~\cite{greub1},
\[(\psi-\lambda\iota)_{\eprod}^{(n)}=\sum_{i=0}^n\tilde{S}_i\]
where
\[\tilde{S}_i(\eprods{x}{1}{n})=\frac{(-\lambda)^{n-i}}{i!(n-i)!}\sum_{\sigma}\sign{\sigma}\eprods{\varphi x}{\sigma(1)}{\sigma(i)}\eprod\eprods{x}{\sigma(i+1)}{\sigma(n)}\]
This uniquely determines \(\psi_i^{(n)}\) with \(\tilde{S}_i=\psi_i^{(n)}\lambda^{n-i}\). By (4.47) and~(4.48) of~\cite{greub1},
\[\Delta_{\eprod}\after\tilde{S}_i=\alpha_i\lambda^{n-i}\Delta_{\eprod}=\Delta_{\eprod}\after(\alpha_i\lambda^{n-i}\iota)\]
where \(\Delta\ne 0\) is a determinant function in~\(E\). Since \(\Delta_{\eprod}\)~is injective, this implies
\[\psi_i^{(n)}\lambda^{n-i}=\tilde{S}_i=\alpha_i\lambda^{n-i}\iota\]
so \(\psi_i^{(n)}=\alpha_i\iota\) and \(\tr\psi_i^{(n)}=\alpha_i\).
\end{proof}
\begin{rmk}
We have \(\psi_0^{(n)}=(-1)^n\iota\), \(\psi_{1}^{(n)}=(-1)^{n-1}\theta_{\eprod}^{(n)}(\psi)\), and \(\psi_n^{(n)}=\psi_{\eprod}^{(n)}\).
\end{rmk}

\subsection*{\S~13}
\begin{rmk}
The multiplication map \(\mu:\medeprod E\to L(\medeprod E;\medeprod E)\) is linear, and is an algebra homomorphism by~(5.37). Dually, the insertion map \(i:\medeprod E\to L(\medeprod E^*;\medeprod E^*)\) is an algebra antihomomorphism by~(5.38).
\end{rmk}

\begin{rmk}
The insertion operator \(i(a):\medeprod E^*\from\medeprod E^*\) generalizes the substitution operator on skew-symmetric functions if \(E\)~is finite-dimensional. In particular if \(h\in E\), then the following diagram commutes:\footnote{See subsection~5.33.}
\begin{diagram}
\medeprod E^*	&\lTo^{i(h)}	&\medeprod E^*\\
\dTo<{\iso}		&				&\dTo>{\iso}\\
A^{\adot}(E)	&\lTo_{i_A(h)}	& A^{\adot}(E)
\end{diagram}
The results of this subsection show that insertion is dual to multiplication.
\end{rmk}

\subsection*{\S~14}
\begin{rmk}
The fact that \(i(h)^2=0\) is equivalent to the fact that skew-symmetric functions are alternating, when \(E\)~is finite-dimensional.
\end{rmk}

\begin{rmk}
Let \(E,E^*\) be a pair of dual \(n\)-dimensional vector spaces. If \(v^*\in\bigeprod^n E^*\) and \(v^*\ne 0\), then for any \(u^*\in\bigeprod^{n-1}E^*\) there is a unique vector \(h\in E\) with
\[i(h)v^*=u^*\]
\end{rmk}
\begin{proof}
If \(u^*=0\), we take \(h=0\). If \(u^*\ne 0\), then since \(u^*\)~has degree \(n-1\) it follows from Poincar\'e duality\footnote{See subsection~6.13.} that \(u^*\)~is decomposable and we may write
\[u^*=\eprods{x^*}{1}{n-1}\qquad(x^*_i\in E^*)\]
Fix \(h^*\in E^*\) with \(v^*=h^*\eprod u^*\). Let \(F^*\)~be the span of \(x^*_1,\ldots,x^*_{n-1}\), and let \(h\)~be the basis vector of~\(\oc{(F^*)}\) in~\(E\) with
\[i(h)h^*=\sprod{h^*}{h}=1\]
Since \(i(h)\)~is an antiderivation with respect to the canonical involution in~\(\bigeprod E^*\), it follows that
\[i(h)v^*=i(h)(h^*\eprod u^*)=i(h)h^*\eprod u^*-h^*\eprod i(h)u^*=u^*\]
The uniqueness of $h$ is clear.
\end{proof}
\begin{rmk}
This result is equivalent to the result in problem~5.1.2(b), which is also covered in the remarks on chapter~IV, \S~1 of~\cite{greub1} above, by the isomorphism \(\medeprod E^*\iso A^{\adot}(E)\) of \S~33 below.
\end{rmk}

\begin{exer}[1]
If \(u^*\in\medeprod^p E^*\) and \(i(a)u^*=0\) for all \(a\in\medeprod^k E\) for some \(k\le p\), then \(u^*=0\).
\end{exer}
\begin{proof}
We claim \(\sprod{u^*}{v}=0\) for all \(v\in\medeprod^p E\), from which it follows that \(u^*=0\). Since \(k\le p\), we may write
\[v=\sum_{\nu}a_{\nu}\eprod v_{\nu}\qquad a_{\nu}\in\medeprod^k E,\ v_{\nu}\in\medeprod^{p-k}E\]
Then
\[\sprod{u^*}{v}=\sum_{\nu}\sprod{u^*}{a_{\nu}\eprod v_{\nu}}=\sum_{\nu}\sprod{i(a_{\nu})u^*}{v_{\nu}}=0\qedhere\]
\end{proof}
\begin{rmk}
This result generalizes Proposition~5.14.2, which in turn generalizes the familiar fact that if a vector is zero under all linear functions, then the vector must be zero.
\end{rmk}

\begin{exer}[2]
Let \(E,E^*\) be a pair of dual \(n\)-dimensional vector spaces and \(e_{\nu},e^{*\nu}\) a pair of dual bases. If \(\varphi:E\to E\) is linear, then
\begin{align*}
\theta_{\eprod}(\varphi)&=\sum_{\nu}\mu(\varphi e_{\nu})i(e^{*\nu})\tag{1}\\
\theta^{\eprod}(\varphi)&=\sum_{\nu}\mu(e^{*\nu})i(\varphi e_{\nu})\tag{2}
\end{align*}
\end{exer}
\begin{proof}
By Corollary~II to Proposition~5.14.1,
\[i(e^{*\nu})(\eprods{x}{1}{p})=\sum_{\mu=1}^p(-1)^{\mu-1}\sprod{e^{*\nu}}{x_{\mu}}x_1\eprod\cdots\eprod\delete{x_{\mu}}\eprod\cdots\eprod x_p\]
It follows that
\[\mu(\varphi e_{\nu})i(e^{*\nu})(\eprods{x}{1}{p})=\sum_{\mu=1}^p x_1\eprod\cdots\eprod\varphi\bigl(\sprod{e^{*\nu}}{x_{\mu}}e_{\nu}\bigr)\eprod\cdots\eprod x_p\]
and so
\begin{align*}
\sum_{\nu=1}^n\mu(\varphi e_{\nu})i(e^{*\nu})(\eprods{x}{1}{p})&=\sum_{\mu=1}^p x_1\eprod\cdots\eprod\varphi\bigl(\sum_{\nu=1}^n\sprod{e^{*\nu}}{x_{\mu}}e_{\nu}\bigr)\eprod\cdots\eprod x_p\\
	&=\sum_{\mu=1}^p x_1\eprod\cdots\eprod\varphi x_{\mu}\eprod\cdots\eprod x_p\\
	&=\theta_{\eprod}(\varphi)(\eprods{x}{1}{p})
\end{align*}
This establishes~(1). Now (2)~follows from~(1) by duality since
\[\theta^{\eprod}(\varphi)=\theta_{\eprod}(\varphi)^*=\sum_{\nu=1}^n i(e^{*\nu})^*\mu(\varphi e_{\nu})^*=\sum_{\nu=1}^n\mu(e^{*\nu})i(\varphi e_{\nu})\qedhere\]
\end{proof}

\begin{exer}[3]
Let \(E,E^*\) be a pair of dual \(n\)-dimensional vector spaces and \(e_{\nu},e^{*\nu}\) a pair of dual bases.
\begin{enumerate}
\item[(a)] For \(x\in E\) and \(x^*\in E^*\),
\[i(x^*)\mu(x)+\mu(x)i(x^*)=\sprod{x^*}{x}\iota\]
\item[(b)] For \(u\in\medeprod^p E\),
\[\sum_{\nu}\mu(e_{\nu})i(e^{*\nu})u=pu\]
\item[(c)] For \(u\in\medeprod^p E\),
\[\sum_{\nu}i(e^{*\nu})\mu(e_{\nu})u=(n-p)u\]
\end{enumerate}
\end{exer}
\begin{proof}
For~(a), dualize Corollary~I to Proposition~5.14.1; for~(b), apply problem~2 with \(\varphi=\iota\); for~(c), apply (a) and~(b) to obtain
\[\sum_{\nu=1}^n i(e^{*\nu})\mu(e_{\nu})u=\sum_{\nu=1}^n\sprod{e^{*\nu}}{e_{\nu}}u-\sum_{\nu=1}^n\mu(e_{\nu})i(e^{*\nu})u=nu-pu\qedhere\]
\end{proof}

\begin{exer}[6]
If \(a\in\medeprod^p E\) and \(p\le q\), then
\[i(a)(\eprods{x^*}{1}{q})=\sum_{\nu_1<\cdots<\nu_p}(-1)^{\sum_{i=1}^p(\nu_i-i)}\sprod{\eprods{x^*}{\nu_1}{\nu_p}}{a}\eprods{x^*}{\nu_{p+1}}{\nu_q}\]
where \((\nu_{p+1},\ldots,\nu_q)\) is the ordered tuple complementary to \((\nu_1,\ldots,\nu_p)\).
\end{exer}
\begin{proof}
By induction on~\(p\). For \(p=0\) the result is trivial, and for \(p=1\) it is just Corollary~II to Proposition~5.14.1. We illustrate the induction step for \(p=2\), where we may assume that \(a=a_1\eprod a_2\) and write \(u^*=\eprods{x^*}{1}{q}\):
\begin{align*}
i(a)u^*&=i(a_2)i(a_1)(\eprods{x^*}{1}{q})\\
	&=i(a_2)\sum_{\nu_1}(-1)^{\nu_1-1}\sprod{x^*_{\nu_1}}{a_1}x^*_1\eprod\cdots\eprod\delete{x^*_{\nu_1}}\eprod\cdots\eprod x^*_q\\
	&=\ \ \sum_{\nu_1}(-1)^{\nu_1-1}\sprod{x^*_{\nu_1}}{a_1}i(a_2)(x^*_1\eprod\cdots\eprod\delete{x^*_{\nu_1}}\eprod\cdots\eprod x^*_q)\\
	&=\sum_{\nu_1<\nu_2}(-1)^{(\nu_1-1)+(\nu_2-2)}C_{\nu_1,\nu_2} x^*_1\eprod\cdots\eprod\delete{x^*_{\nu_1}}\eprod\cdots\eprod\delete{x^*_{\nu_2}}\eprod\cdots\eprod x^*_q
\end{align*}
where
\[C_{\nu_1,\nu_2}=\sum_{\sigma}\sign{\sigma}\sprod{x^*_{\nu_{\sigma(1)}}}{a_1}\sprod{x^*_{\nu_{\sigma(2)}}}{a_2}=\det(\sprod{x^*_{\nu_i}}{a_j})=\sprod{x^*_{\nu_1}\eprod x^*_{\nu_2}}{a_1\eprod a_2}\qedhere\]
\end{proof}

\subsection*{\S~15}
\begin{rmk}
The elements of~\(\medeprod(E\dsum F)\) are sums of wedge products with factors from~\(E\dsum F\), which by bilinearity of the wedge product expand to sums of wedge products with factors from \(E\) and~\(F\). By anticommutativity of the wedge product, the factors from~\(E\) in any product can be moved to the left of the factors from~\(F\) with at most a change in sign, resulting in a wedge product with a factor from~\(\medeprod E\) and a factor from~\(\medeprod F\). This leads to the natural isomorphism
\[\medeprod(E\dsum F)\iso\medeprod E\stprod\medeprod F\]
The anticommutative tensor product ensures it is an algebra isomorphism, in fact a \emph{graded algebra} isomorphism.
\end{rmk}

\begin{rmk}
In the proof of Theorem~5.15.1, it follows from~(5.42) that \(f\)~is homogeneous of degree zero. It follows from~(5.51) that \(f\)~preserves scalar products.
\end{rmk}

\subsection*{\S~16}
\begin{rmk}
The formula~(5.45) expresses commutativity of the following diagram:
\begin{diagram}
\medeprod(E\dsum F)				&\rTo^{(\varphi\dsum\psi)_{\eprod}}			&\medeprod(E'\dsum F')\\
\dTo<h>{\iso}					&											&\dTo>{h'}<{\iso}\\
\medeprod E\stprod\medeprod F	&\rTo_{\varphi_{\eprod}\tprod\psi_{\eprod}}	&\medeprod E'\stprod\medeprod F'
\end{diagram}
For \(x\in E\) and \(y\in F\),
\begin{align*}
(h'\after(\varphi\dsum\psi)_{\eprod})(x+y)&=h'(\varphi x+\psi y)\\
	&=\varphi x\tprod 1+1\tprod\psi y\\
	&=(\varphi_{\eprod}\tprod\psi_{\eprod})(x\tprod 1+1\tprod y)\\
	&=((\varphi_{\eprod}\tprod\psi_{\eprod})\after h)(x+y)
\end{align*}
It follows that the diagram commutes since \(\medeprod(E\dsum F)\) is generated as an algebra by the elements of \(E\dsum F\) and the scalar~\(1\).
\end{rmk}

\subsection*{\S~18}
\begin{rmk}
To prove~(5.48), for \(u\in\medeprod^p E\) and \(v\in\medeprod F\) observe that
\begin{align*}
\mu(a\tprod b)(u\tprod v)&=(a\tprod b)\eprod(u\tprod v)\\
	&=(-1)^{pq}(a\eprod u)\tprod(b\eprod v)\\
	&=(a\eprod(-1)^{pq}u)\tprod(b\eprod v)\\
	&=\bigl((\mu(a)\after\omega^q)u\bigr)\tprod\bigl(\mu(b)v\bigr)\\
	&=\bigl((\mu(a)\after\omega^q)\tprod\mu(b)\bigr)(u\tprod v)
\end{align*}
The result now follows by multilinearity.
\end{rmk}

\begin{rmk}
We provide an alternative proof of~(5.51).\footnote{This proof is similar to that of Proposition~6.18.1.} For \(u=\eprods{x}{1}{p}\in\medeprod^p E\), \(v=\eprods{y}{1}{r}\in\medeprod^r F\), \(u^*=\eprods{x^*}{1}{q}\in\medeprod^q E^*\), and \(v^*=\eprods{y^*}{1}{s}\in\medeprod^s F^*\), let
\[z_i=\begin{cases}
x_i&\text{for }1\le i\le p\\
y_{i-p}&\text{for }p<i\le p+r
\end{cases}\]
and dually for~\(z^*_j\). Then
\[\ssprod{u^*\tprod v^*}{u\tprod v}=\sprod{\eprods{z^*}{1}{q+s}}{\eprods{z}{1}{p+r}}\]
If \(p+r\ne q+s\), then the right-hand side is zero and~(5.51) holds, otherwise
\[\sprod{\eprods{z^*}{1}{q+s}}{\eprods{z}{1}{p+r}}=\det\sprod{z^*_j}{z_i}\]
Now \(\sprod{z^*_j}{z_i}=0\) if \(1\le i\le p\) and \(q<j\le q+s\), and also if \(p<i\le p+r\) and \(1\le j\le q\), so it follows that \(\det\sprod{z^*_j}{z_i}=0\) unless \(p=q\) and \(r=s\), in which case the determinant is block diagonal with
\[\det\sprod{z^*_j}{z_i}=\det\sprod{x^*_j}{x_i}\mult\det\sprod{y^*_j}{y_i}\]
Since \(\det\sprod{x^*_j}{x_i}=\sprod{u^*}{u}\) and \(\det\sprod{y^*_j}{y_i}=\sprod{v^*}{v}\), (5.51)~holds. The general case follows by multilinearity.
\end{rmk}

\subsection*{\S~21}
\begin{exer}[1]
In the exterior algebra~\(\medeprod E\),
\[u\eprod v=\pi_{\eprod}(u\tprod v)\qquad(u,v\in\medeprod E)\]
where \(\pi_1\) and~\(\pi_2\) are the canonical projections of \(E\dsum E\) onto~\(E\) and \(\pi=\pi_1+\pi_2\).
\end{exer}
\begin{proof}
By~(5.52), interchanging \(E\) and a dual space~\(E^*\) and noting that
\[\Delta^{\eprod}=(\Delta^*)_{\eprod}=(i_1^*+i_2^*)_{\eprod}=(\pi_1+\pi_2)_{\eprod}=\pi_{\eprod}\qedhere\]
\end{proof}
\begin{rmk}
This shows that \(\pi_{\eprod}\)~is the structure map for~\(\medeprod E\). If \(\mu\)~is the structure map for~\(\medtprod E\), then the following diagram commutes:
\begin{diagram}
\medtprod E\stprod\medtprod E	&\rTo^{\mu}			&\medtprod E\\
\dTo<{\pi\tprod\pi}				&					&\dTo>{\pi}\\
\medeprod E\stprod\medeprod E	&\rTo_{\pi_{\eprod}}&\medeprod E
\end{diagram}
Also \(\pi_{\eprod}\)~is isomorphic to~\(\pi_A\) on~\(X(E)\).\footnote{See problem~1 in chapter~4, \S~8 above.}
\end{rmk}

\begin{exer}[2]
Let \(E=E_1+E_2\) and \(E_{12}=E_1\sect E_2\). For \(\proj{E}=E/E_{12}\), \(\proj{E_1}=E_1/E_{12}\), and \(\proj{E_2}=E_2/E_{12}\), the following diagram commutes:
\begin{diagram}
\medeprod E_1\stprod\medeprod E_2				&\rTo^{f}_{\iso}	&\medeprod(E_1\dsum E_2)				&\rTo^{\varphi_{\eprod}}	&\medeprod E\\
\dTo<{(\rho_1)_{\eprod}\tprod(\rho_2)_{\eprod}}	&					&										&							&\dTo>{\rho_{\eprod}}\\
\medeprod\proj{E_1}\stprod\medeprod\proj{E_2}	&\rTo_{g}^{\iso}	&\medeprod(\proj{E_1}\dsum\proj{E_2})	&\rTo_{\psi_{\eprod}}^{\iso}&\medeprod\proj{E}
\end{diagram}
Here \(f\) and~\(g\) are defined as in~(5.42),
\[\varphi=[i_1,i_2]:E_1\dsum E_2\to E\]
where \(i_1:E_1\to E\) and \(i_2:E_2\to E\) are the inclusions,
\[\psi=[\proj{i_1},\proj{i_2}]:\proj{E_1}\dsum\proj{E_2}\to\proj{E}\]
is the induced isomorphism described in chapter~II, \S~4 of~\cite{greub1} above, and the \(\rho\)'s are canonical projections.
\end{exer}
\begin{proof}
By definition of~\(\psi\), this diagram of canonical injections commutes:
\begin{diagram}[nohug]
\proj{E_1}&\rTo^{j_1}&\proj{E_1}\dsum\proj{E_2}&\lTo^{j_2}&\proj{E_2}\\
&\rdTo<{\proj{i_1}}&\dTo>{\psi}&\ldTo>{\proj{i_2}}&\\
&&\proj{E}&&
\end{diagram}
Hence for \(u_1\in\medeprod E_1\) and \(u_2\in\medeprod E_2\),
\begin{align*}
(\rho_{\eprod}\after\varphi_{\eprod}\after f)(u_1\tprod u_2)&=\rho_{\eprod}\bigl((i_1)_{\eprod}(u_1)\eprod(i_2)_{\eprod}(u_2)\bigr)\\
	&=(\rho\after i_1)_{\eprod}(u_1)\eprod(\rho\after i_2)_{\eprod}(u_2)\\
	&=(\proj{i_1}\after\rho_1)_{\eprod}(u_1)\eprod(\proj{i_2}\after\rho_2)_{\eprod}(u_2)\\
	&=(\psi\after j_1\after\rho_1)_{\eprod}(u_1)\eprod(\psi\after j_2\after\rho_2)_{\eprod}(u_2)\\
	&=\psi_{\eprod}\bigl[(j_1)_{\eprod}\bigl((\rho_1)_{\eprod}(u_1)\bigr)\eprod(j_2)_{\eprod}\bigl((\rho_2)_{\eprod}(u_2)\bigr)\bigr]\\
	&=\psi_{\eprod}\bigl[g\bigl((\rho_1)_{\eprod}(u_1)\tprod(\rho_2)_{\eprod}(u_2)\bigr)\bigr]\\
	&=\bigl[\psi_{\eprod}\after g\after\bigl((\rho_1)_{\eprod}\tprod(\rho_2)_{\eprod}\bigr)\bigr](u_1\tprod u_2)
\end{align*}
The result now follows by linearity.
\end{proof}

\subsection*{\S~23}
\begin{rmk}
By~(5.54) and a remark in \S~15 above, the elements of~\(I_{E_1}\) are sums of wedge products with \emph{at least one factor from~\(E_1\)} followed by zero or more factors from~\(E_2\), so (5.55)~follows. Since any element of~\(\medeprod E\) is equal to such an element plus a sum of wedge products with factors from~\(E_2\) (zero factors from~\(E_1\)), (5.56)~follows.
\end{rmk}

\begin{rmk}
By~(5.63), we have
\[I_{\bigeprod^p E_1}=I_{\bigeprod^{p+1}E_1}\dsum\bigl(\medeprod^p E_1\stprod\medeprod E_2\bigr)\]
Taking \(p=0\) in this equation yields~(5.56).
\end{rmk}

\subsection*{\S~25}
\begin{rmk}
In the proof of Proposition~5.25.2, we see that if \(u\in\medeprod^r E\), then we may take \(v\in\medeprod^{n-r}E\).
\end{rmk}

\begin{rmk}
In the proof of the second part of the corollary to Proposition~5.25.2, if \(E\)~has infinite dimension, then \(I^q\ne 0\) for each~\(q\) (since for example there are \(q\)~linearly independent vectors in~\(E\), so \(\medeprod^q E\ne 0\)). If there were a minimal ideal~\(I\) in~\(\medeprod E\), then we would have \(0\ne I\subseteq I^q\) for all~\(q\), hence \(0\ne I\subseteq\bigsect_q I^q\). Since \(\bigsect_q I^q=0\) (which is equally true when \(E\)~has finite dimension), it follows that there is no such minimal ideal~\(I\).
\end{rmk}

\subsection*{\S~26}
\begin{rmk}
We prove that
\[N(E)=\begin{cases}
0&\text{if }\dim E=\infty\\
\medeprod^n E&\text{if }\dim E=n
\end{cases}\]
First if \(u\in N(E)\), then \(u=\sum_p u_p\) with \(u_p\in N(E)\sect\medeprod^p E\) since \(N(E)\)~is a graded ideal. We claim that \(u_p=0\) for all \(p<\dim E\). We know that \(u_p\in\medeprod^p F\subseteq\medeprod^p E\) for some \(m\)-dimensional subspace~\(F\) of~\(E\) with \(p<m\). Let \(e_1,\ldots,e_m\) be a basis of~\(F\) and write
\[u_p=\sum_{<}\lambda^{\nu_1,\ldots,\nu_p}\eprods{e}{\nu_1}{\nu_p}\]
For any given \(1\le\mu_1<\cdots<\mu_p\le m\), fix \(1\le\mu\le m\) with \(\mu\ne\mu_i\) for all \(1\le i\le p\). Since \(u_p\in N(E)\),
\[0=e_{\mu}\eprod u_p=\sum_{<}\lambda^{\nu_1,\ldots,\nu_p}e_{\mu}\eprod\eprods{e}{\nu_1}{\nu_p}\]
Now the \(e_{\mu}\eprod\eprods{e}{\nu_1}{\nu_p}\) that are nonzero are linearly independent in~\(\medeprod^{p+1}F\). These include \(e_{\mu}\eprod\eprods{e}{\mu_1}{\mu_p}\), so we must have \(\lambda^{\mu_1,\ldots,\mu_p}=0\). Since the~\(\mu_i\) were arbitrary, it follows that \(u_p=0\). Now if \(\dim E=\infty\) this implies \(u=0\), and if \(\dim E=n\) this implies \(u=u_n\in\medeprod^n E\). Conversely in the latter case, if we take \(F=E\) and \(x=\sum_i\lambda^i e_i\) arbitrary, then
\[x\eprod\eprods{e}{1}{n}=\sum_i\lambda^i e_i\eprod\eprods{e}{1}{n}=0\]
so \(\medeprod^n E\subseteq N(E)\), completing the proof.

The fact that \(N(E)=\medeprod^n E\) if \(\dim E=n\) also follows from Proposition~5.26.1 by taking \(F=E\) and \(p=n\).
\end{rmk}

\begin{rmk}
If \(E\)~is \(n\)-dimensional, then we have the following inclusions:
\begin{diagram}
N(\medeprod^0 E)	&\subseteq	&N(\medeprod^1 E)	&\subseteq	&\cdots	&\subseteq	&N(\medeprod^n E)	&\subseteq	&N(\medeprod^{n+1}E)\\
\dEqualto			&			&\dEqualto			&			&		&			&\dEqualto			&			&\dEqualto\\
I_{\bigeprod^{n+1}E}&\subseteq	&I_{\bigeprod^n E}	&\subseteq	&\cdots	&\subseteq	&I_{\bigeprod^1 E}	&\subseteq	&I_{\bigeprod^0 E}
\end{diagram}
\end{rmk}

\begin{rmk}
In the corollary to Proposition~5.26.1, note that \(N(y)=I_y\), so it follows that \(u\)~is divisible by~\(y\) if and only if \(y\eprod u=0\) for \emph{any} \(u\in\medeprod E\).
\end{rmk}

\subsection*{\S~27}
\begin{rmk}
If \(\varphi:E\to\medeprod E\) is linear and \(F\)~is a finite-dimensional subspace of~\(E\) such that
\[y\eprod\varphi y=0\qquad(y\in F)\]
then there is \(v\in\medeprod E\) with
\[\varphi y=y\eprod v\qquad(y\in F)\]
If \(\varphi\)~is homogeneous of degree~\(k\) (\(\im\varphi\subseteq\medeprod^{k+1}E\)), then we may take \(v\in\medeprod^k E\).
\end{rmk}
\begin{proof}
By induction on \(m=\dim F\). If \(m=0\), then we can take \(v=0\), say. If \(m\ge 1\) and the result holds for subspaces of dimension \(m-1\), fix \(a\in F\) with \(a\ne 0\). Since \(a\eprod\varphi a=0\), it follows from the corollary to Proposition~5.26.1 that there is \(c\in\medeprod E\) with \(\varphi a=a\eprod c\).

Define \(\sigma:E\to\medeprod E\) by \(\sigma x=\varphi x-x\eprod c\). Then \(\sigma a=0\), \(y\eprod\sigma y=0\) for all \(y\in F\), and \(a\eprod\sigma y=0\) for all \(y\in F\), as in~(5.70).

Write \(E=\gen{a}\dsum E'\) and \(F=\gen{a}\dsum F'\). Since \(\medeprod E=\medeprod\gen{a}\stprod\medeprod E'\), there are linear maps \(\sigma_i:E\to\medeprod E'\) (\(i=0,1\)) with
\[\sigma x=1\tprod\sigma_0 x+a\tprod\sigma_1 x\qquad(x\in E)\]
Taking \(x=y\in F\) and multiplying both sides by~\(a\), it follows that \(\sigma_0 y=0\) for all \(y\in F\), so \(\sigma y=a\tprod\sigma_1 y\) for all \(y\in F\). A simple computation then shows that \(y\eprod\sigma_1 y=0\) for all \(y\in F\), and hence in particular for all \(y\in F'\).

By induction, there is \(v'\in\medeprod E\) with \(\sigma_1 y'=y'\eprod v'\) for all \(y'\in F'\). For \(y=\lambda a+y'\) with \(\lambda\in\Gamma\) and \(y'\in F'\), it follows that \(\sigma y=-y\eprod(a\eprod v')\), as in~(5.73). Therefore
\[\varphi y=y\eprod c+\sigma y=y\eprod(c-a\eprod v')\qquad(y\in F)\]
The result now follows by taking \(v=c-a\eprod v'\).

If \(\im\varphi\subseteq\medeprod^{k+1} E\), the refinement follows by projecting onto~\(\medeprod^{k+1}E\) as in the proof of the corollary to Proposition~5.27.1.
\end{proof}

\begin{exer}[4]
For a subspace~\(F\) of~\(E\), there is a natural isomorphism of graded algebras \(\medeprod(E/F)\iso\medeprod E/I_F\).
\end{exer}
\begin{proof}
If \(\pi:E\to E/F\) denotes the canonical projection, then \(\pi_{\eprod}:\medeprod E\to\medeprod(E/F)\) is a surjective homomorphism homogeneous of degree zero with
\[\ker\pi_{\eprod}=I_{\ker\pi}=I_F\]
by~(5.57). It follows that there is an induced isomorphism \(\proj{\pi_{\eprod}}:\medeprod E/I_F\to\medeprod(E/F)\) which is also homogeneous of degree zero.
\end{proof}

\subsection*{\S~28}
\begin{exer}[2]
Let \(E,E^*\) be a pair of dual finite-dimensional vector spaces. If \(F\)~is a subspace of~\(E\), then
\[\medeprod F=\bigsect_{u^*\in I_{\oc{F}}}\ker i(u^*)\]
\end{exer}
\begin{proof}
The right-hand side is a nonzero graded subalgebra of~\(\medeprod E\) which has grade \(1\) subspace~\(F\) and is closed under~\(i(h^*)\) for all \(h^*\in E^*\) by~(5.38), so the result follows from Proposition~5.28.2.
\end{proof}

\subsection*{\S~29}
\begin{rmk} The formula~(5.81) follows from~(4.11) and a diagram in \S~1 above.
\end{rmk}

\subsection*{\S~30}
\begin{rmk}
Why are there factorials in the definition of the Grassmann product? From problem~5.7.6, we know they are not needed algebraically, although they may be more natural geometrically in certain applications. Their presence has the following consequences:
\begin{itemize}
\item The substitution operator~\(i_A(h)\) must be rescaled in \S~32 (see below).
\item The isomorphism \(\medtprod E^*\to T^{\tdot}(E)\) must be rescaled when it is restricted to obtain the isomorphism \(X(E^*)\to A^{\tdot}(E)\) in \S~33.
\item The scalar product between \(T^p(E)\) and~\(T_p(E)\) must be rescaled to obtain the scalar product between \(A^p(E)\) and~\(A_p(E)\) in \S~34.
\item The definitions and proofs are uglier, and do not generalize beyond fields of characteristic zero.
\end{itemize}
\end{rmk}

\subsection*{\S~32}
\begin{rmk}
In this subsection, the substitution operator~\(i_A(h)\) should be redefined on~\(T^p(E)\) as
\[i_A(h)=\frac{1}{p}\sum_{\nu=1}^p(-1)^{\nu-1}i_{\nu}(h)\]
Then for \(\Phi\in A^p(E)\),
\begin{align*}
p\mult(i_A(h)\Phi)(x_1,\ldots,x_{p-1})&=\sum_{\nu=1}^p(-1)^{\nu-1}(i_{\nu}(h)\Phi)(x_1,\ldots,x_{p-1})\\
	&=\sum_{\nu=1}^p(-1)^{\nu-1}\Phi(x_1,\ldots,x_{\nu-1},h,x_{\nu+1},\ldots,x_{p-1})\\
	&=\sum_{\nu=1}^p\Phi(h,x_1,\ldots,x_{p-1})\\
	&=p\mult\Phi(h,x_1,\ldots,x_{p-1})\\
	&=p\mult(i_1(h)\Phi)(x_1,\ldots,x_{p-1})
\end{align*}
so
\[i_A(h)\Phi=i_1(h)\Phi\]
as desired. This redefinition is required to make~\(i_A(h)\) an antiderivation in the algebra~\(A^{\adot}(E)\) of skew-symmetric multilinear functions (under the definition of Grassmann product in subsection~5.30), although under this redefinition \(i_A(h)\)~is not an antiderivation in the algebra~\(T^{\tdot}(E)\) of multilinear functions.\footnote{See also the corrected version of problem~5.12.6 above.}
\end{rmk}

\subsection*{\S~33}
\begin{rmk}
The algebra isomorphism \(\beta\after\eta:\medeprod E^*\to A^{\adot}(E)\) satisfies
\[\eprods{f^*}{1}{p}\mapsto\eprods{f^*}{1}{p}=p!\,A(\fprods{f^*}{1}{p})\qquad f^*_i\in E^*\]
The product on the left is the exterior product and the product on the right is the Grassmann product, where \(f^*_i\)~has been identified with \(\sprod{f^*_i}{-}\).
\end{rmk}

\subsection*{\S~34}
\begin{rmk}
We see that the algebra isomorphisms \(\medeprod E^*\iso A^{\adot}(E)\) and \(\medeprod E\iso A_{\adot}(E)\) preserve scalar products.
\end{rmk}

\newpage
\section*{Chapter~6}
{\boldmath\textbf{In this chapter, \(E\) and~\(E^*\) are dual vector spaces over~\(\Gamma\). In \S6--16, they are \(n\)-dimensional.}}

\subsection*{Products}
\noindent This table summarizes the way in which products are transformed by maps for a \emph{finite-dimensional} vector space~\(E\):
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{Map}&\textbf{Source product}&\textbf{Target product}\\
\hline
\(T_E\)&[Skew] mixed exterior&[Skew] box\\
	&[Skew] mixed intersection&[Skew] intersection\\
	&Composition&Composition\\
	&Inner&Inner\\
\hline
\(D_E\)&[Skew] mixed exterior&[Skew] mixed intersection\\
	&[Skew] mixed intersection&[Skew] mixed exterior\\
	&Composition&Composition (opposite)\\
	&Inner&Inner\\
\hline
\end{tabular}
\end{center}

\subsection*{\S~1}
% TODO: mixed skew-symmetric function implementation of mixed exterior algebra

\begin{rmk}
Recall that
\[M^p_q(E^*,E)=\underbrace{E^*\times\cdots\times E^*}_p\times\underbrace{E\times\cdots\times E}_q\]
Let \(\medeprod^p_q(E^*,E)=\medeprod^p E^*\tprod\medeprod^q E\). The map \(\varphi:M^p_q(E^*,E)\to\medeprod^p_q(E^*,E)\) defined by
\[\varphi(x^*_1,\ldots,x^*_p\,;\,x_1,\ldots,x_q)=(\eprods{x^*}{1}{p})\tprod(\eprods{x}{1}{q})\]
is skew-symmetric of type~\((p,q)\), and Proposition~6.1.1 shows that \(\varphi\)~is \emph{universal} with this property in that every skew-symmetric map of type~\((p,q)\) factors in a unique way through~\(\varphi\): if \(\psi:M^p_q(E^*,E)\to H\) is any such map, then there is a unique linear map \(f:\medeprod^p_q(E^*,E)\to H\) with \(f\after\varphi=\psi\):
\begin{diagram}[nohug]
M^p_q(E^*,E)		&\rTo^{\psi}&H\\
\dTo<{\varphi}		&\ruTo>f	&\\
\medeprod^p_q(E^*,E)&			&
\end{diagram}
In the proof in the book, observe that \(N^p(E^*)\subseteq N_1(\beta)\) and \(N^q(E)\subseteq N_2(\beta)\), so \(\beta\) induces~\(\gamma\) as in problem~1.3.5(a).

As an alternative proof, for fixed covectors \(x^*_1,\ldots,x^*_p\in E^*\) observe that the map defined by
\[(x_1,\ldots,x_q)\mapsto\psi(x^*_1,\ldots,x^*_p\,;\,x_1,\ldots,x_q)\]
is skew-symmetric, so there is a linear map \(g_{x^*_1\cdots x^*_p}:\medeprod^q E\to H\) with
\[g_{x^*_1\cdots x^*_p}(\eprods{x}{1}{q})=\psi(x^*_1,\ldots,x^*_p\,;\,x_1,\ldots,x_q)\]
The map \((x^*_1,\ldots,x^*_p)\mapsto g_{x^*_1\cdots x^*_p}\) is also skew-symmetric, so there is a linear map \(\medeprod^p E^*\to L(\medeprod^q E,H)\) which corresponds to the bilinear map \(\gamma:\medeprod^p E^*\times\medeprod^q E\to H\) with
\begin{align*}
\gamma(\eprods{x^*}{1}{p}\,,\eprods{x}{1}{q})&=g_{x^*_1\cdots x^*_p}(\eprods{x}{1}{q})\\
	&=\psi(x^*_1,\ldots,x^*_p\,;\,x_1,\ldots,x_q)
\end{align*}
The linear map \(f:\medeprod^p_q(E^*,E)\to H\) induced by~\(\gamma\) satisfies \(f\after\varphi=\psi\) and is unique with this property.
\end{rmk}

\begin{rmk}
Let \(\pi_1:\medtprod^p E^*\to\medeprod^p E^*\) and \(\pi_2:\medtprod^q E\to\medeprod^q E\) denote the canonical projections. The map
\[\pi_1\tprod\pi_2:\medtprod^p_q(E^*,E)\to\medeprod^p_q(E^*,E)\]
is surjective with
\begin{align*}
\ker(\pi_1\tprod\pi_2)&=\ker\pi_1\tprod\medtprod^q E+\medtprod^p E^*\tprod\ker\pi_2\\
	&=N^p(E^*)\tprod\medtprod^q E+\medtprod^p E^*\tprod N^q(E)\\
	&=T(N^p(E^*),N^q(E))
\end{align*}
so it induces the canonical isomorphism
\[\frac{\medtprod^p_q(E^*,E)}{T(N^p(E^*),N^q(E))}\ \iso\ \medeprod^p_q(E^*,E)\]
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
There is a canonical isomorphism
\[\frac{\medtprod(E^*,E)}{T(N(E^*),N(E))}\ \iso\ \medeprod(E^*,E)\]
\end{rmk}

\begin{rmk}
If \(w\in\medeprod(E^*,E)\), then
\[w^{p+q}=\frac{1}{(p+q)!}\underbrace{\mprods{w}{}{}}_p\mprod\underbrace{\mprods{w}{}{}}_q=\frac{p!\,q!}{(p+q)!}w^p\mprod w^q\qquad(p,q\ge 0)\]
In particular,
\[w\mprod w^k=w^k\mprod w=(k+1)w^{k+1}\qquad(k\ge 0)\]
\end{rmk}

\begin{rmk}
The inner product in~(6.2), which is just a special case of~(1.25), is compatible with the bigradation in~\(\medeprod(E^*,E)\) and therefore restricts to a scalar product between \(\medeprod^p_q(E^*,E)\) and \(\medeprod^q_p(E^*,E)\). In particular it restricts to an inner product in~\(\medeprod^p_p(E^*,E)\).

The flip isomorphism used in the definition of~(1.25) induces flips in many results in the mixed exterior algebra---for example, in various results related to the bigradation, and results like
\[i(u^*\tprod u)=i(u)\tprod i(u^*)\qquad(u^*\in\medeprod E^*,u\in\medeprod E)\]
and~(6.4).
\end{rmk}

\begin{rmk}
If \(z\in\medeprod^p_q(E^*,E)\) and \(w\in\medeprod^r_s(E^*,E)\), then \(\mu(z)w\in\medeprod^{p+r}_{q+s}(E^*,E)\), so \(\mu(z)\)~is homogeneous of bidegree~\((p,q)\). It follows from this that \(i(z)\)~is homogeneous of bidegree~\((-q,-p)\) (note the flip!).
\end{rmk}

\begin{rmk}
If \(u^*\in\medeprod E^*\), \(u\in\medeprod E\), \(x^*\in E^*\), and \(x\in E\), then
\[\sprod{x^*\tprod u}{u^*\tprod x}=\sprod{\mu(x^*)u^*}{\mu(x)u}+\sprod{i(x)u^*}{i(x^*)u}\]
\end{rmk}
\begin{proof}
By Corollary~I to Proposition~5.14.1,
\begin{align*}
\sprod{x^*\tprod u}{u^*\tprod x}&=\sprod{x^*}{x}\sprod{u^*}{u}\\
	&=\bigsprod{\sprod{x^*}{x}u^*}{u}\\
	&=\bigsprod{i(x)\mu(x^*)u^*+\mu(x^*)i(x)u^*}{u}\\
	&=\sprod{i(x)\mu(x^*)u^*}{u}+\sprod{\mu(x^*)i(x)u^*}{u}\\
	&=\sprod{\mu(x^*)u^*}{\mu(x)u}+\sprod{i(x)u^*}{i(x^*)u}\qedhere
\end{align*}
\end{proof}

\begin{rmk}
The flip operator \(Q_E:\medeprod(E^*,E)\to\medeprod(E,E^*)\) restricts to operators
\[Q_E^{p,q}:\medeprod^p_q(E^*,E)\to\medeprod^q_p(E,E^*)\]
In particular \(Q_E^{p,p}:\Delta_p E\to\Delta_p E^*\). It follows that \(Q_{E^*}^{q,p}\after Q_E^{p,q}=\iota\), so the operators are linear isomorphisms. They also preserve scalar products. Finally, \(Q_E^*=Q_{E^*}\) and \((Q_E^{p,q})^*=Q_{E^*}^{p,q}\).
\end{rmk}

\begin{rmk}
Interchanging the roles of \(E\) and~\(E^*\), we obtain the map
\[T_{E^*}:\medeprod(E,E^*)\to L(\medeprod E^*;\medeprod E^*)\]
given by
\[T_{E^*}(b\tprod a^*)v^*=\sprod{v^*}{b}a^*\qquad(v^*\in\medeprod E^*)\]
Note \(T_{E^*}(b\tprod a^*)\) is dual to \(T_E(a^*\tprod b)\).
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
The \(p\)-ary box product, which is a generalization of the \(p\)-th exterior power, is \(p\)-linear and symmetric, so factors through the \(p\)-th symmetric power of~\(L(E;E)\).
\end{rmk}

\begin{rmk}
If \(E\)~is finite-dimensional and \(\varphi_1,\ldots,\varphi_p\) are linear transformations of~\(E\), then
\[(\bprods{\varphi}{1}{p})^*=\bprods{\varphi^*}{1}{p}\]
\end{rmk}
\begin{proof}
By multilinearity, we may assume without loss of generality that
\[\varphi_i=T_E(a^*_i\tprod a_i)\qquad(a^*_i\in E^*,a_i\in E)\]
Then
\[\varphi_i^*=T_{E^*}(a_i\tprod a^*_i)\]
and
\begin{align*}
\bprods{\varphi^*}{1}{p}&=T_{E^*}[(a_1\tprod a^*_1)\mprod\cdots\mprod(a_p\tprod a^*_p)]\\
	&=T_{E^*}[(\eprods{a}{1}{p})\tprod(\eprods{a^*}{1}{p})]\\
	&=T_E[(\eprods{a^*}{1}{p})\tprod(\eprods{a}{1}{p})]^*\\
	&=T_E[(a^*_1\tprod a_1)\mprod\cdots\mprod(a^*_p\tprod a_p)]^*\\
	&=(\bprods{\varphi}{1}{p})^*\qedhere
\end{align*}
\end{proof}

\begin{rmk}
Let \(E,E^*\) be a pair of dual \(n\)-dimensional spaces with dual bases \(e_i,e^{*i}\) and let \(\varphi_1,\ldots,\varphi_p\) be linear transformations of~\(E\) with \(1\le p\le n\). If the induced basis of~\(\medeprod^p E\) is ordered lexically, then for
\[1\le\nu_1<\cdots<\nu_p\le n\qquad\text{and}\qquad 1\le\mu_1<\cdots<\mu_p\le n\]
the \((\nu,\mu)\) entry of the \(\binom{n}{p}\times\binom{n}{p}\) matrix of \(\bprods{\varphi}{1}{p}\) with respect to the basis is
\[\sum_{\sigma,\tau\in S_p}\sign{\sigma}\sign{\tau}\sprod{e^{*\mu_{\tau(1)}}}{\varphi_1e_{\nu_{\sigma(1)}}}\cdots\sprod{e^{*\mu_{\tau(p)}}}{\varphi_pe_{\nu_{\sigma(p)}}}\]
For \(n\times n\) matrices \(A_1,\ldots,A_p\), we define the \(\binom{n}{p}\times\binom{n}{p}\) box product matrix by
\[(\bprods{A}{1}{p})^{\mu}_{\nu}=\sum_{\sigma,\tau\in S_p}\sign{\sigma}\sign{\tau}(A_1)^{\mu_{\tau(1)}}_{\nu_{\sigma(1)}}\cdots(A_p)^{\mu_{\tau(p)}}_{\nu_{\sigma(p)}}\]
It then follows that
\[M(\bprods{\varphi}{1}{p})=M(\varphi_1)\bprod\cdots\bprod M(\varphi_p)\]
with respect to the given bases.
\end{rmk}

\begin{rmk}
If \(E\)~is finite-dimensional, we can generalize the definition of the box product. For linear transformations \(\varphi\) and~\(\psi\) of~\(\medeprod E\), we define
\[\varphi\bprod\psi=T_E(T_E^{-1}\varphi\mprod T_E^{-1}\psi)\]
and similarly for more than two transformations. Then \(T_E\)~becomes an algebra isomorphism from the mixed exterior algebra \(\medeprod(E^*,E)\) to a box product algebra in~\(L(\medeprod E;\medeprod E)\). The box product is associative and unital, and is \emph{commutative} on the diagonal subalgebra in~\(L(\medeprod E;\medeprod E)\). We also define
\[\medeprod^p\varphi=\frac{1}{p!}\underbrace{\bprods{\varphi}{}{}}_p\]
when it is understood to be a linear transformation of~\(\medeprod E\). By Proposition~6.3.1, these definitions agree with the ones previously given for transformations of~\(E\) when \(L(\medeprod^p E;\medeprod^p E)\)~is viewed as a subspace of~\(L(\bigeprod E;\bigeprod E)\).\footnote{See the remark in chapter~II, \S~4 of~\cite{greub1} above.}

For fixed~\(\varphi\), there is a left multiplication operator~\(\mu_{\bprod}(\varphi)\) given by
\[\mu_{\bprod}(\varphi)\psi=\varphi\bprod\psi\]
and a dual insertion operator \(i_{\bprod}(\varphi)\) satisfying
\[\sprod{i_{\bprod}(\varphi)\chi}{\psi}=\sprod{\chi}{\varphi\bprod\psi}=\sprod{\chi}{\mu_{\bprod}(\varphi)\psi}\]
For \(z\in\medeprod(E^*,E)\), the following diagrams commute:
\begin{diagram}
\medeprod(E^*,E)&\rTo^{T_E}_{\iso}	&L(\medeprod E;\medeprod E)	&&&\medeprod(E^*,E)	&\rTo^{T_E}_{\iso}	&L(\medeprod E;\medeprod E)\\
\dTo<{\mu(z)}	&					&\dTo>{\mu_{\bprod}(T_Ez)}	&&&\dTo<{i(z)}		&					&\dTo>{i_{\bprod}(T_Ez)}\\
\medeprod(E^*,E)&\rTo_{T_E}^{\iso}	&L(\medeprod E;\medeprod E)	&&&\medeprod(E^*,E)	&\rTo_{T_E}^{\iso}	&L(\medeprod E;\medeprod E)
\end{diagram}
An argument similar to that given in the previous remark shows that
\[(\varphi\bprod\psi)^*=\varphi^*\bprod\psi^*\]
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The diagonal subspace \(\Delta E=\sum_p\Delta_p E\) is closed under the composition product. In fact, if \(w=\sum_p w_p\) with \(w_p\in\Delta_p E\) and \(z=\sum_q z_q\) with \(z_q\in\Delta_q E\), then
\[w\after z=\sum_{p,q}w_p\after z_q\]
But \(w_p\after z_q=0\) if \(p\ne q\) by~(6.10), and \(w_p\after z_p\in\Delta_p E\) by~(6.11), so
\[w\after z=\sum_p w_p\after z_p\in\Delta E\]
For this reason \(\Delta E\)~is a subalgebra under the composition product, although it is not commutative like it is under the mixed exterior product if \(\dim E>1\).
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Proposition~6.5.1 helps us convert problems in the \emph{non-commutative} composition algebra in~\(\Delta E\) into problems in the \emph{commutative} mixed exterior algebra in~\(\Delta E\). Because many of the results of classical linear algebra have to do with the composition product, this is an important and powerful technique.
\end{rmk}

\begin{rmk}
In the Corollary to Proposition~6.5.1,
\begin{align*}
p!\,i(z)(w^p)&=i(z)(\underbrace{\mprods{w}{}{}}_p)\\
	&=\sum_{\nu}\sprod{z}{w}\underbrace{\mprods{w}{}{}}_{p-1}-\sum_{\nu<\mu}2(w\after z\after w)\underbrace{\mprods{w}{}{}}_{p-2}\\
	&=p\mult(p-1)!\,\sprod{z}{w}w^{p-1}-\frac{p(p-1)}{2}\mult 2\mult(p-2)!\,(w\after z\after w)w^{p-2}\\
	&=p!\,\bigl[\sprod{z}{w}w^{p-1}-(w\after z\after w)w^{p-2}\bigr]
\end{align*}
Dividing both sides by~\(p!\) yields the desired result.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
It follows from~(6.12) that \(T_E\)~is an isometry, so \(T_E^*=T_E^{-1}\).
\end{rmk}

\subsection*{\S~7}
\begin{rmk}
It follows from~(6.14) and a remark in \S~2 above that
\[\unit\mprod\unit_p=\unit_p\mprod\unit=(p+1)\unit_{p+1}\qquad(0\le p\le n-1)\]
\end{rmk}

\begin{rmk}
In~(6.17), we see that the unit tensor~\(\unit_p\) in~\(\medeprod^p_p(E^*,E)\) encodes all of the information about the scalar product between \(\medeprod^p E^*\) and~\(\medeprod^p E\), just like a metric tensor does for an inner product.\footnote{See the remarks in chapter~3, \S~17 above.}
\end{rmk}

\begin{rmk}
In the proof of Proposition~6.7.1, if (6.19)~holds for some~\(q\ge 1\), then for any \(p\ge q+1\) we have
\begin{align*}
(q+1)i(\unit_{q+1})\unit_p&=i(\unit_q\mprod\unit)\unit_p\\
	&=i(\unit)(i(\unit_q)\unit_p)\\
	&=\binom{n-p+q}{q}i(\unit)\unit_{p-q}\\
	&=\binom{n-p+q}{q}(n-p+q+1)\unit_{p-(q+1)}
\end{align*}
Since
\[\binom{n-p+q}{q}\frac{n-p}{q+1}=\binom{n-p+q}{q+1}\]
and
\[\binom{n-p+q}{q+1}+\binom{n-p+q}{q}=\binom{n-p+q+1}{q+1}\]
it follows that
\[i(\unit_{q+1})\unit_p=\binom{n-p+q+1}{q+1}\unit_{p-(q+1)}\]
so (6.19)~holds for~\(q+1\).
\end{rmk}

\subsection*{\S~8}
\begin{rmk}
It follows from Theorem~6.8.1 parts (2) and~(3) that
\[D_p^{-1}=(D^p)^*\qquad\text{and}\qquad(D^p)^{-1}=D_p^*\]
\end{rmk}

\begin{rmk}
Since \(D_e=\sum_p D_p\), it follows from Theorem~6.8.1(2) that
\[D_e^*=\sum_p D_p^*=\sum_p(-1)^{p(n-p)}D_{n-p}=\sum_p(-1)^{p(n-p)}D_p=\omega_{E^*}\after D_e\]
where \(\omega_{E^*}(u^*)=(-1)^{p(n-p)}u^*\) for \(u^*\in\medeprod^p E^*\). Similarly \((D^e)^*=\omega_E\after D^e\).

It follows from Theorem~6.8.1(3) that
\[D^e\after D_e=\sum_p(D^{n-p}\after D_p)=\sum_p(-1)^{p(n-p)}\iota=\omega_E\]
Since \(\omega_E^2=\iota\), this implies
\[D_e^{-1}=\omega_E\after D^e=(D^e)^*\]
Similarly
\[(D^e)^{-1}=\omega_{E^*}\after D_e=D_e^*\]
\end{rmk}

\begin{rmk}
Let \(\Delta,\Delta^*\ne 0\) be determinant functions in \(E,E^*\) and let \(e^*,e\) be the corresponding (not necessarily dual) vectors in \(\medeprod^n E^*,\medeprod^n E\) with \(\alpha=\sprod{e^*}{e}\ne 0\). Then
\[\Delta^*(x^*_1,\ldots,x^*_n)=\sprod{\eprods{x^*}{1}{n}}{e}=i(\eprods{x^*}{1}{n})e=D^e(\eprods{x^*}{1}{n})\]
and
\[\Delta(x_1,\ldots,x_n)=\sprod{e^*}{\eprods{x}{1}{n}}=i(\eprods{x}{1}{n})e^*=\alpha D_e(\eprods{x}{1}{n})\]
By Theorem~6.8.1(1),
\begin{align*}
\Delta^*(x^*_1,\ldots,x^*_n)\,\Delta(x_1,\ldots,x_n)&=\alpha\sprod{D_e(\eprods{x}{1}{n})}{D^e(\eprods{x^*}{1}{n})}\\
	&=\alpha\sprod{\eprods{x^*}{1}{n}}{\eprods{x}{1}{n}}\\
	&=\alpha\det\sprod{x^*_j}{x_i}
\end{align*}
This is~(4.21) in~\cite{greub1}, which was proved there using the equivalent of the universal property of~\(\medeprod^n_n(E^*,E)\). This shows that \(\Delta,\Delta^*\) are dual if and only if \(e^*,e\) are dual.
\end{rmk}

\subsection*{\S~9}
\begin{rmk}
Let \(\cat{V}\)~denote the category of vector spaces and linear maps over~\(\Gamma\). Let \(\cat{C}\)~denote the category of objects \((E,e)\) where \(E\)~is an \(n\)-dimensional vector space over~\(\Gamma\) and \(e\)~is a basis vector of~\(\medeprod^n E\), and arrows \(\varphi:(E,e)\to(F,f)\) where \(\varphi:E\to F\) is a linear isomorphism with \(\varphi_{\eprod}e=f\).

Define \(F:\cat{C}\to\cat{V}\) by
\[F(E,e)=\medeprod E\qquad\text{and}\qquad F(\varphi)=\varphi_{\eprod}\]
and define \(G:\cat{C}\to\cat{V}\) by
\[G(E,e)=\medeprod L(E)\qquad\text{and}\qquad G(\varphi)=(\varphi^{\eprod})^{-1}\]
Then \(F\) and~\(G\) are functors, and the diagrams in this section show that the maps \(D_{(-)}:F\to G\) and \(D^{(-)}:G\to F\) are natural transformations.
\end{rmk}

\subsection*{\S~10}
\begin{rmk}
By~(6.20),
\[D_E(u^*\tprod u)=D_e u\tprod D^e u^*=Q_{E^*}(D^e u^*\tprod D_e u)=\bigl[Q_{E^*}\after(D^e\tprod D_e)\bigr](u^*\tprod u)\]
so it follows that
\[D_E=Q_{E^*}\after(D^e\tprod D_e)\]
\end{rmk}

\begin{rmk}
For the proof of Theorem~6.10.1(2), we have by~(6.21) and results above
\begin{align*}
D_E^*&=Q_E^*\after(D_e\tprod D^e)^*\\
	&=Q_{E^*}\after\bigl[(\omega_E\after D^e)\tprod(\omega_{E^*}\after D_e)\bigr]\\
	&=Q_{E^*}\after(\omega_E\tprod\omega_{E^*})\after(D^e\tprod D_e)\\
	&=(\omega_{E^*}\tprod\omega_E)\after Q_{E^*}\after(D^e\tprod D_e)\\
	&=\Omega_E\after D_E
\end{align*}
\end{rmk}

\begin{rmk}
It follows from Theorem~6.10.1 that \(D_E^*=D_E^{-1}\). We have
\[D_E^{-1}=[(D^e)^{-1}\tprod(D_e)^{-1}]\after Q_E=Q_{E^*}\after[(D_e)^{-1}\tprod(D^e)^{-1}]\]
\end{rmk}

\begin{rmk}
The operator~\(D_E\) restricts to operators
\[D_E^{p,q}:\medeprod^p_q(E^*,E)\to\medeprod^{n-q}_{n-p}(E^*,E)\]
In particular \(D_E^{p,p}:\Delta_p E\to\Delta_{n-p}E\) and \(D_{\Delta}=\sum_p D_E^{p,p}\). The operators are linear isomorphisms and the scalar product, dual, and composition properties follow from Theorem~6.10.1.
\end{rmk}

\begin{rmk}
For \(w,z\in\medeprod(E^*,E)\),
\[D_E(w\after z)=D_Ez\after D_Ew\]
\end{rmk}
\begin{proof}
We may assume that \(w=u^*\tprod u\) and \(z=v^*\tprod v\), so by~(6.7)
\[w\after z=\sprod{u^*}{v}v^*\tprod u\]
and therefore by~(6.20) and Theorem~6.8.1(1),
\begin{align*}
D_E(w\after z)&=\sprod{u^*}{v}D_E(v^*\tprod u)\\
	&=\sprod{D_e v}{D^e u^*}D_e u\tprod D^e v^*\\
	&=(D_e v\tprod D^e v^*)\after(D_e u\tprod D^e u^*)\\
	&=D_E(v^*\tprod v)\after D_E(u^*\tprod u)\\
	&=D_Ez\after D_Ew\qedhere
\end{align*}
\end{proof}

\begin{rmk}
Interchanging the roles of \(E\) and~\(E^*\), we obtain the map
\[D_{E^*}:\medeprod(E,E^*)\to\medeprod(E,E^*)\]
satisfying
\[D_{E^*}=(D^e\tprod D_e)\after Q_{E^*}=Q_E\after(D_e\tprod D^e)\]
\end{rmk}

\subsection*{\S~11}
\begin{rmk}
Let \(\cat{C}\)~denote the category of finite-dimensional vector spaces over~\(\Gamma\) and the linear isomorphisms between them. For an object \(E\in\cat{C}\), define
\[\alpha(E)=\medeprod(L(E),E)\]
and for an arrow \(\varphi:E\to F\) in~\(\cat{C}\), define
\[\alpha(\varphi)=\alpha_{\varphi}=(\varphi^{\eprod})^{-1}\tprod\varphi_{\eprod}:\alpha(E)\to\alpha(F)\]
Then \(\alpha:\cat{C}\to\cat{C}\) is a functor. Indeed, \(\alpha\)~maps objects to objects and arrows to arrows, respecting domains and codomains and identity arrows. If \(\psi:F\to G\) is another arrow in~\(\cat{C}\), then
\begin{align*}
\alpha(\psi\after\varphi)&=((\psi\after\varphi)^{\eprod})^{-1}\tprod(\psi\after\varphi)_{\eprod}\\
	&=((\psi^{\eprod})^{-1}\after(\varphi^{\eprod})^{-1})\tprod(\psi_{\eprod}\after\varphi_{\eprod})\\
	&=((\psi^{\eprod})^{-1}\tprod\psi_{\eprod})\after((\varphi^{\eprod})^{-1}\tprod\varphi_{\eprod})\\
	&=\alpha(\psi)\after\alpha(\varphi)
\end{align*}
so \(\alpha\)~also respects composites.

The diagram in this section shows that \(D_E\)~is a natural isomorphism of the functor~\(\alpha\):
\begin{diagram}
\alpha(E)			&\rTo^{\alpha_{\varphi}}_{\iso}	&\alpha(F)\\
\dTo<{D_E}>{\iso}	&								&\dTo>{D_F}<{\iso}\\
\alpha(E)			&\rTo_{\alpha_{\varphi}}^{\iso}	&\alpha(F)
\end{diagram}
To see that the diagram actually commutes, let \(e\) and~\(e^*\) be dual basis vectors of \(\medeprod^n E\) and~\(\medeprod^n E^*\) and let \(f=\varphi_{\eprod}e\) and \(f^*=(\varphi^{\eprod})^{-1}e^*\) be the corresponding dual basis vectors of \(\medeprod^n F\) and \(\medeprod^n F^*\). Then
\begin{align*}
D_F\after\alpha_{\varphi}&=(D_f\tprod D^f)\after Q_F\after((\varphi^{\eprod})^{-1}\tprod\varphi_{\eprod})\\
	&=(D_f\tprod D^f)\after(\varphi_{\eprod}\tprod(\varphi^{\eprod})^{-1})\after Q_E\\
	&=\bigl[(D_f\after\varphi_{\eprod})\tprod(D^f\after(\varphi^{\eprod})^{-1})\bigr]\after Q_E\\
	&=\bigl[((\varphi^{\eprod})^{-1}\after D_e)\tprod(\varphi_{\eprod}\after D^e)\bigr]\after Q_E\\
	&=((\varphi^{\eprod})^{-1}\tprod\varphi_{\eprod})\after(D_e\tprod D^e)\after Q_E\\
	&=\alpha_{\varphi}\after D_E
\end{align*}
\end{rmk}

\subsection*{\S~12}
% TODO: geometric interpretation of intersection product

\begin{rmk}
The intersection product is associative since the wedge product is: if \(u,v,w\in\medeprod E\), then writing \(D=D^e\) we have
\begin{align*}
(u\sect v)\sect w&=D[D^{-1}(u\sect v)\eprod D^{-1}w]\\
	&=D[(D^{-1}u\eprod D^{-1}v)\eprod D^{-1}w]\\
	&=D[D^{-1}u\eprod(D^{-1}v\eprod D^{-1}w)]\\
	&=D[D^{-1}u\eprod D^{-1}(v\sect w)]\\
	&=u\sect(v\sect w)
\end{align*}
Therefore we may simply write \(u\sect v\sect w\).
\end{rmk}

\begin{rmk}
We have
\[u\sect v=i[(D^e)^{-1}v]u=i(D_e^* v)u\]
In particular if \(u\in\medeprod^p E\) and \(v\in\medeprod^{n-p}E\), then \(u\sect v\)~is a scalar and
\begin{align*}
u\sect v&=\sprod{u\sect v}{1}\\
	&=\sprod{i(D_e^* v)u}{1}\\
	&=\sprod{u}{D_e^* v}\\
	&=\sprod{D_e u}{v}\\
	&=\sprod{i(u)e^*}{v}\\
	&=\sprod{e^*}{u\eprod v}
\end{align*}
\end{rmk}

\begin{rmk}
Let \(u^*,v^*\in\medeprod E^*\). Taking \(u=D^e u^*\) and \(v=D^e v^*\) in the definition of the intersection product for~\(\medeprod E\), we obtain
\[D^e(u^*\eprod v^*)=D^e u^*\sect D^e v^*\]
so \(D^e\)~is an algebra isomorphism from the exterior algebra to the intersection algebra. Applying \(D_e^*=(D^e)^{-1}\) to both sides of the definition, we obtain
\[D_e^*(u\sect v)=D_e^* u\eprod D_e^* v\]
so \(D_e^*\)~is an algebra isomorphism from the intersection algebra to the exterior algebra. This all strongly suggests that \(D_e\)~should be an algebra isomorphism, which a computation involving sign factors verifies.
\end{rmk}

\begin{rmk}
The intersection product in~\(\medeprod E^*\) is given by
\[u^*\sect v^*=D_e[(D_e)^{-1}u^*\eprod(D_e)^{-1}v^*]\qquad(u^*,v^*\in\medeprod E^*)\]
\end{rmk}

\begin{rmk}
For \(w,z\in\medeprod(E^*,E)\), we define the \emph{mixed intersection product} by
\[w\sect z=D_E(D_E^{-1}w\mprod D_E^{-1}z)\]
If \(w\in\medeprod^p_q(E^*,E)\) and \(z\in\medeprod^r_s(E^*,E)\), then \(w\sect z\in\medeprod^{p+r-n}_{q+s-n}(E^*,E)\).

This definition makes \(\medeprod(E^*,E)\) into an algebra called the \emph{mixed intersection algebra} which is associative, unital (with unit \(\unit_n\)), and anticommutative. It is immediate from the definition that
\[D_E(w\mprod z)=D_Ew\sect D_Ez\]
so \(D_E\)~is an \emph{algebra} isomorphism from the mixed exterior algebra to the mixed intersection algebra.

For \(u,v\in\medeprod E\) and \(u^*,v^*\in\medeprod E^*\), we have
\begin{align*}
(u^*\tprod u)\sect(v^*\tprod v)&=D_E\bigl[\bigl((D^e)^{-1}u\tprod(D_e)^{-1} u^*\bigr)\mprod\bigl((D^e)^{-1}v\tprod(D_e)^{-1}v^*\bigr)\bigr]\\
	&=D_E\bigl[\bigl((D^e)^{-1}u\eprod(D^e)^{-1}v\bigr)\tprod\bigl((D_e)^{-1}u^*\eprod(D_e)^{-1}v^*\bigr)\bigr]\\
	&=D_e[(D_e)^{-1}u^*\eprod(D_e)^{-1}v^*]\tprod D^e[(D^e)^{-1}u\eprod(D^e)^{-1}v]\\
	&=(u^*\sect v^*)\tprod(u\sect v)
\end{align*}
It follows that
\begin{align*}
D_E[(u^*\tprod u)\sect(v^*\tprod v)]&=D_e(u\sect v)\tprod D^e(u^*\sect v^*)\\
	&=(D_eu\eprod D_ev)\tprod(D^eu^*\eprod D^ev^*)\\
	&=(D_eu\tprod D^e u^*)\mprod(D_ev\tprod D^ev^*)\\
	&=D_E(u^*\tprod u)\mprod D_E(v^*\tprod v)
\end{align*}
By multilinearity, it follows that
\[D_E(w\sect z)=D_Ew\mprod D_Ez\]
for all \(w,z\in\medeprod(E^*,E)\), so \(D_E\)~is also an algebra isomorphism from the mixed intersection algebra to the mixed exterior algebra.
\end{rmk}

\subsection*{\S~13}
\begin{rmk}
Let \(e_i,e^{*i}\) be a pair of dual bases in \(E,E^*\) with \(\eprods{e}{1}{n}=e\). Write
\[e_{\mu_1\cdots\mu_q}=\eprods{e}{\mu_1}{\mu_q}\qquad\text{and}\qquad e^{\nu_1\cdots\nu_p}=e^{*\nu_1}\eprod\cdots\eprod e^{*\nu_p}\]
for ordered tuples \(1\le\mu_1<\cdots<\mu_q\le n\) and \(1\le\nu_1<\cdots<\nu_p\le n\). Then
\[e_{\mu_1\cdots\mu_q}^{\nu_1\cdots\nu_p}=e^{\nu_1\cdots\nu_p}\tprod e_{\mu_1\cdots\mu_q}\]
form a basis of~\(\medeprod(E^*,E)\) and for \(z\in\medeprod(E^*,E)\) we write
\[z=\sum_{<}(z)^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}e_{\mu_1\cdots\mu_q}^{\nu_1\cdots\nu_p}\]
The components \((z)^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}\) are also the matrix entries of~\(T_Ez\) with respect to the lexically ordered basis \(e_{\nu_1\cdots\nu_p}\) of~\(\medeprod E\).\footnote{See the remark in chapter~1, \S~28 above.} In particular if \(z\in E^*\tprod E\), then \((z^p)^{\mu_1\cdots\mu_p}_{\nu_1\cdots\nu_p}\) is the \((\nu,\mu)\) entry of the matrix of~\(\medeprod^p T_Ez\), which is the \(p\times p\) minor determinant of the matrix of~\(T_Ez\) with rows \(\nu_1,\ldots,\nu_p\) and columns \(\mu_1,\ldots,\mu_p\).\footnote{See the remark in chapter~5, \S~8 above.}

By (6.20) and~(6.23),
\begin{align*}
D_E(e_{\mu_1\cdots\mu_q}^{\nu_1\cdots\nu_p})&=D_e(e_{\mu_1\cdots\mu_q})\tprod D^e(e^{\nu_1\cdots\nu_p})\\
	&=\bigl[(-1)^{\sum_{j=1}^q(\mu_j-j)}e^{\mu_{q+1}\cdots\mu_n}\bigr]\tprod\bigl[(-1)^{\sum_{i=1}^p(\nu_i-i)}e_{\nu_{p+1}\cdots\nu_n}\bigr]\\
	&=(-1)^{\sum_{i=1}^p(\nu_i-i)+\sum_{j=1}^q(\mu_j-j)}e_{\nu_{p+1}\cdots\nu_n}^{\mu_{q+1}\cdots\mu_n}
\end{align*}
For \(z\in\medeprod(E^*,E)\), it follows that
\begin{align*}
D_E z&=\sum_{<}(z)^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}D_E(e_{\mu_1\cdots\mu_q}^{\nu_1\cdots\nu_p})\\
	&=\sum_{<}(-1)^{\sum_{i=1}^p(\nu_i-i)+\sum_{j=1}^q(\mu_j-j)}(z)^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}e_{\nu_{p+1}\cdots\nu_n}^{\mu_{q+1}\cdots\mu_n}
\end{align*}
so\footnote{This is problem~6.15.10.}
\[(D_E z)^{\nu_{p+1}\cdots\nu_n}_{\mu_{q+1}\cdots\mu_n}=(-1)^{\sum_{i=1}^p(\nu_i-i)+\sum_{j=1}^q(\mu_j-j)}(z)^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}\]
In particular if \(z\in E^*\tprod E\), then \((D_E z^{n-p})^{\nu_1\cdots\nu_p}_{\mu_1\cdots\mu_p}\) is the signed \((n-p)\times(n-p)\) minor determinant of the matrix of~\(T_Ez\) with rows complementary to \(\nu_1,\ldots,\nu_p\) and columns complementary to \(\mu_1,\ldots,\mu_p\).
\end{rmk}

\begin{rmk}
Every \(u\in\medeprod^{n-1}E\) is decomposable.
\end{rmk}
\begin{proof}
If \(u=0\), the result is trivial, otherwise \(v^*_1=D_e u\) can be extended to a basis \(v^*_1,\ldots,v^*_n\) in~\(E^*\) with dual basis \(v_1,\ldots,v_n\) in~\(E\). Let \(v=\eprods{v}{1}{n}\). Then \(v=\lambda e\) for some \(\lambda\ne 0\) and \(D^v=\lambda D^e\). Now
\[u=\pm D^e(D_e u)=\pm\lambda^{-1}D^v v^*_1=\pm\lambda^{-1}\eprods{v}{2}{n}\qedhere\]
\end{proof}

\subsection*{\S~14}
\begin{rmk}
The external product on~\(E\) makes the following diagram commute:
\begin{diagram}[nohug]
E^{n-1}					&\rTo^{[-]}		&E^*\\
\dTo<{\medeprod^{n-1}}	&\ruTo>{D_{n-1}}&\\
\medeprod^{n-1}E		&				&
\end{diagram}
\end{rmk}

\begin{rmk}
We define the \emph{mixed external product} \(M^{n-1}_{n-1}(E^*,E)\to E^*\tprod E\) by
\[[x^*_1,\ldots,x^*_{n-1},x_1,\ldots,x_{n-1}]=D_E[(x^*_1\tprod x_1)\mprod\cdots\mprod(x^*_{n-1}\tprod x_{n-1})]\]
This product is skew-symmetric of type \((n-1,n-1)\). We have
\begin{align*}
[x^*_1,\ldots,x^*_{n-1},x_1,\ldots,x_{n-1}]&=D_E[(\eprods{x^*}{1}{n-1})\tprod(\eprods{x}{1}{n-1})]\\
	&=D_e(\eprods{x}{1}{n-1})\tprod D^e(\eprods{x^*}{1}{n-1})\\
	&=[x_1,\ldots,x_{n-1}]\tprod[x^*_1,\ldots,x^*_{n-1}]
\end{align*}
It follows that
\[\sprod{[x^*_1,\ldots,x^*_{n-1},x_1,\ldots,x_{n-1}]}{[x^*_1,\ldots,x^*_{n-1},x_1,\ldots,x_{n-1}]}=\bigl(\det\sprod{x^*_j}{x_i}\bigr)^2\]
\end{rmk}

\subsection*{\S~15}
\begin{rmk}
In the case of a finite-dimensional oriented Euclidean space, Poincar\'e duality reduces to Hodge duality and the Poincar\'e map is just the Hodge star.
\end{rmk}

\begin{exer}[1]
For an isomorphism \(\varphi:E\to E\),
\[\alpha_{\varphi}\unit^p=\unit^p\qquad(0\le p\le n)\]
where \(\alpha_{\varphi}=(\varphi^{\eprod})^{-1}\tprod\varphi_{\eprod}\).
\end{exer}
\begin{proof}
Let \(e_i,e^{*i}\) be a pair of dual bases in \(E,E^*\). Then
\[\unit^p=\sum_{<}e^{\nu_1\cdots\nu_p}_{\nu_1\cdots\nu_p}\]
If \(\bar{e}_i=\varphi e_i\) and \(\bar{e}^{*i}=(\varphi^*)^{-1}e^{*i}\), then \(\bar{e}_i,\bar{e}^{*i}\) is another pair of dual bases in \(E,E^*\) and
\[\alpha_{\varphi}\unit^p=\sum_{<}\alpha_{\varphi}(e^{\nu_1\cdots\nu_p}_{\nu_1\cdots\nu_p})=\sum_{<}\bar{e}^{\nu_1\cdots\nu_p}_{\nu_1\cdots\nu_p}=\unit^p\qedhere\]
\end{proof}
\begin{rmk}
This result shows that unit tensors in the mixed exterior algebra are \emph{invariant} in that their descriptions do not depend upon the choice of basis.
\end{rmk}

\begin{exer}[3]
\begin{align*}
D_e i(v^*)u&=(-1)^{q(p-q)}v^*\eprod D_eu&&u\in\medeprod^p E,v^*\in\medeprod^q E^*\\
D^e i(v)u^*&=(-1)^{p(q-p)}v\eprod D^e u^*&&v\in\medeprod^p E,u^*\in\medeprod^q E^*\\
D_E i(z)w&=(-1)^{p(s-p)+q(r-q)}z\mprod D_Ew&&z\in\medeprod^p_q(E^*,E),w\in\medeprod^r_s(E^*,E)
\end{align*}
\end{exer}
\begin{proof}
For any \(w^*\in\medeprod^{p-q}E^*\),
\begin{align*}
\sprod{D_e i(v^*)u}{D^e w^*}&=\sprod{w^*}{i(v^*)u}\\
	&=\sprod{v^*\eprod w^*}{u}\\
	&=\sprod{D_e u}{D^e(v^*\eprod w^*)}\\
	&=(-1)^{q(p-q)}\sprod{D_e u}{D^e(w^*\eprod v^*)}\\
	&=(-1)^{q(p-q)}\sprod{D_e u}{i(v^*)D^e w^*}\\
	&=\sprod{(-1)^{q(p-q)}v^*\eprod D_e u}{D^e w^*}
\end{align*}
The first formula follows since \(D^e\)~is an isomorphism, and the second formula follows from the first by symmetry of duality. Finally if \(z=v^*\tprod v\) and \(w=u^*\tprod u\), then
\begin{align*}
D_E i(z)w&=D_E[i(v)u^*\tprod i(v^*)u]\\
	&=D_e i(v^*)u\tprod D^e i(v)u^*\\
	&=(-1)^{p(s-p)+q(r-q)}(v^*\eprod D_e u)\tprod(v\eprod D^e u^*)\\
	&=(-1)^{p(s-p)+q(r-q)}(v^*\tprod v)\mprod(D_e u\tprod D^e u^*)\\
	&=(-1)^{p(s-p)+q(r-q)}z\mprod D_E w
\end{align*}
The third formula follows by multilinearity.
\end{proof}
\begin{rmk}
This result shows that multiplication is the dual of insertion under Poincar\'e duality.
\end{rmk}

\begin{exer}[4]
\[D_{\Delta}\mu(z)=i(z)D_{\Delta}\qquad(z\in\Delta E)\]
\end{exer}
\begin{proof}
For \(z=v^*\tprod v\) and \(w=u^*\tprod u\) in~\(\Delta E\),
\begin{align*}
D_{\Delta}\mu(z)w&=D_{\Delta}(z\mprod w)\\
	&=D_{\Delta}(w\mprod z)\\
	&=D_{\Delta}[(u^*\eprod v^*)\tprod(u\eprod v)]\\
	&=D_e(u\eprod v)\tprod D^e(u^*\eprod v^*)\\
	&=[i(v)D_e u]\tprod[i(v^*)D^e u^*]\\
	&=i(v^*\tprod v)(D_e u\tprod D^e u^*)\\
	&=i(z)D_{\Delta}w
\end{align*}
The formula follows by multilinearity.
\end{proof}
\begin{rmk}
This result shows that insertion is the dual of multiplication under Poincar\'e duality in the diagonal subalgebra.
\end{rmk}

\begin{exer}[7]
Let \(E\)~be an oriented \(3\)-dimensional Euclidean space. Then
\[x\cross y=D_e(x\eprod y)=[x,y]\qquad(x,y\in E)\]
It follows that
\[\iprod{x_1\cross y_1}{x_2\cross y_2}=\iprod{x_1}{x_2}\iprod{y_1}{y_2}-\iprod{x_1}{y_2}\iprod{x_2}{y_1}\]
and
\[(x\cross y)\cross z=\iprod{x}{z}y-\iprod{y}{z}x\]
\end{exer}
\begin{proof}
Let \(\Delta\)~be the normed determinant function in~\(E\) which represents the orientation. Then for any \(z\in E\),
\[\iprod{D_e(x\eprod y)}{z}=\iprod{[x,y]}{z}=\iprod{e}{x\eprod y\eprod z}=\Delta(x,y,z)=\iprod{x\cross y}{z}\]
so the first equation follows by definiteness of the inner product. Now by the Lagrange identity,
\begin{align*}
\iprod{x_1\cross y_1}{x_2\cross y_2}&=\iprod{[x_1,y_1]}{[x_2,y_2]}\\
	&=\begin{vmatrix}\iprod{x_1}{x_2}&\iprod{x_1}{y_2}\\\iprod{y_1}{x_2}&\iprod{y_1}{y_2}\end{vmatrix}\\
	&=\iprod{x_1}{x_2}\iprod{y_1}{y_2}-\iprod{x_1}{y_2}\iprod{x_2}{y_1}
\end{align*}
Finally,
\begin{align*}
(x\cross y)\cross z&=D_e[D_e(x\eprod y)\eprod z]\\
	&=i(z)D_e[D_e(x\eprod y)]\\
	&=i(z)(x\eprod y)\\
	&=\iprod{x}{z}y-\iprod{y}{z}x\qedhere
\end{align*}
\end{proof}
\begin{rmk}
This result shows that the cross product is dual to the wedge product under Poincar\'e duality, and is just a special case of the external product.
\end{rmk}

\begin{exer}[8]
Let \(e_i,e^{*i}\) be a pair of dual bases in \(E,E^*\) with \(e_{1\cdots n}=e\). Then
\[D_e(\eprods{x}{1}{p})=\sum_{\nu_1<\cdots<\nu_p}(-1)^{\sum_{i=1}^p(\nu_i-i)}\det\sprod{e^{*\nu_j}}{x_i}e^{\nu_{p+1}\cdots\nu_n}\]
where \((\nu_1,\ldots,\nu_p)\) and \((\nu_{p+1},\ldots,\nu_n)\) are complementary ordered tuples.
\end{exer}
\begin{proof}
We know
\[\eprods{x}{1}{p}=\sum_{<}\sprod{e^{\nu_1\cdots\nu_p}}{\eprods{x}{1}{p}}e_{\nu_1\cdots\nu_p}=\sum_{<}\det\sprod{e^{*\nu_j}}{x_i}e_{\nu_1\cdots\nu_p}\]
so
\begin{align*}
D_e(\eprods{x}{1}{p})&=\sum_{<}\det\sprod{e^{*\nu_j}}{x_i}D_e(e_{\nu_1\cdots\nu_p})\\
	&=\sum_{<}(-1)^{\sum_{i=1}^p(\nu_i-i)}\det\sprod{e^{*\nu_j}}{x_i}e^{\nu_{p+1}\cdots\nu_n}\qedhere
\end{align*}
\end{proof}
\begin{rmk}
This result generalizes~(6.23).
\end{rmk}

\begin{exer}[11]
Let \(E,E^*\) be a pair of dual \(3\)-dimensional vector spaces with \(a\in E\) and \(b^*\in E^*\). If \(a\ne 0\), then there is \(x\in E\) with \([a,x]=b^*\) if and only if \(\sprod{b^*}{a}=0\). If \(x_0\in E\) is a particular solution, then the general solution is given by \(x=x_0+\lambda a\) for \(\lambda\in\Gamma\).
\end{exer}
\begin{proof}
If \([a,x]=b^*\), then
\[\sprod{b^*}{a}=\sprod{[a,x]}{a}=\sprod{e^*}{a\eprod x\eprod a}=0\]
Conversely if \(\sprod{b^*}{a}=0\) and \(a^*\in E^*\) is chosen with \(\sprod{a^*}{a}=-1\), then the vector \(x=[a^*,b^*]\) satisfies
\begin{align*}
[a,x]&=D_e(a\eprod D^e(a^*\eprod b^*))\\
	&=-i(a)(D_e(D^e(a^*\eprod b^*)))\\
	&=-i(a)(a^*\eprod b^*)\\
	&=\sprod{b^*}{a}a^*-\sprod{a^*}{a}b^*\\
	&=b^*
\end{align*}
If \([a,x_0]=b^*\), then
\[[a,x_0+\lambda a]=[a,x_0]+\lambda[a,a]=b^*\]
If also \([a,x]=b^*\), then
\[[a,x-x_0]=[a,x]-[a,x_0]=b^*-b^*=0\]
so \(a\eprod(x-x_0)=0\), which implies \(x-x_0=\lambda a\) for some \(\lambda\in\Gamma\).\footnote{See the corollary to Proposition~5.26.1.}
\end{proof}
\begin{rmk}
If \(E\)~is an oriented \(3\)-dimensional Euclidean space, this result shows that for \(a\ne 0\) there is \(x\) with \(a\cross x=b\) if and only if \(\iprod{a}{b}=0\).
\end{rmk}

\begin{exer}[12]
If \(\varphi:E^{n-1}\to F\) is an \((n-1)\)-linear skew-symmetric map, then there is a unique linear map \(\chi:E^*\to F\) with
\[\varphi(x_1,\ldots,x_{n-1})=\chi[x_1,\ldots,x_{n-1}]\qquad(x_i\in E)\]
\end{exer}
\begin{proof}
If there is such a~\(\chi\), then by definition of the external product
\[\chi D_{n-1}(\eprods{x}{1}{n-1})=\varphi(x_1,\ldots,x_{n-1})\qquad(x_i\in E)\]
so \(f=\chi\after D_{n-1}:\medeprod^{n-1}E\to F\) is the linear map induced by~\(\varphi\) on~\(\medeprod^{n-1} E\):
\begin{diagram}[nohug]
E^{n-1}					&\rTo^{\varphi}	&F\\
\dTo<{\medeprod^{n-1}}	&\ruDashto>f	&\\
\medeprod^{n-1}E		&				&
\end{diagram}
It follows that \(\chi=f\after D_{n-1}^{-1}\), so \(\chi\)~is uniquely determined. On the other hand, this map satisfies the property.
\end{proof}
\begin{rmk}
This is the universal property of the external product, which is dual to the universal property of the wedge product under Poincar\'e duality.
\end{rmk}

\begin{exer}[13]
If \(\varphi:E\to E\) is an isomorphism, then
\[[\varphi x_1,\ldots,\varphi x_{n-1}]=\det\varphi\mult(\varphi^{-1})^*[x_1,\ldots,x_{n-1}]\qquad(x_i\in E)\]
\end{exer}
\begin{proof}
For any \(x_n\in E\),
\begin{align*}
\sprod{[\varphi x_1,\ldots,\varphi x_{n-1}]}{\varphi x_n}&=\sprod{e^*}{\eprods{\varphi x}{1}{n}}\\
	&=\det\varphi\mult\sprod{e^*}{\eprods{x}{1}{n}}\\
	&=\det\varphi\mult\sprod{[x_1,\ldots,x_{n-1}]}{\varphi^{-1}\varphi x_n}\\
	&=\sprod{\det\varphi\mult(\varphi^{-1})^*[x_1,\ldots,x_{n-1}]}{\varphi x_n}
\end{align*}
The result follows by nondegeneracy of the scalar product.
\end{proof}
\begin{rmk}
Substituting \((\varphi^{-1})^*\) for~\(\varphi\), we obtain
\[\varphi[x^*_1,\ldots,x^*_{n-1}]=\det\varphi\mult[(\varphi^{-1})^*x^*_1,\ldots,(\varphi^{-1})^*x^*_{n-1}]\qquad(x^*_i\in E^*)\]
In particular if \(E\)~is an oriented \(3\)-dimensional Euclidean space, we have\footnote{See the remarks in chapter~VIII, \S~4 of \cite{greub1} above.}
\[\varphi x\cross\varphi y=\det\varphi\mult\adj{\varphi}^{-1}(x\cross y)\qquad\text{and}\qquad\varphi(x\cross y)=\det\varphi\mult(\adj{\varphi}^{-1}x\cross\adj{\varphi}^{-1}y)\]
\end{rmk}

\begin{exer}[14]
Let \(e_i,e^{*i}\) be a pair of dual bases in \(E,E^*\) with \(e_{1\cdots n}=e\) and
\[x_i=\sum_{\nu=1}^n\xi_i^{\nu}e_{\nu}\qquad(1\le i\le n-1,\ \xi_i^{\nu}\in\Gamma)\]
Then
\[[x_1,\ldots,x_{n-1}]=\sum_{\nu=1}^n\eta_{\nu}e^{*\nu}\]
where
\[\setlength\arraycolsep{1pt}
\eta_{\nu}=(-1)^{n-\nu}\begin{vmatrix}
\xi_1^1&\cdots&\delete{\xi}_1^{\nu}&\cdots&\xi_1^n\\
\vdots&&&&\vdots\\
\xi_{n-1}^1&\cdots&\delete{\xi}_{n-1}^{\nu}&\cdots&\xi_{n-1}^n
\end{vmatrix}\]
\end{exer}
\begin{proof}
By the corrected version of problem~8 above.
\end{proof}

\begin{exer}[15 - Lagrange]
\[\setlength\arraycolsep{1pt}
\begin{vmatrix}
\displaystyle\sum_{\nu=1}^n\xi_1^{\nu}\eta^1_{\nu}&\cdots&\displaystyle\sum_{\nu=1}^n\xi_1^{\nu}\eta^{n-1}_{\nu}\\
\vdots&\ddots&\vdots\\
\displaystyle\sum_{\nu=1}^n\xi_{n-1}^{\nu}\eta^1_{\nu}&\cdots&\displaystyle\sum_{\nu=1}^n\xi_{n-1}^{\nu}\eta^{n-1}_{\nu}
\end{vmatrix}=\sum_{\nu=1}^n\begin{vmatrix}
\xi_1^1&\cdots&\delete{\xi}_1^{\nu}&\cdots&\xi_1^n\\
\vdots&&&&\vdots\\
\xi_{n-1}^1&\cdots&\delete{\xi}_{n-1}^{\nu}&\cdots&\xi_{n-1}^n
\end{vmatrix}\begin{vmatrix}
\eta_1^1&\cdots&\delete{\eta}_1^{\nu}&\cdots&\eta_1^n\\
\vdots&&&&\vdots\\
\eta_{n-1}^1&\cdots&\delete{\eta}_{n-1}^{\nu}&\cdots&\eta_{n-1}^n
\end{vmatrix}
\]
\end{exer}
\begin{proof}
Let \(e_i,e^{*i}\) be a pair of dual bases in \(E,E^*\) with \(e_{1\cdots n}=e\) and
\[x_i=\sum_{\nu=1}^n\xi_i^{\nu}e_{\nu}\qquad x^{*j}=\sum_{\mu=1}^n\eta^j_{\mu}e^{*\mu}\qquad(1\le i,j\le n-1)\]
By the Lagrange identity, the left-hand side of the equation is
\[\det\sprod{x^{*j}}{x_i}=\sprod{[x_1,\ldots,x_{n-1}]}{[x^{*1},\ldots,x^{*n-1}]}\]
which equals the right-hand side of the equation by problem~14.
\end{proof}

\subsection*{\S~16}
\begin{rmk}
The material in this subsection really belongs in chapter~1, as none of it is specific to the mixed exterior algebra. Some of it is also redundant, as noted below.
\end{rmk}

\begin{rmk}
Let \(T_1:E^*\tprod E\to L(E;E)\) denote the isomorphism from subsection~1.26 (using the notation from subsection~6.4) given by
\[T_1(b^*\tprod a)=\sprod{b^*}{-}a\qquad(a\in E,\ b^*\in E^*)\]
Let \(e_i,e^{*i}\) be a pair of dual bases in \(E,E^*\). Then
\begin{align*}
T(T_1(b^*\tprod a))&=\bigl(\iota\tprod T_1(b^*\tprod a)\bigr)\bigl(\,\sum_{\nu}e^{*\nu}\tprod e_{\nu}\bigr)\\
	&=\sum_{\nu}e^{*\nu}\tprod\sprod{b^*}{e_{\nu}}a\\
	&=\bigl(\,\sum_{\nu}\sprod{b^*}{e_{\nu}}e^{*\nu}\bigr)\tprod a\\
	&=b^*\tprod a
\end{align*}
It follows that \(T=T_1^{-1}=T_1^*\).
\end{rmk}

\begin{rmk}
(6.28)~follows from~(1.30), or from~(6.16) with \(p=1\), since \(T=T_1^{-1}\).
\end{rmk}

\begin{rmk}
\(T\)~is an \emph{algebra} isomorphism since \(T_1\)~is, so
\[T(\psi\varphi)=T(\psi)T(\varphi)=(\iota\tprod\psi)T(\varphi)\]
This it the correct version of~(6.30).
\end{rmk}

\begin{rmk}
We know that \(T\) and~\(\widetilde{T}\) are isometries since \(T_1\)~is. It is thus unnecessary to prove that they are linear isomorphisms with the aid of (6.28) and~(6.30), and also circular given how nondegeneracy of the trace form is established in subsection~1.28!
\end{rmk}

\subsection*{\S~17}
\begin{rmk}
(6.31)~follows from the fact that the skew tensor product of two skew graded algebras is skew.\footnote{See subsection~2.8.} In detail, if
\[w_1=u^*\tprod u\in\medeprod^p_q(E^*,E)\qquad\text{and}\qquad w_2=v^*\tprod v\in\medeprod^r_s(E^*,E)\]
then
\begin{align*}
w_1\eprod w_2&=(u^*\tprod u)\eprod(v^*\tprod v)\\
	&=(-1)^{qr}(u^*\eprod v^*)\tprod(u\eprod v)\\
	&=(-1)^{qr+pr+qs}(v^*\eprod u^*)\tprod(v\eprod u)\\
	&=(-1)^{qr+pr+qs+ps}(v^*\tprod v)\eprod(u^*\tprod u)\\
	&=(-1)^{(p+q)(r+s)}w_2\eprod w_1
\end{align*}
\end{rmk}

\begin{rmk}
It follows from~(6.33) and Proposition~6.3.1 that
\[T_E(\eprods{z}{1}{k})=(-1)^{k(k-1)/2}\bprods{T_Ez}{1}{k}\qquad(z_i\in E^*\tprod E)\]
In particular
\[T_E(z^{\widehat{k}})=(-1)^{k(k-1)/2}\medeprod^k T_Ez\qquad(z\in E^*\tprod E)\]
\end{rmk}

\begin{rmk}
If \(E\)~is \emph{finite-dimensional}, we can define the \emph{skew box product} of linear transformations \(\varphi\) and~\(\psi\) of~\(\medeprod E\) by
\[\varphi\sbprod\psi=T_E(T_E^{-1}\varphi\eprod T_E^{-1}\psi)\]
so \(T_E\)~becomes an \emph{algebra} isomorphism from the skew mixed exterior algebra to a skew box product algebra.

We can also define the \emph{skew mixed intersection product} by
\[w\ssect z=D_E(D_E^{-1}w\eprod D_E^{-1}z)\]
\end{rmk}

\subsection*{\S~18}
\begin{rmk}
The inner product \(\ssprod{-}{-}\) in \(\medeprod E^*\stprod\medeprod E\) is obtained from the scalar product between \(\medeprod E^*\stprod\medeprod E\) and \(\medeprod E\stprod\medeprod E^*\) using the \emph{skew} flip operator\footnote{See the remarks in subsection 2.8 above.}
\[\widehat{f}:\medeprod E^*\stprod\medeprod E\to\medeprod E\stprod\medeprod E^*\]
defined by
\[\widehat{f}(u^*\tprod u)=(-1)^{pq}u\tprod u^*\qquad(u^*\in\medeprod^p E^*,\ u\in\medeprod^q E)\]
This introduces the sign factors into the inner product, which are preserved by the canonical isomorphism
\[f:\medeprod E^*\stprod\medeprod E\to\medeprod(E^*\dsum E)\]
as seen in Proposition~6.18.1.
\end{rmk}

\begin{rmk}
The proof of Proposition~6.18.1 is similar to the alternative proof of~(5.51) we provide in chapter~5, \S~18 above.
\end{rmk}

\newpage
\section*{Chapter~7}
{\boldmath\textbf{In this chapter, \(E\) and~\(E^*\) are dual \(n\)-dimensional vector spaces over~\(\Gamma\).}}

\subsection*{\S~1}
\begin{rmk}
The map \(D_L=T_E\after D_E\after T_E^{-1}\) is an isometry since it is a composite of isometries. We have
\[D_L^*=(T_E^{-1})^*\after D_E^*\after T_E^*=T_E\after\Omega_E\after D_E\after T_E^{-1}=\Omega_L\after D_L\]
and
\[D_L^2=T_E\after D_E^2\after T_E^{-1}=T_E\after\Omega_E\after T_E^{-1}=\Omega_L\]
\end{rmk}

\begin{rmk}
In Proposition~7.1.1, (1)~is essentially the correlate of~(6.20) for~\(D_L\), and (2)~also follows from the corresponding result about~\(D_E\).\footnote{See the remark in chapter~6, \S~10 above.} It follows from~(1) that
\[\det(D_L\alpha)=\det\alpha\]
We also have
\[D_L^{-1}\alpha=D_e^{-1}\after\alpha^*\after D_e\]
\end{rmk}

\begin{rmk}
For linear transformations \(\alpha\) and~\(\beta\) of~\(\medeprod E\), we define the \emph{intersection product} in terms of the generalized box product\footnote{See the remark in chapter~6, \S~3 above.} by
\[\alpha\sect\beta=D_L(D_L^{-1}\alpha\bprod D_L^{-1}\beta)\]
This definition makes \(L(\medeprod E;\medeprod E)\) into an algebra called the \emph{intersection algebra}, and makes \(T_E\)~an \emph{algebra} isomorphism from the mixed intersection algebra\footnote{See the remark in chapter~6, \S~12 above.} since for \(w,z\in\medeprod(E^*,E)\),
\begin{align*}
T_E(w\sect z)&=T_ED_E(D_E^{-1}w\mprod D_E^{-1}z)\\
	&=D_L(T_ED_E^{-1}w\bprod T_ED_E^{-1}z)\\
	&=D_L(D_L^{-1}T_Ew\bprod D_L^{-1}T_Ez)\\
	&=T_Ew\sect T_Ez
\end{align*}
It follows that the intersection algebra is associative, unital (with unit \(\iota_{\eprod^n E}\)), and anticommutative. It also follows that \(D_L\)~is an \emph{algebra} isomorphism from the box product algebra to the intersection algebra \emph{and vice versa} since
\begin{align*}
D_L(\alpha\sect\beta)&=D_LT_E(T_E^{-1}\alpha\sect T_E^{-1}\beta)\\
	&=T_E(D_ET_E^{-1}\alpha\mprod D_ET_E^{-1}\beta)\\
	&=D_L\alpha\bprod D_L\beta
\end{align*}
For a linear transformation \(\varphi\) of~\(E\), viewed as a linear transformation of~\(\medeprod E\), we define
\[\medsect^p\varphi=\frac{1}{p!}\underbrace{\sects{\varphi}{}{}}_p\]
so
\[D_L(\medeprod^p\varphi)=\medsect^p D_L\varphi\qquad\text{and}\qquad D_L(\medsect^p\varphi)=\medeprod^p D_L\varphi\]
\end{rmk}

\begin{rmk}
Interchanging the roles of \(E\) and~\(E^*\), we obtain the map
\[D_{L^*}:L(\medeprod E^*;\medeprod E^*)\to L(\medeprod E^*;\medeprod E^*)\]
given by\footnote{See the remarks in chapter~6, \S~2,10 above.}
\[D_{L^*}=T_{E^*}\after D_{E^*}\after T_{E^*}^{-1}\]
\end{rmk}

\begin{rmk}
If \(\alpha\)~is a linear transformation of~\(\medeprod E\), then by Proposition~7.1.1(1),
\[(D_L\alpha)^*={(D^e)^{-1}}^*\after\alpha\after(D^e)^*=D_e\after\alpha\after D_e^{-1}=D_{L^*}\alpha^*\]
and by a remark above
\[(D_L^{-1}\alpha)^*=D_{L^*}^{-1}\alpha^*\]
In particular if \(\varphi_1,\ldots,\varphi_p,\varphi\) are linear transformations of~\(E\), then
\[(D_L(\bprods{\varphi}{1}{p}))^*=D_{L^*}(\bprods{\varphi^*}{1}{p})\]
and
\[(D_L(\medeprod^p\varphi))^*=D_{L^*}(\medeprod^p\varphi^*)\]
If \(\beta\)~is also a linear transformation of~\(\medeprod E\), then
\[(\alpha\sect\beta)^*=\alpha^*\sect\beta^*\]
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In the corollary to Proposition~7.2.1, taking duals on both sides of~(1) yields
\[D_{L^*}(\medeprod^{n-p}\varphi^*)\after\medeprod^p\varphi^*=\det\varphi^*\mult\iota_{\eprod^p E^*}\]
Substituting \(E\) for~\(E^*\), \(\varphi\) for~\(\varphi^*\), and \(n-p\) for~\(p\) yields~(2). Similarly taking duals and making substitutions in~(2) yields~(1).
\end{rmk}

\begin{rmk}
Let \(z\in E^*\tprod E\), \(\varphi=T_Ez\), and \(M=M(\varphi)\) the matrix of~\(\varphi\) with respect to some basis of~\(E\). The matrix \(M(\medeprod^p\varphi)\) with respect to the induced basis of~\(\medeprod^p E\) consists of the \(p\times p\) minors of~\(M\).\footnote{See the remark in chapter~5, \S~8 above.} On the other hand, since
\[D_L(\medeprod^{n-p}\varphi)=D_LT_Ez^{n-p}=T_ED_Ez^{n-p}\]
the matrix \(M(D_L(\medeprod^{n-p}\varphi))\) consists of the components of~\(D_Ez^{n-p}\), so is the transpose of the matrix of signed \((n-p)\times(n-p)\) minors of~\(M\) complementary to the minors in~\(M(\medeprod^p\varphi)\).\footnote{See the remark in chapter~6, \S~13 above.} By part~(1) of the corollary to Proposition~7.2.1,
\[M(\medeprod^p\varphi)M(D_L(\medeprod^{n-p}\varphi))=\det M\mult I\]
which, by the above results, expresses the classical Laplace expansion of~\(\det M\) by minors from any \(p\)~rows of~\(M\) and their corresponding cofactors.\footnote{See also problem~7.9.9.} Part~(2) of the corollary expresses the Laplace expansion from columns of~\(M\).
\end{rmk}

\begin{rmk}
It follows from Proposition~7.2.2 that\footnote{See the remark in chapter~6, \S~3 above.}
\[i_{\bprod}(D_L(\medeprod^p\varphi))(\medeprod^q\varphi)=\binom{2n-p-q}{n-p}\det\varphi\mult\medeprod^{p+q-n}\varphi\tag{1}\]
and
\[D_L(\medeprod^p\varphi)\bprod D_L(\medeprod^q\varphi)=\binom{2n-p-q}{n-p}\det\varphi\mult D_L(\medeprod^{p+q-n}\varphi)\tag{2}\]
Applying \(D_L\)~to both sides of~(2) yields
\[\medeprod^p\varphi\sect\medeprod^q\varphi=\binom{2n-p-q}{n-p}\det\varphi\mult\medeprod^{p+q-n}\varphi\]
and dually
\[\medsect^p\varphi\bprod\medsect^q\varphi=\binom{2n-p-q}{n-p}\det\varphi\mult\medsect^{p+q-n}\varphi\]
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
We have
\[\Ad(x^*_1\tprod x_1,\ldots,x^*_{n-1}\tprod x_{n-1})=[x^*_1,\ldots,x^*_{n-1},x_1,\ldots,x_{n-1}]\]
where the product on the right is the mixed external product.\footnote{See the remark in chapter~6, \S~14 above.}
\end{rmk}

\begin{rmk}
For linear transformations \(\varphi_1,\ldots,\varphi_{n-1}\) of~\(E\), it is possible to \emph{define}
\[\Ad(\varphi_1,\ldots,\varphi_{n-1})=T_E\Ad(T_E^{-1}\varphi_1,\cdots,T_E^{-1}\varphi_{n-1})\]
Then
\begin{align*}
\Ad(\varphi_1,\ldots,\varphi_{n-1})&=T_ED_E(\mprods{T_E^{-1}\varphi}{1}{n-1})\\
	&=D_L(\bprods{\varphi}{1}{n-1})\\
	&=\sects{D_L\varphi}{1}{n-1}
\end{align*}
For a linear transformation \(\varphi\) of~\(E\), it is also possible to \emph{define}
\[\ad\varphi=\frac{1}{(n-1)!}\Ad(\varphi,\ldots,\varphi)\]
Then
\[\ad\varphi=\medsect^{n-1}D_L\varphi=D_L(\medeprod^{n-1}\varphi)\]
It follows from the Laplace formula that
\[\varphi\after\ad(\varphi)=\ad(\varphi)\after\varphi=\det\varphi\mult\iota\]
This provides an alternative approach to the classical adjoint.\footnote{Compare with subsection~7.4.}
\end{rmk}

\begin{rmk}
It follows from Proposition~7.2.2(1) that for \(z\in E^*\tprod E\),
\[i(\ad z)z^{n-p}=(p+1)\det(T_Ez)z^{n-(p+1)}\qquad(0\le p\le n)\]
\end{rmk}

\begin{rmk}
We provide an alternative inductive proof of the Jacobi identity~(7.6). The identity is certainly true for \(p=1\), and if it is true for~\(p\) then by the previous remark
\begin{align*}
(p+1)D_E[(\ad z)^{p+1}]&=D_E[(\ad z)^p\mprod\ad z]\\
	&=i(\ad z)D_E[(\ad z)^p]\\
	&=(\det T_Ez)^{p-1}i(\ad z)z^{n-p}\\
	&=(p+1)(\det T_Ez)^pz^{n-(p+1)}
\end{align*}
Dividing both sides by~\(p+1\) shows that the identity holds for~\(p+1\).
\end{rmk}

\begin{rmk}
Applying \(D_E\) to both sides of the Jacobi identity~(7.6) yields
\[(\ad z)^p=(\det T_Ez)^{p-1}D_E(z^{n-p})\]
Applying \(T_E\) to both sides of this with \(\varphi=T_Ez\) yields
\[\medeprod^p(\ad\varphi)=(\det\varphi)^{p-1}D_L(\medeprod^{n-p}\varphi)\]
In terms of matrices, this identity relates the minors of a matrix to the minors of its adjoint matrix.\footnote{See problem~2.9.11.}
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
To see that \(\Omega_{\Delta}\)~is skew-symmetric, observe that it is multilinear and for \(\tau\in S_n\)
\begin{align*}
(\tau\Omega_{\Delta})(x_1,\ldots,x_n)h&=\Omega_{\Delta}(x_{\tau(1)},\ldots,x_{\tau(n)})h\\
	&=\sum_{\sigma}\sign{\sigma}\Delta(\varphi_1x_{\tau\sigma(1)},\ldots,\varphi_{n-1}x_{\tau\sigma(n-1)},h)x_{\tau\sigma(n)}\\
	&=\sign{\tau}\sum_{\sigma}\sign{\tau\sigma}\Delta(\varphi_1x_{\tau\sigma(1)},\ldots,\varphi_{n-1}x_{\tau\sigma(n-1)},h)x_{\tau\sigma(n)}\\
	&=\sign{\tau}\Omega_{\Delta}(x_1,\ldots,x_n)h
\end{align*}
It follows that \(\tau\Omega_{\Delta}=\sign{\tau}\Omega_{\Delta}\).
\end{rmk}

\begin{rmk}
\(\Ad(\varphi_1,\ldots,\varphi_{n-1})\) does not depend upon the choice of~\(\Delta\), and therefore only depends upon \(\varphi_1,\ldots,\varphi_{n-1}\). Since \(\Ad\)~is \((n-1)\)-linear and symmetric, it factors through the \((n-1)\)-th symmetric power of \(L(E;E)\).
\end{rmk}

\begin{rmk}
We have\footnote{Compare with subsection~6.3.}
\[\Omega_{\Delta}(x_1,\ldots,x_n)h=\sum_{\sigma}\Delta(\varphi_{\sigma^{-1}(1)}x_1,\ldots,h,\ldots,\varphi_{\sigma^{-1}(n)}x_n)x_{\sigma(n)}\]
where \(h\)~is in the \(\sigma(n)\)-th position of~\(\Delta\). For \(\varphi_1=\cdots=\varphi_{n-1}=\varphi\) this simplifies to
\begin{align*}
\Omega_{\Delta}(x_1,\ldots,x_n)h&=\sum_{\sigma}\Delta(\varphi x_1,\ldots,h,\ldots,\varphi x_n)x_{\sigma(n)}\\
	&=(n-1)!\sum_{i=1}^n\Delta(\varphi x_1,\ldots,h,\ldots,\varphi x_n)x_i\\
	&=(n-1)!\sum_{i=1}^n(-1)^{i-1}\Delta(h,\varphi x_1,\ldots,\delete{\varphi x_i},\ldots,\varphi x_n)x_i
\end{align*}
Therefore
\[\Delta(x_1,\ldots,x_n)\ad(\varphi)h=\sum_{i=1}^n(-1)^{i-1}\Delta(h,\varphi x_1,\ldots,\delete{\varphi x_i},\ldots,\varphi x_n)x_i\]
which agrees with~(4.12) in~\cite{greub1}.
\end{rmk}

\begin{rmk}
We have
\begin{align*}
\Ad(\varphi_1,\ldots,\varphi_{n-1})^*&=D_L(\bprods{\varphi}{1}{n-1})^*\\
	&=D_{L^*}(\bprods{\varphi^*}{1}{n-1})\\
	&=\Ad(\varphi_1^*,\ldots,\varphi_{n-1}^*)
\end{align*}
In particular
\[\ad(\varphi)^*=\ad(\varphi^*)\]
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
If \(\varphi\) and~\(\psi\) are linear transformations of~\(E\), then we have the binomial formula
\[\medeprod^n(\varphi+\psi)=\sum_{p+q=n}\medeprod^p\varphi\bprod\medeprod^q\psi\tag{1}\]
Taking the trace of both sides yields
\[\det(\varphi+\psi)=\sum_{p+q=n}\tr(\medeprod^p\varphi\bprod\medeprod^q\psi)\tag{2}\]
or equivalently
\[\det(\varphi+\psi)=\sum_{p+q=n}\frac{1}{p!q!}\Det(\underbrace{\varphi,\ldots,\varphi}_p,\underbrace{\psi,\ldots,\psi}_q)\]
which is the correct version of the formula in the book.

Taking \(\psi=\lambda\iota\) in~(2) yields Proposition~7.5.1.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
In the proof of Proposition~7.6.1, (7.11)~is obtained by induction on~\(p\). For \(p=0\) the result is certainly true, and if it is true for \(p-1<n\), then it is also true for~\(p\) since
\begin{align*}
A_p(\varphi)&=C_p(\varphi)\mult\iota-\varphi\after A_{p-1}(\varphi)\\
	&=C_p(\varphi)\varphi^0-\sum_{\nu=0}^{p-1}(-1)^{\nu+1}C_{p-(\nu+1)}(\varphi)\varphi^{\nu+1}\\
	&=\sum_{\nu=0}^p(-1)^{\nu}C_{p-\nu}(\varphi)\varphi^{\nu}
\end{align*}
\end{rmk}

\begin{rmk}
It follows from the binomial formula that
\[\medeprod^{n-1}(\varphi+\lambda\iota)=\sum_{p=0}^{n-1}\medeprod^p\varphi\bprod\medeprod^{n-p-1}\iota\mult\lambda^{n-p-1}\]
so
\begin{align*}
\ad(\varphi+\lambda\iota)&=D_L(\medeprod^{n-1}(\varphi+\lambda\iota))\\
	&=\sum_{p=0}^{n-1}D_L(\medeprod^p\varphi)\sect\medeprod^{p+1}\iota\mult\lambda^{n-p-1}
\end{align*}
and therefore
\[A_p(\varphi)=D_L(\medeprod^p\varphi)\sect\medeprod^{p+1}\iota\qquad(0\le p\le n-1)\]
\end{rmk}

\subsection*{\S~9}
\begin{exer}[4]
If \(z\in E^*\tprod E\) and \(\varphi=T_Ez\), then
\[\sprod{z^p}{(\ad z)^p}=\binom{n}{p}\det(\varphi)^p\qquad(1\le p\le n)\]
\end{exer}
\begin{proof}
By the Jacobi identity~(7.6) and Proposition~7.2.2,
\begin{align*}
\sprod{z^p}{(\ad z)^p}&=\sprod{D_Ez^p}{D_E(\ad z)^p}\\
	&=\det(\varphi)^{p-1}\sprod{D_Ez^p}{z^{n-p}}\\
	&=\det(\varphi)^{p-1}i(D_Ez^p)z^{n-p}\\
	&=\binom{n}{p}\det(\varphi)^p\qedhere
\end{align*}
\end{proof}

\begin{exer}[5 - Cauchy]
If \(\varphi:E\to E\) is linear, then
\[\det(\medeprod^p\varphi)\mult\det(\medeprod^{n-p}\varphi)=\det(\varphi)^{\binom{n}{p}}\qquad(0\le p\le n)\]
\end{exer}
\begin{proof}
By taking the determinant on both sides of the Laplace formula, using the fact that \(\det D_L(\medeprod^{n-p}\varphi)=\det(\medeprod^{n-p}\varphi)\).
\end{proof}

\begin{exer}[6]
Let \(A\)~be an \(n\times n\) matrix. The coefficient of~\(\lambda^{n-p}\) in \(\det(A-\lambda I)\) is \((-1)^{n-p}\) times the sum of the principal \(p\times p\) minors of~\(A\).
\end{exer}
\begin{proof}
Write \(A=M(\varphi)\), so \(\det(A-\lambda I)=\det(\varphi-\lambda\iota)\). By Proposition~7.5.1, the coefficient of~\(\lambda^{n-p}\) in this determinant is
\[(-1)^{n-p}C_p(\varphi)=(-1)^{n-p}\tr(\medeprod^p\varphi)\]
which is just the sum of the principal \(p\times p\) minors of~\(A\) by a remark in chapter~5, \S~8 above.
\end{proof}

\begin{exer}[9, 12 - Laplace]
Let \(A\)~be an \(n\times n\) matrix. For any \(1\le\lambda_1<\cdots<\lambda_p\le n\),
\[\det A=\sum_{\nu_1<\cdots<\nu_p}(-1)^{\sum_{i=1}^p\lambda_i+\nu_i}\det A_{\lambda_1\cdots\lambda_p}^{\nu_1\cdots\nu_p}\mult\det A_{\lambda_{p+1}\cdots\lambda_n}^{\nu_{p+1}\cdots\nu_n}\]
where \((\lambda_{p+1},\ldots,\lambda_n)\) and \((\nu_{p+1},\ldots,\nu_n)\) are the ordered \((n-p)\)-tuples complementary to \((\lambda_1,\ldots,\lambda_p)\) and \((\nu_1,\ldots,\nu_p)\), respectively.
\end{exer}
\begin{proof}
By a remark in \S~2 above.

Alternatively, write \(A=(\alpha_i^j)\). Assume without loss of generality that \(\lambda_i=i\) for all \(1\le i\le p\). Let \(e_i,e^{*i}\) be a pair of dual bases in \(E,E^*\) with \(e=e_{1\cdots n}\) and \(e^*=e^{1\cdots n}\) and let \(\Delta\)~be the determinant function in~\(E\) with \(\Delta(e_1,\ldots,e_n)=1\). Set
\[x_i=\sum_{j=1}^n\alpha_i^je_j\qquad(1\le i\le n)\]
Then
\begin{align*}
\det A&=\Delta(x_1,\ldots,x_n)\\
	&=\sprod{e^*}{\eprods{x}{1}{n}}\\
	&=\sprod{e^*}{(\eprods{x}{1}{p})\eprod(\eprods{x}{p+1}{n})}\\
	&=\sprod{D_e(\eprods{x}{1}{p})}{\eprods{x}{p+1}{n}}\tag{1}
\end{align*}
Now we know from the corrected version of problem~8 in chapter~6, \S~15 above that
\[D_e(\eprods{x}{1}{p})=\sum_{\nu_1<\cdots<\nu_p}(-1)^{\sum_{i=1}^pi+\nu_i}\det A_{1\cdots p}^{\nu_1\cdots\nu_p}\mult e^{\nu_{p+1}\cdots\nu_n}\tag{2}\]
and we also know from a remark in chapter~5, \S~7 above that
\[\sprod{e^{\nu_{p+1}\cdots\nu_n}}{\eprods{x}{p+1}{n}}=\det A_{p+1\cdots n}^{\nu_{p+1}\cdots\nu_n}\tag{3}\]
The result follows from (1), (2), and~(3).
\end{proof}

\begin{exer}[10]
If \(n\ge 2\), the rank of the adjoint transformation is given by
\[r(\ad\varphi)=\begin{cases}
n&\text{if }r(\varphi)=n\\
1&\text{if }r(\varphi)=n-1\\
0&\text{if }r(\varphi)\le n-2
\end{cases}\]
\end{exer}
\begin{proof}
By the Jacobi identity~(7.6),
\[\medeprod^p(\ad\varphi)=(\det\varphi)^{p-1}D_L(\medeprod^{n-p}\varphi)\]
The result now follows from the fact that the rank of a transformation~\(\psi\) is the largest~\(r\) for which \(\medeprod^r\psi\ne0\).
\end{proof}

% References
\newpage
\begin{thebibliography}{0}
\bibitem{greub1} Greub, W. \textit{Linear Algebra}, 4th~ed. Springer, 1975.
\bibitem{greub2} Greub,W. \textit{Multilinear Algebra}, 2nd~ed. Springer, 1978.
\end{thebibliography}
\end{document}