% Notes and exercises from Linear Algebra by Greub
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier,diagrams,stmaryrd}

\newcommand{\R}{\mathbb{R}}

\newcommand{\from}{\leftarrow}
\newcommand{\iso}{\cong}
\renewcommand{\equiv}{\sim}

\newcommand{\after}{\circ}
\newcommand{\dsum}{\oplus}

\newcommand{\delete}{\widehat}
\newcommand{\gen}[1]{\langle#1\rangle}
\newcommand{\pair}[2]{\langle#1,#2\rangle}
\newcommand{\sprod}[2]{\langle#1,#2\rangle}
\newcommand{\oc}[1]{#1^{\perp}}
\newcommand{\occ}[1]{#1^{\perp\perp}}

\newarrow{Dashto}{}{dash}{}{dash}>

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

% Meta
\title{Notes and exercises from\\\textit{Linear Algebra}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

\section*{Introduction}
This document contains notes and exercises from~\cite{greub}. Unless otherwise stated, \(\Gamma\)~denotes a field of characteristic~\(0\).

\section*{Chapter~I}
\subsection*{\S~1}
\begin{rmk}
The free vector space~\(C(X)\) is intuitively the space of all ``formal linear combinations'' of \(x\in X\).
\end{rmk}

\subsection*{\S~2}
\begin{exer}[5 - Universal property of~\(C(X)\)]
Let \(X\)~be a set and \(C(X)\)~the free vector space on~\(X\) (\S~1.7). Recall 
\[C(X)=\{\,f:X\to\Gamma\mid f(x)=0\text{ for all but finitely many }x\in X\,\}\]
The inclusion map \(i_X:X\to C(X)\) is defined by \(a\mapsto f_a\) where \(f_a\)~is the ``characteristic function'' of~\(a\): \(f_a(a)=1\) and \(f_a(x)=0\) for all \(x\ne a\). For \(f\in C(X)\), \(f=\sum_{a\in X}f(a)f_a\).
\begin{enumerate}
\item[(i)] If \(F\)~is a vector space and \(f:X\to F\), there is a unique \emph{linear} \(\varphi:C(X)\to F\) ``extending~\(f\)'' in the sense that \(\varphi\after i_X=f\):
\begin{diagram}[nohug]
X	&\rTo^{i_X}	&C(X)\\
	&\rdTo_f	&\dDashto>{\varphi}\\
	&			&F
\end{diagram}

\item[(ii)] If \(\alpha:X\to Y\), there is a unique \emph{linear} \(\alpha_*:C(X)\to C(Y)\) which makes the following diagram commute:
\begin{diagram}
X			&\rTo^{\alpha}			&Y\\
\dTo<{i_X}	&						&\dTo>{i_Y}\\
C(X)		&\rDashto_{\alpha_*}	&C(Y)
\end{diagram}
If \(\beta:Y\to Z\), then \((\beta\after\alpha)_*=\beta_*\after\alpha_*\).

\item[(iii)] If \(E\)~is a vector space, there is a unique linear map \(\pi_E:C(E)\to E\) such that \(\pi_E\after i_E=\iota_E\) (where \(\iota_E:E\to E\) is the identity map):
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&C(E)\\
	&\rdTo_{\iota_E}	&\dDashto>{\pi_E}\\
	&			&E
\end{diagram}

\item[(iv)] If \(E\) and~\(F\) are vector spaces and \(\varphi:E\to F\), then \(\varphi\)~is linear if and only if \(\pi_F\after\varphi_*=\varphi\after\pi_E\):
\begin{diagram}[nohug]
E			&				&\rTo^{\varphi}		&				&F				&				&\\
			&\rdTo			&					&				&\vLine>{i_F}	&\rdTo			&\\
\dTo<{i_E}	&				&E					&\rTo^{\varphi}	&\HonV			&				&F\\
			&\ruTo>{\pi_E}	&					&				&\dTo			&\ruTo>{\pi_F}	&\\
C(E)		&				&\rTo_{\varphi_*}	&				&C(F)			&				&
\end{diagram}

\item[(v)] Let \(E\)~be a vector space and \(N(E)\)~the subspace of~\(C(E)\) generated by all elements of the form
\[f_{\lambda a+\mu b}-\lambda f_a-\mu f_b\qquad(a,b\in E\text{ and }\lambda,\mu\in\Gamma)\]
Then \(\ker\pi_E=N(E)\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] By Proposition~II, since \(i_X(X)\)~is a basis of~\(C(X)\).

\item[(ii)] By~(i), applied to~\(i_Y\after\alpha\). Note \(\beta_*\after\alpha_*\)~is linear such that
\[(\beta_*\after\alpha_*)\after i_X=i_Z\after(\beta\after\alpha)\]
so \(\beta_*\after\alpha_*=(\beta\after\alpha)_*\) by uniqueness:
\begin{diagram}
X			&\rTo^{\alpha}		&Y			&\rTo^{\beta}	&Z\\
\dTo<{i_X}	&					&\dTo>{i_Y}	&				&\dTo>{i_Z}\\
C(X)		&\rTo_{\alpha_*}	&C(Y)		&\rTo_{\beta_*}	&C(Z)
\end{diagram}

\item[(iii)] By~(i), applied to~\(\iota_E\).

\item[(iv)] If \(\varphi\)~is linear, then \(\varphi\after\pi_E:C(E)\to F\) is linear and extends~\(\varphi\) in the sense that \(\varphi\after\pi_E\after i_E=\varphi\after\iota_E=\varphi\). However, \(\pi_F\after\varphi_*:C(E)\to F\) is also linear and extends~\(\varphi\) since
\[\pi_F\after\varphi_*\after i_E=\pi_F\after i_F\after\varphi=\iota_F\after\varphi=\varphi\]
By uniqueness, these two maps must be equal. Conversely, if these two maps are equal, then \(\varphi\)~is linear since \(\pi_F\after\varphi_*\)~is linear and \(\pi_E\)~is surjective.

\item[(v)] By~(iii),
\begin{align*}
\pi_E(f_{\lambda a+\mu b}-\lambda f_a-\mu f_b)&=\pi_E(f_{\lambda a+\mu b})-\lambda\pi_E(f_a)-\mu\pi_E(f_b)\\
	&=\lambda a+\mu b-\lambda a-\mu b\\
	&=0
\end{align*}
for all \(a,b\in E\) and \(\lambda,\mu\in\Gamma\). It follows that \(N(E)\subseteq\ker\pi_E\) since \(N(E)\)~is the \emph{smallest} subspace containing these elements and \(\ker\pi_E\)~is a subspace.

On the other hand, it follows from the fact that \(N(E)\)~is a subspace that
\[\sum\lambda_i f_{a_i}-f_{\;\sum\lambda_i a_i}\in N(E)\]
for all (finite) linear combinations. Now if \(g=\sum_{a\in E}g(a)f_a\in\ker\pi_E\), then
\[0=\pi_E(g)=\sum_{a\in E}g(a)\pi_E(f_a)=\sum_{a\in E}g(a)a\]
This implies \(f_{\;\sum_{a\in E}g(a)a}=f_0\in N(E)\). But by the above, \(g-f_0\in N(E)\), so \(g\in N(E)\). Therefore also \(\ker\pi_E\subseteq N(E)\).\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
Note (i)~shows that \(C(X)\)~is a universal (initial) object in the category of ``vector spaces with maps of~\(X\) into them''. In this category, the objects are maps \(X\to F\), for vector spaces~\(F\), and the arrows are \emph{linear} (i.e. structure-preserving) maps \(F\to G\) between the vector spaces which respect the mappings of~\(X\):
\begin{diagram}[nohug]
X	&\rTo	&F\\
	&\rdTo	&\dTo\\
	&		&G
\end{diagram}
By~(i), every object \(X\to F\) in this category can be obtained from the inclusion map \(X\to C(X)\) in a unique way. This is why \(C(X)\) is called ``universal''. This is only possible because \(C(X)\)~is free from any nontrivial relations among the elements of~\(X\), so any relations among the images of those elements in~\(F\) can be obtained starting from~\(C(X)\). This is why \(C(X)\)~is called ``free''. It is immediate from the universal property that \(C(X)\)~is unique up to isomorphism: if \(X\to U\) is also universal, then the composites \(\psi\after\varphi\) and~\(\varphi\after\psi\) of the induced linear maps \(\varphi:C(X)\to U\) and \(\psi:U\to C(X)\) are linear and extend the inclusion maps, so must be the identity maps on \(C(X)\) and~\(U\) by uniqueness; that is, \(\varphi\) and~\(\psi\) are mutually inverse and hence \emph{isomorphisms}. In fact they are also unique by the universal property.

Now (ii)~shows that we have a \emph{functor} from the category of sets into the category of vector spaces, which sends sets \(X\) and~\(Y\) to the vector spaces \(C(X)\) and~\(C(Y)\), and which sends a set map \(\alpha:X\to Y\) to the linear map \(\alpha_*:C(X)\to C(Y)\). The functor preserves the category structure of composites of arrows.

In~(iii), we are ``forgetting'' the linear structure of~\(E\) when forming~\(C(E)\). For example, if \(E=\R^2\), then \(\pair{1}{1}=\pair{1}{0}+\pair{0}{1}\) in~\(E\), but \emph{not} in~\(C(E)\). The ``formal'' linear combination
\[\pair{1}{1}-\pair{1}{0}-\pair{0}{1}\]
is not zero in~\(C(E)\) because the pairs are unrelated elements (symbols) which are \emph{linearly independent}. Note \(\pi_E\)~is surjective (since \(\iota_E\)~is), so \(E\)~is a projection of~\(C(E)\). In~(iv), we see that \(\varphi:E\to F\) is linear if and only if it is a ``projection'' of \(\varphi_*:C(E)\to C(F)\).

In~(v), we see that \(\pi_E\)~just recalls the linear structure of~\(E\) that was forgotten in~\(C(E)\). In particular, \(C(E)/N(E)\iso E\). In other words, if you start with~\(E\), then forget about its linear structure, then recall that linear structure, you just get \(E\)~again.
\end{rmk}

\subsection*{\S~4}
\begin{exer}[11]
Let \(E\)~be a real vector space and \(E_1\)~a vector hyperplane in~\(E\) (that is, a subspace of codimension~\(1\)). Define an equivalence relation on \(E^1=E-E_1\) as follows: for \(x,y\in E^1\), \(x\equiv y\) if the segment
\[x(t)=(1-t)x+ty\qquad(0\le t\le 1)\]
is disjoint from~\(E_1\). Then there are precisely two equivalence classes.
\end{exer}
\begin{proof}
Fix \(e\in E^1\) with \(E=E_1\dsum\gen{e}\) and define \(\alpha:E\to\R\) by \(x-\alpha(x)e\in E_1\) for all \(x\in E\). It is clear that \(\alpha\)~is linear, and \(x\in E_1\) if and only if \(\alpha(x)=0\). For \(x,y\in E^1\), it follows that \(x\equiv y\) if and only if
\[0\ne\alpha(x(t))=\alpha((1-t)x+ty)=(1-t)\alpha(x)+t\alpha(y)\]
for all \(0\le t\le 1\). But this is just equivalent to \(\alpha(x)\alpha(y)>0\).

Now if \(x\in E^1\), then \(\alpha(x)\ne0\), so \(\alpha(x)^2>0\) and \(x\equiv x\). If \(x\equiv y\), then \(\alpha(y)\alpha(x)=\alpha(x)\alpha(y)>0\), so \(y\equiv x\). If also \(y\equiv z\), then \(\alpha(y)\alpha(z)>0\), so \(\alpha(x)\alpha(z)>0\) and \(x\equiv z\). In other words, this is indeed an equivalence relation.

Note there are at least two equivalence classes since \(\alpha(e)=1\) and \(\alpha(-e)=-1\), so \(\alpha(e)\alpha(-e)=-1<0\) and \(e\not\equiv -e\). On the other hand, there are at most two classes since if \(x\in E^1\), then either \(\alpha(x)>0\) and \(x\equiv e\) or \(\alpha(x)<0\) and \(x\equiv -e\).
\end{proof}
\begin{rmk}
This result shows that the hyperplane separates the vector space into two disjoint half-spaces.
\end{rmk}

\section*{Chapter~II}
\subsection*{\S~4}
\begin{rmk}
The direct sum \(E\dsum F\) is a coproduct in the category of vector spaces in the following sense: if \(\varphi:E\to G\) and \(\psi:F\to G\) are linear maps, there is a unique linear map \(\chi:E\dsum F\to G\) such that \(\varphi=\chi\after i_E\) and \(\psi=\chi\after i_F\), where \(i_E\) and~\(i_F\) are the canonical injections:
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&E\dsum F		&\lTo^{i_F}		&F\\
	&\rdTo<{\varphi}	&\dDashto{\chi}	&\ldTo>{\psi}	&\\
	&					&G				&				&
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x+y)=\varphi(x)+\psi(y)\) for \(x\in E\), \(y\in F\). It is the unique linear map ``extending'' both \(\varphi\) and~\(\psi\). This property makes \(E\dsum F\) unique up to a unique isomorphism.

Dually, \(E\dsum F\) is a product in the following sense: if \(\varphi:G\to E\) and \(\psi:G\to F\) are linear maps, there is a unique linear map \(\chi:G\to E\dsum F\) such that \(\varphi=\pi_E\after\chi\) and \(\psi=\pi_F\after\chi\):
\begin{diagram}[nohug]
	&					&G				&				&\\
	&\ldTo<{\varphi}	&\dDashto{\chi}	&\rdTo>{\psi}	&\\
E	&\lTo^{\pi_E}		&E\dsum F		&\rTo^{\pi_F}	&F
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x)=\varphi(x)+\psi(x)\), and ``combines'' \(\varphi\) and~\(\psi\). This property also makes \(E\dsum F\) unique up to a unique isomorphism. An infinite direct sum is also a coproduct, but \emph{not} a product, essentially because it has no infinite sums of elements.

In the proof of Proposition~I, \(\sigma\)~is the product map and \(\tau\)~is the coproduct map. If \(\varphi_1:E_1\to F_1\) and \(\varphi_2:E_2\to F_2\) are linear maps, then \(\varphi=\varphi_1\dsum\varphi_2\) is both a coproduct and product map:
\begin{diagram}
E_1				&\pile{\rTo\\\lTo}	&E_1\dsum E_2	&\pile{\lTo\\\rTo}	&E_2\\
\dTo<{\varphi_1}&					&\dTo>{\varphi}	&					&\dTo>{\varphi_2}\\
F_1				&\pile{\rTo\\\lTo}	&F_1\dsum F_2	&\pile{\lTo\\\rTo}	&F_2
\end{diagram}
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Let \(E\)~be a vector space and \((x_{\alpha})_{\alpha\in A}\) be a basis of~\(E\). For each \(x\in E\), write \(x=\sum_{\alpha\in A}f_{\alpha}(x)x_{\alpha}\). Then \(f_{\alpha}\in L(E)\) for each \(\alpha\in A\). The function \(f_{\alpha}\) is called the \emph{\(\alpha\)-th coordinate functional} for the basis.

Coordinate functionals can be used in an alternative proof of Proposition~IV. If \(E_1\)~is a subspace of~\(E\), let \(B_1\)~be a basis of~\(E_1\) and extend it to a basis \(B\) of~\(E\). For each \(x_{\alpha}\in B-B_1\), we have \(f_{\alpha}\in\oc{E_1}\). If \(x\in\occ{E_1}\), then \(f_{\alpha}(x)=\sprod{f_{\alpha}}{x}=0\) for all such \(\alpha\), so \(x\in E_1\). In other words, \(\occ{E_1}\subseteq E_1\).
\end{rmk}

\begin{rmk}
For \(\varphi:E\to F\) a linear map, let \(L(\varphi):L(E)\from L(F)\) be the dual map given by \(L(\varphi)(f)=f\after\varphi\) (2.50). Then \(L\)~linearly embeds \(L(E;F)\) in \(L(L(F);L(E))\), by (2.43) and~(2.44). Also, \(L(\psi\after\varphi)=L(\varphi)\after L(\psi)\) and \(L(\iota_{E})=\iota_{L(E)}\). This shows that \(L\)~is a contravariant functor in the category of vector spaces. This functor preserves exactness of sequences (see~2.29), and finite direct sums, which are just (co)products in the category (see~2.30), among other things.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(E\)~has finite dimension, then every basis of a dual space~\(E^*\) is a dual basis. Indeed, if \(f_1,\ldots,f_n\) is a basis of~\(E^*\), let \(e_1,\ldots,e_n\) be its dual basis in~\(E\). Then \(\sprod{f_i}{e_j}=\delta_{ij}\) by~(2.62), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\), again by~(2.62).

Alternatively, with \(E^*=L(E)\), let \(f_1^*,\ldots,f_n^*\) be the dual basis of \(f_1,\ldots,f_n\) in \(E^{**}=L(L(E))\), so \(\sprod{f_j^*}{f_i}=\delta_{ij}\) by~(2.62). Let \(e_1,\ldots,e_n\in E\) be defined by \(\sprod{f_j^*}{f}=\sprod{f}{e_j}\) for all \(f\in E^*\) (see \S~5, problem~3). Then \(\sprod{f_i}{e_j}=\sprod{f_j^*}{f_i}=\delta_{ij}\), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\).

The first proof here uses the symmetry between \(E\) and~\(E^*\), while the second uses the natural isomorphism \(E\iso E^{**}\).
\end{rmk}

\section*{Chapter~III}
\begin{rmk}
Greub's notational choices in this chapter are insane.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
In section~3.13, note \((\alpha^{\mu}_{\nu})=M(\iota,\bar{x}_{\nu},x_{\mu})\) by~(3.22), \((\check{\alpha}^{\mu}_{\nu})=M(\iota,x_{\nu},\bar{x}_{\mu})\) by~(3.23), and \((\beta^{\varrho}_{\sigma})=M(\iota,\bar{x}^{*\varrho},x^{*\sigma})\) by~(3.24). It follows from~(3.4) that
\[\bigl(\beta^{\mu}_{\nu}\bigr)=\bigl(\check{\alpha}^{\mu}_{\nu}\bigr)^*=\bigl(\bigl(\alpha^{\mu}_{\nu}\bigr)^{-1}\bigr)^*\]
In other words, the matrix of the dual basis transformation \(x^{*\nu}\mapsto\bar{x}^{*\nu}\) in~\(E^*\) is the \emph{transpose} of the inverse of the matrix of the basis transformation \(x_{\nu}\mapsto\bar{x}_{\nu}\) in~\(E\), contrary to what the book says. It's easier to remember that the matrix of \(x^{*\nu}\mapsfrom\bar{x}^{*\nu}\) (arrow reversed!) is the transpose of the matrix of \(x_{\nu}\mapsto\bar{x}_{\nu}\).
\end{rmk}

\section*{Chapter~IV}
\subsection*{\S~1}
\begin{rmk} To see why~(4.1) holds, observe by definition of~\(\tau(\sigma\Phi)\) that
\[(\tau(\sigma\Phi))(x_1,\ldots,x_p)=(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})\]
Let \(y_i=x_{\tau(i)}\). Then by definition of \(\sigma\Phi\) and~\((\tau\sigma)\Phi\),
\begin{align*}
(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})&=(\sigma\Phi)(y_1,\ldots,y_p)\\
	&=\Phi(y_{\sigma(1)},\ldots,y_{\sigma(p)})\\
	&=\Phi(x_{\tau(\sigma(1))},\ldots,x_{\tau(\sigma(p))})\\
	&=\Phi(x_{(\tau\sigma)(1)},\ldots,x_{(\tau\sigma)(p)})\\
	&=((\tau\sigma)\Phi)(x_1,\ldots,x_p)
\end{align*}
Therefore \(\tau(\sigma\Phi)=(\tau\sigma)\Phi\).
\end{rmk}

\begin{rmk}
By Proposition~I(iii) and Proposition~II, a determinant function \(\Delta\ne 0\) ``determines'' linear independence in the sense that \(\Delta(x_1,\ldots,x_n)\ne0\) if and only if \(x_1,\ldots,x_n\) are linearly independent. By~(4.8), it follows that \(\det\varphi\) ``determines'' whether a linear transformation~\(\varphi\) preserves linear independence, i.e. whether or not \(\varphi\)~is invertible.
\end{rmk}

\begin{rmk}
We provide an alternative proof of Proposition~IV. First note
\[(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)=\Delta(x_1,\ldots,x,\ldots,x_n)\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{\(\delete{x_j}\)~denotes deletion of \(x_j\)~from the sequence on the left.} Therefore
\[\sum_{j=1}^n(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)x_j=\Delta(x,x_2,\ldots,x_n)x_1+\cdots+\Delta(x_1,\ldots,x_{n-1},x)x_n\]
Viewing this as a function of \(x_1,\ldots,x_n\) (that is, a set map from \(E^n\to L(E;E)\)), it is obviously multilinear and skew symmetric (by Proposition~I(ii)). Therefore if \(x_1,\ldots,x_n\) are linearly dependent, it is zero (by Proposition~I(iii)). If \(x_1,\ldots,x_n\) are linearly independent (and hence a basis), then viewing it as a function of~\(x\), its value at~\(x_i\) is just \(\Delta(x_1,\ldots,x_n)x_i\) (by Proposition~I(ii)), so it agrees on a basis with \(\Delta(x_1,\ldots,x_n)x\) and hence is equal to it.
\end{rmk}


\subsection*{\S~2}
\begin{rmk}
In section~4.6, we want a linear transformation~\(\psi\) with \(\psi\varphi=(\det\varphi)\iota\). We choose a basis \(x_1,\ldots,x_n\) in~\(E\) with \(\Delta(x_1,\ldots,x_n)=1\). Then we want
\begin{align*}
(\psi\varphi)x_i=\psi(\varphi x_i)&=(\det\varphi)x_i\\
	&=(\det\varphi)\Delta(x_1,\ldots,x_n)x_i\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_n)x_i
\end{align*}
To achieve this, we define
\[\psi(x)=\sum_{j=1}^n\Delta(\varphi x_1,\ldots,x,\ldots,\varphi x_n)x_j\]
where \(x\)~is in the \(j\)-th position on the right. Then \(\psi\)~obviously satisfies the above properties, by multilinearity and skew symmetry of~\(\Delta\).

To obtain~\(\psi\) in a ``coordinate-free''  manner (without choosing a basis), we observe that the construction on the right is skew symmetric and multilinear in \(x_1,\ldots,x_n\) when viewed as a mapping \(\Phi:E^n\to L(E;E)\). By the universal property of~\(\Delta\) (Proposition~III), there is a unique \(\psi\in L(E;E)\) satisfying the above. It is also seen to be independent of the choice of~\(\Delta\).
\end{rmk}

% References
\begin{thebibliography}{0}
\bibitem{greub} Greub, W. \textit{Linear Algebra}, 4th~ed. Springer, 1975.
\end{thebibliography}
\end{document}