% Notes and exercises from Linear Algebra by Greub
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier,diagrams,stmaryrd}

\newcommand{\R}{\mathbb{R}}

\newcommand{\from}{\leftarrow}
\newcommand{\iso}{\cong}
\renewcommand{\equiv}{\sim}
\newcommand{\orth}{\perp}

\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\tr}{tr}

\newcommand{\sect}{\cap}
\newcommand{\after}{\circ}
\newcommand{\dsum}{\oplus}
\newcommand{\mult}{\cdot}
\newcommand{\cross}{\times}
\newcommand{\grad}{\nabla}

\newcommand{\delete}{\widehat}
\newcommand{\gen}[1]{\langle#1\rangle}
\newcommand{\pair}[2]{\langle#1,#2\rangle}
\newcommand{\sprod}[2]{\langle#1,#2\rangle}
\newcommand{\oc}[1]{#1^{\perp}}
\newcommand{\occ}[1]{#1^{\perp\perp}}
\newcommand{\opp}[1]{#1^{\mathrm{opp}}}
\newcommand{\iprod}[2]{(#1,#2)}
\newcommand{\norm}[1]{|#1|}
\newcommand{\abs}[1]{|#1|}
\newcommand{\adj}[1]{\widetilde{#1}}

\newarrow{Dashto}{}{dash}{}{dash}>

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{warn}{Warning}

% Meta
\title{Notes and exercises from\\\textit{Linear Algebra}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

\section*{Introduction}
This document contains notes and exercises from~\cite{greub}. {\boldmath\textbf{Unless otherwise stated, \(\Gamma\)~denotes a field of characteristic~\(0\).}}

\section*{Chapter~I}
\subsection*{\S~1}
\begin{rmk}
The free vector space~\(C(X)\) is intuitively the space of all ``formal linear combinations'' of \(x\in X\).
\end{rmk}

\subsection*{\S~2}
\begin{exer}[5 - Universal property of~\(C(X)\)]
Let \(X\)~be a set and \(C(X)\)~the free vector space on~\(X\) (subsection~1.7). Recall
\[C(X)=\{\,f:X\to\Gamma\mid f(x)=0\text{ for all but finitely many }x\in X\,\}\]
The inclusion map \(i_X:X\to C(X)\) is defined by \(a\mapsto f_a\) where \(f_a\)~is the ``characteristic function'' of~\(a\): \(f_a(a)=1\) and \(f_a(x)=0\) for all \(x\ne a\). For \(f\in C(X)\), \(f=\sum_{a\in X}f(a)f_a\).
\begin{enumerate}
\item[(i)] If \(F\)~is a vector space and \(f:X\to F\), there is a unique \emph{linear} \(\varphi:C(X)\to F\) ``extending~\(f\)'' in the sense that \(\varphi\after i_X=f\):
\begin{diagram}[nohug]
X	&\rTo^{i_X}	&C(X)\\
	&\rdTo_f	&\dDashto>{\varphi}\\
	&			&F
\end{diagram}

\item[(ii)] If \(\alpha:X\to Y\), there is a unique \emph{linear} \(\alpha_*:C(X)\to C(Y)\) which makes the following diagram commute:
\begin{diagram}
X			&\rTo^{\alpha}			&Y\\
\dTo<{i_X}	&						&\dTo>{i_Y}\\
C(X)		&\rDashto_{\alpha_*}	&C(Y)
\end{diagram}
If \(\beta:Y\to Z\), then \((\beta\after\alpha)_*=\beta_*\after\alpha_*\).

\item[(iii)] If \(E\)~is a vector space, there is a unique linear map \(\pi_E:C(E)\to E\) such that \(\pi_E\after i_E=\iota_E\) (where \(\iota_E:E\to E\) is the identity map):
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&C(E)\\
	&\rdTo_{\iota_E}	&\dDashto>{\pi_E}\\
	&			&E
\end{diagram}

\item[(iv)] If \(E\) and~\(F\) are vector spaces and \(\varphi:E\to F\), then \(\varphi\)~is linear if and only if \(\pi_F\after\varphi_*=\varphi\after\pi_E\):
\begin{diagram}[nohug]
E			&				&\rTo^{\varphi}		&				&F				&				&\\
			&\rdTo			&					&				&\vLine>{i_F}	&\rdTo			&\\
\dTo<{i_E}	&				&E					&\rTo^{\varphi}	&\HonV			&				&F\\
			&\ruTo>{\pi_E}	&					&				&\dTo			&\ruTo>{\pi_F}	&\\
C(E)		&				&\rTo_{\varphi_*}	&				&C(F)			&				&
\end{diagram}

\item[(v)] Let \(E\)~be a vector space and \(N(E)\)~the subspace of~\(C(E)\) generated by all elements of the form
\[f_{\lambda a+\mu b}-\lambda f_a-\mu f_b\qquad(a,b\in E\text{ and }\lambda,\mu\in\Gamma)\]
Then \(\ker\pi_E=N(E)\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] By Proposition~II, since \(i_X(X)\)~is a basis of~\(C(X)\).

\item[(ii)] By~(i), applied to~\(i_Y\after\alpha\). Note \(\beta_*\after\alpha_*\)~is linear such that
\[(\beta_*\after\alpha_*)\after i_X=i_Z\after(\beta\after\alpha)\]
so \(\beta_*\after\alpha_*=(\beta\after\alpha)_*\) by uniqueness:
\begin{diagram}
X			&\rTo^{\alpha}		&Y			&\rTo^{\beta}	&Z\\
\dTo<{i_X}	&					&\dTo>{i_Y}	&				&\dTo>{i_Z}\\
C(X)		&\rTo_{\alpha_*}	&C(Y)		&\rTo_{\beta_*}	&C(Z)
\end{diagram}

\item[(iii)] By~(i), applied to~\(\iota_E\).

\item[(iv)] If \(\varphi\)~is linear, then \(\varphi\after\pi_E:C(E)\to F\) is linear and extends~\(\varphi\) in the sense that \(\varphi\after\pi_E\after i_E=\varphi\after\iota_E=\varphi\). However, \(\pi_F\after\varphi_*:C(E)\to F\) is also linear and extends~\(\varphi\) since
\[\pi_F\after\varphi_*\after i_E=\pi_F\after i_F\after\varphi=\iota_F\after\varphi=\varphi\]
By uniqueness, these two maps must be equal. Conversely, if these two maps are equal, then \(\varphi\)~is linear since \(\pi_F\after\varphi_*\)~is linear and \(\pi_E\)~is surjective.

\item[(v)] By~(iii),
\begin{align*}
\pi_E(f_{\lambda a+\mu b}-\lambda f_a-\mu f_b)&=\pi_E(f_{\lambda a+\mu b})-\lambda\pi_E(f_a)-\mu\pi_E(f_b)\\
	&=\lambda a+\mu b-\lambda a-\mu b\\
	&=0
\end{align*}
for all \(a,b\in E\) and \(\lambda,\mu\in\Gamma\). It follows that \(N(E)\subseteq\ker\pi_E\) since \(N(E)\)~is the \emph{smallest} subspace containing these elements and \(\ker\pi_E\)~is a subspace.

On the other hand, it follows from the fact that \(N(E)\)~is a subspace that
\[\sum\lambda_i f_{a_i}-f_{\;\sum\lambda_i a_i}\in N(E)\]
for all (finite) linear combinations. Now if \(g=\sum_{a\in E}g(a)f_a\in\ker\pi_E\), then
\[0=\pi_E(g)=\sum_{a\in E}g(a)\pi_E(f_a)=\sum_{a\in E}g(a)a\]
This implies \(f_{\;\sum_{a\in E}g(a)a}=f_0\in N(E)\). But by the above, \(g-f_0\in N(E)\), so \(g\in N(E)\). Therefore also \(\ker\pi_E\subseteq N(E)\).\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
Note (i)~shows that \(C(X)\)~is a universal (initial) object in the category of ``vector spaces with maps of~\(X\) into them''. In this category, the objects are maps \(X\to F\), for vector spaces~\(F\), and the arrows are \emph{linear} (i.e. structure-preserving) maps \(F\to G\) between the vector spaces which respect the mappings of~\(X\):
\begin{diagram}[nohug]
X	&\rTo	&F\\
	&\rdTo	&\dTo\\
	&		&G
\end{diagram}
By~(i), every object \(X\to F\) in this category can be obtained from the inclusion map \(X\to C(X)\) in a unique way. This is why \(C(X)\) is called ``universal''. This is only possible because \(C(X)\)~is free from any nontrivial relations among the elements of~\(X\), so any relations among the images of those elements in~\(F\) can be obtained starting from~\(C(X)\). This is why \(C(X)\)~is called ``free''. It is immediate from the universal property that \(C(X)\)~is unique up to isomorphism: if \(X\to U\) is also universal, then the composites \(\psi\after\varphi\) and~\(\varphi\after\psi\) of the induced linear maps \(\varphi:C(X)\to U\) and \(\psi:U\to C(X)\) are linear and extend the inclusion maps, so must be the identity maps on \(C(X)\) and~\(U\) by uniqueness; that is, \(\varphi\) and~\(\psi\) are mutually inverse and hence \emph{isomorphisms}. In fact they are also unique by the universal property.

Now (ii)~shows that we have a \emph{functor} from the category of sets into the category of vector spaces, which sends sets \(X\) and~\(Y\) to the vector spaces \(C(X)\) and~\(C(Y)\), and which sends a set map \(\alpha:X\to Y\) to the linear map \(\alpha_*:C(X)\to C(Y)\). The functor preserves the category structure of composites of arrows.

In~(iii), we are ``forgetting'' the linear structure of~\(E\) when forming~\(C(E)\). For example, if \(E=\R^2\), then \(\pair{1}{1}=\pair{1}{0}+\pair{0}{1}\) in~\(E\), but \emph{not} in~\(C(E)\). The ``formal'' linear combination
\[\pair{1}{1}-\pair{1}{0}-\pair{0}{1}\]
is not zero in~\(C(E)\) because the pairs are unrelated elements (symbols) which are \emph{linearly independent}. Note \(\pi_E\)~is surjective (since \(\iota_E\)~is), so \(E\)~is a projection of~\(C(E)\). In~(iv), we see that \(\varphi:E\to F\) is linear if and only if it is a ``projection'' of \(\varphi_*:C(E)\to C(F)\).

In~(v), we see that \(\pi_E\)~just recalls the linear structure of~\(E\) that was forgotten in~\(C(E)\). In particular, \(C(E)/N(E)\iso E\). In other words, if you start with~\(E\), then forget about its linear structure, then recall that linear structure, you just get \(E\)~again.
\end{rmk}

\subsection*{\S~4}
\begin{exer}[11]
Let \(E\)~be a real vector space and \(E_1\)~a vector hyperplane in~\(E\) (that is, a subspace of codimension~\(1\)). Define an equivalence relation on \(E^1=E-E_1\) as follows: for \(x,y\in E^1\), \(x\equiv y\) if the segment
\[x(t)=(1-t)x+ty\qquad(0\le t\le 1)\]
is disjoint from~\(E_1\). Then there are precisely two equivalence classes.
\end{exer}
\begin{proof}
Fix \(e\in E^1\) with \(E=E_1\dsum\gen{e}\) and define \(\alpha:E\to\R\) by \(x-\alpha(x)e\in E_1\) for all \(x\in E\). It is clear that \(\alpha\)~is linear, and \(x\in E_1\) if and only if \(\alpha(x)=0\). For \(x,y\in E^1\), it follows that \(x\equiv y\) if and only if
\[0\ne\alpha(x(t))=\alpha((1-t)x+ty)=(1-t)\alpha(x)+t\alpha(y)\]
for all \(0\le t\le 1\). But this is just equivalent to \(\alpha(x)\alpha(y)>0\).

Now if \(x\in E^1\), then \(\alpha(x)\ne0\), so \(\alpha(x)^2>0\) and \(x\equiv x\). If \(x\equiv y\), then \(\alpha(y)\alpha(x)=\alpha(x)\alpha(y)>0\), so \(y\equiv x\). If also \(y\equiv z\), then \(\alpha(y)\alpha(z)>0\), so \(\alpha(x)\alpha(z)>0\) and \(x\equiv z\). In other words, this is indeed an equivalence relation.

Note there are at least two equivalence classes since \(\alpha(e)=1\) and \(\alpha(-e)=-1\), so \(\alpha(e)\alpha(-e)=-1<0\) and \(e\not\equiv -e\). On the other hand, there are at most two classes since if \(x\in E^1\), then either \(\alpha(x)>0\) and \(x\equiv e\) or \(\alpha(x)<0\) and \(x\equiv -e\).
\end{proof}
\begin{rmk}
This result shows that the hyperplane separates the vector space into two disjoint half-spaces.
\end{rmk}

\section*{Chapter~II}
\subsection*{\S~2}
\begin{rmk}
In subsection~2.11, in the second part of the proof of Proposition~I, just let \(\psi:E\from F\) be any linear mapping extending \(\varphi_1^{-1}:E\from\im\varphi\).\footnote{See Corollary~I to Proposition~I in subsection~1.15.}
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The direct sum \(E\dsum F\) is a coproduct in the category of vector spaces in the following sense: if \(\varphi:E\to G\) and \(\psi:F\to G\) are linear maps, there is a unique linear map \(\chi:E\dsum F\to G\) such that \(\varphi=\chi\after i_E\) and \(\psi=\chi\after i_F\), where \(i_E\) and~\(i_F\) are the canonical injections:
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&E\dsum F		&\lTo^{i_F}		&F\\
	&\rdTo<{\varphi}	&\dDashto{\chi}	&\ldTo>{\psi}	&\\
	&					&G				&				&
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x+y)=\varphi(x)+\psi(y)\) for \(x\in E\), \(y\in F\). It is the unique linear map ``extending'' both \(\varphi\) and~\(\psi\). This property makes \(E\dsum F\) unique up to a unique isomorphism.

Dually, \(E\dsum F\) is a product in the following sense: if \(\varphi:G\to E\) and \(\psi:G\to F\) are linear maps, there is a unique linear map \(\chi:G\to E\dsum F\) such that \(\varphi=\pi_E\after\chi\) and \(\psi=\pi_F\after\chi\):
\begin{diagram}[nohug]
	&					&G				&				&\\
	&\ldTo<{\varphi}	&\dDashto{\chi}	&\rdTo>{\psi}	&\\
E	&\lTo^{\pi_E}		&E\dsum F		&\rTo^{\pi_F}	&F
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x)=\varphi(x)+\psi(x)\), and ``combines'' \(\varphi\) and~\(\psi\). This property also makes \(E\dsum F\) unique up to a unique isomorphism. An infinite direct sum is also a coproduct, but \emph{not} a product, essentially because it has no infinite sums of elements.

In the proof of Proposition~I, \(\sigma\)~is the product map and \(\tau\)~is the coproduct map. If \(\varphi_1:E_1\to F_1\) and \(\varphi_2:E_2\to F_2\) are linear maps, then \(\varphi=\varphi_1\dsum\varphi_2\) is both a coproduct and product map:
\begin{diagram}
E_1				&\pile{\rTo\\\lTo}	&E_1\dsum E_2	&\pile{\lTo\\\rTo}	&E_2\\
\dTo<{\varphi_1}&					&\dTo>{\varphi}	&					&\dTo>{\varphi_2}\\
F_1				&\pile{\rTo\\\lTo}	&F_1\dsum F_2	&\pile{\lTo\\\rTo}	&F_2
\end{diagram}
The structure of~\(\varphi\) is completely determined by the structures of \(\varphi_1\) and~\(\varphi_2\). In particular, \(\varphi\)~is injective (surjective, bijective) if and only if \(\varphi_1\) and~\(\varphi_2\) are.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
The definition of dual space is fundamentally \emph{symmetrical} between \(E\) and~\(E^*\), as is the definition of dual mapping between \(\varphi\) and~\(\varphi^*\). This symmetry often allows us to use bidirectional reasoning and derive two theorems from one proof. For example, (2.48) actually follows from~(2.47) by symmetry of \(\varphi\) and~\(\varphi^*\). The proof of Proposition~I in subsection~2.23 exploits symmetry, as do other proofs in the book. Many other books simply \emph{define} the dual space of~\(E\) to be~\(L(E)\) (no doubt in light of Proposition~I of this section), at the expense of this symmetry.
\end{rmk}

\begin{rmk}
If \(E,E^*\) and \(F,F^*\) are pairs of dual spaces and \(\varphi:E\to F\) is linear, then \(\varphi^*:E^*\from F^*\) is dual to~\(\varphi\) if and only if the following diagram commutes:
\begin{diagram}
F^*\times E&\rTo^{\varphi^*\times\iota_E}&E^*\times E\\
\dTo<{\iota_{F^*}\times\varphi}&		&\dTo>{\sprod{\ }{\ }}\\
F^*\times F&\rTo_{\sprod{\ }{\ }}&\Gamma
\end{diagram}
\end{rmk}

\begin{rmk}
Let \(E\)~be a vector space and \((x_{\alpha})_{\alpha\in A}\) be a basis of~\(E\). For each \(x\in E\), write \(x=\sum_{\alpha\in A}f_{\alpha}(x)x_{\alpha}\). Then \(f_{\alpha}\in L(E)\) for each \(\alpha\in A\). The function \(f_{\alpha}\) is called the \emph{\(\alpha\)-th coordinate function} for the basis.

Coordinate functions can be used in an alternative proof of Proposition~IV. If \(E_1\)~is a subspace of~\(E\), let \(B_1\)~be a basis of~\(E_1\) and extend it to a basis \(B\) of~\(E\). For each \(x_{\alpha}\in B-B_1\), we have \(f_{\alpha}\in\oc{E_1}\). If \(x\in\occ{E_1}\), then \(f_{\alpha}(x)=\sprod{f_{\alpha}}{x}=0\) for all such \(\alpha\), so \(x\in E_1\). In other words, \(\occ{E_1}\subseteq E_1\).
\end{rmk}

\begin{rmk}
In the corollary to Proposition~V, for \(f\in L(E)\) let \(f_k=f\after i_k\after \pi_k\) where \(i_k:E_k\to E\) is the \(k\)-th canonical injection and \(\pi_k:E\to E_k\) is the \(k\)-th canonical projection. Then \(f=\sum_k f_k\) and \(f_k\in\oc{F_k}\) for all~\(k\), so \(L(E)=\sum_k\oc{F_k}\). The sum is direct since if \(f\in\oc{F_k}\sect\sum_{j\ne k}\oc{F_j}\), then \(f\)~kills \(\sum_{j\ne k}E_j\) and~\(E_k\), so \(f=0\). A scalar product is induced between \(E_k,\oc{F_k}\) since \(E_k\sect F_k=0\) and \(\oc{F_k}\sect\oc{E_k}=0\).\footnote{See subsection~2.23.} The induced injection \(\oc{F_k}\to L(E_k)\) is surjective since every linear function on~\(E_k\) can be extended to a linear function on~\(E\) which kills~\(F_k\).
\end{rmk}

\begin{rmk}
For \(\varphi:E\to F\) a linear map, let \(L(\varphi):L(E)\from L(F)\) be the dual map given by \(L(\varphi)(f)=f\after\varphi\) (2.50). Then \(L\)~linearly embeds \(L(E;F)\) in \(L(L(F);L(E))\), by (2.43) and~(2.44). Also, \(L(\psi\after\varphi)=L(\varphi)\after L(\psi)\) and \(L(\iota_{E})=\iota_{L(E)}\). This shows that \(L\)~is a contravariant functor in the category of vector spaces. This functor preserves exactness of sequences (see~2.29), and finite direct sums, which are just (co)products in the category (see~2.30), among other things.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(E\)~is finite-dimensional, then every basis of a dual space~\(E^*\) is a dual basis. Indeed, if \(f_1,\ldots,f_n\) is a basis of~\(E^*\), let \(e_1,\ldots,e_n\) be its dual basis in~\(E\). Then \(\sprod{f_i}{e_j}=\delta_{ij}\) by~(2.62), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\), again by~(2.62).

Alternatively, with \(E^*=L(E)\), let \(f_1^*,\ldots,f_n^*\) be the dual basis of \(f_1,\ldots,f_n\) in \(E^{**}=L(L(E))\), so \(\sprod{f_j^*}{f_i}=\delta_{ij}\) by~(2.62). Let \(e_1,\ldots,e_n\in E\) be defined by \(\sprod{f_j^*}{f}=\sprod{f}{e_j}\) for all \(f\in E^*\) (see \S~5, problem~3). Then \(\sprod{f_i}{e_j}=\sprod{f_j^*}{f_i}=\delta_{ij}\), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\).

The first proof here uses the symmetry between \(E\) and~\(E^*\), while the second uses the natural isomorphism \(E\iso E^{**}\).
\end{rmk}

\begin{exer}[9]
If \(E\) and~\(F\) are finite-dimensional, then the mapping
\[\Phi:L(E;F)\to L(F^*;E^*)\]
defined by \(\varphi\mapsto\varphi^*\) is a linear isomorphism.
\end{exer}
\begin{proof}
By the remark in~\S~5 above, and the fact that \(\varphi^{**}=\varphi\).
\end{proof}

\section*{Chapter~III}
\begin{warn}
Greub's notational choices in this chapter are insane. In particular, although he uses left-hand function notation (writing \(\varphi x\) instead of~\(x\varphi\), and \(\varphi\psi\) to mean \emph{\(\varphi\) after \(\psi\)}), and follows the usual ``row-by-column'' convention for matrix multiplication, his convention for the matrix of a linear mapping is the transpose of that normally used with left-hand notation. This has the following undesirable consequences:
\begin{itemize}
\item The matrix of the linear mapping naturally associated with a system of linear equations has the coefficients from each equation appear \emph{vertically in columns}.
\item If \(M(x)\)~is the \emph{column vector} representing~\(x\), then \(M(\varphi x)=M(\varphi)^*M(x)\), and if \(M(x)\)~is the \emph{row vector} representing~\(x\), then \(M(\varphi x)=M(x)M(\varphi)\).
\item \(M(\varphi\psi)=M(\psi)M(\varphi)\)
\end{itemize}
Compounding the insanity, Greub also has the annoying habit of indexing over columns instead of rows when working in dual spaces. This further increases the risk of confusion and error, as we see below. Greub says that ``it would be very undesirable\dots to agree once and for all to always let the subscript count the rows'', but we couldn't disagree more.
\end{warn}

\subsection*{\S~3}
\begin{rmk}
In subsection~3.13, note \((\alpha^{\mu}_{\nu})=M(\iota;\bar{x}_{\nu},x_{\mu})\) by~(3.22), \((\check{\alpha}^{\mu}_{\nu})=M(\iota;x_{\nu},\bar{x}_{\mu})\) by~(3.23), and \((\beta^{\varrho}_{\sigma})=M(\iota;\bar{x}^{*\varrho},x^{*\sigma})\) by~(3.24). It follows from~(3.4) that
\[\bigl(\beta^{\mu}_{\nu}\bigr)=\bigl(\check{\alpha}^{\mu}_{\nu}\bigr)^*=\bigl(\bigl(\alpha^{\mu}_{\nu}\bigr)^{-1}\bigr)^*\]
In other words, the matrix of the dual basis transformation \(x^{*\nu}\mapsto\bar{x}^{*\nu}\) in~\(E^*\) is the \emph{transpose} of the inverse of the matrix of the basis transformation \(x_{\nu}\mapsto\bar{x}_{\nu}\) in~\(E\), contrary to what the book says. It's easier to remember that the matrix of \(x^{*\nu}\mapsfrom\bar{x}^{*\nu}\) (arrow reversed!) is the transpose of the matrix of \(x_{\nu}\mapsto\bar{x}_{\nu}\).
\end{rmk}

\begin{rmk}
In subsection 3.13, we see that if a basis transformation is effected by~\(\tau\), then the corresponding \emph{coordinate} transformation is effected by~\(\tau^{-1}\). The coordinates of a vector are transformed ``exactly in the same way'' as the vectors of the dual basis, despite the previous remark, because of Greub's notational choices, which introduce transposition into matrix-vector multiplication (see the remarks above).
\end{rmk}

\section*{Chapter~IV}
\begin{rmk}
In this chapter, it is implicitly assumed that all vector spaces have dimension \(n\ge 1\), except in the definition of intersection number (subsection 4.31) where \(n=0\). Here we summarize results for the case \(n=0\):
\begin{itemize}
\item For a set~\(X\), \(X^0=\{\emptyset\}\). Therefore maps \(\Phi:X^0\to Y\) can be identified with elements of~\(Y\).
\item For vector spaces \(E\) and~\(F\), a map \(\Phi:E^0\to F\) is vacuously \(0\)-linear. Since the only permutation in~\(S_0\) is the identity \(\iota=\emptyset\), \(\Phi\)~is also trivially skew symmetric.
\end{itemize}
In particular if \(E=0\), the following results hold:
\begin{itemize}
\item Determinant functions in~\(E\) are just scalars in~\(\Gamma\), and dual determinant functions are just reciprocal scalars.
\item The only transformation of~\(E\) is the zero transformation, which is also the identity transformation. It has determinant~\(1\), trace~\(0\), and constant characteristic polynomial~\(1\). It has no eigenvalues or eigenvectors. Its adjoint is also the zero transformation. Its matrix on the empty basis is empty.
\item If \(E\)~is real (\(\Gamma=\R\)), the orientations in~\(E\) are represented by the scalars~\(\pm 1\), and determine whether the empty basis is positive or negative. The zero transformation is orientation preserving. The empty basis is deformable into itself.
\end{itemize}
\end{rmk}

\subsection*{\S~1}
\begin{rmk} To see why~(4.1) holds, observe by definition of~\(\tau(\sigma\Phi)\) that
\[(\tau(\sigma\Phi))(x_1,\ldots,x_p)=(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})\]
Let \(y_i=x_{\tau(i)}\). Then by definition of \(\sigma\Phi\) and~\((\tau\sigma)\Phi\),
\begin{align*}
(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})&=(\sigma\Phi)(y_1,\ldots,y_p)\\
	&=\Phi(y_{\sigma(1)},\ldots,y_{\sigma(p)})\\
	&=\Phi(x_{\tau(\sigma(1))},\ldots,x_{\tau(\sigma(p))})\\
	&=\Phi(x_{(\tau\sigma)(1)},\ldots,x_{(\tau\sigma)(p)})\\
	&=((\tau\sigma)\Phi)(x_1,\ldots,x_p)
\end{align*}
Therefore \(\tau(\sigma\Phi)=(\tau\sigma)\Phi\).
\end{rmk}

\begin{rmk}
By Proposition~I(iii) and Proposition~II, a determinant function \(\Delta\ne 0\) ``determines'' linear independence in the sense that \(\Delta(x_1,\ldots,x_n)\ne0\) if and only if \(x_1,\ldots,x_n\) are linearly independent. By~(4.8), it follows that \(\det\varphi\) ``determines'' whether a linear transformation~\(\varphi\) preserves linear independence, i.e. whether or not \(\varphi\)~is invertible.

Geometrically, \(\Delta(x_1,\ldots,x_n)\) measures the oriented (signed) volume of the \(n\)-dimensional parallelepiped determined by the vectors \(x_1,\ldots,x_n\). Therefore \(\det\varphi\) is the factor by which \(\varphi\)~changes oriented volume. Since a small change in the vectors \(x_1,\ldots,x_n\) results in a small change in the oriented volume, \(\Delta\)~is continuous.
\end{rmk}

\begin{rmk}
We provide an alternative proof of Proposition~IV. First note
\[(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)=\Delta(x_1,\ldots,x,\ldots,x_n)\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{\(\delete{x_j}\)~denotes deletion of \(x_j\)~from the sequence on the left.} Therefore
\[\sum_{j=1}^n(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)x_j=\Delta(x,x_2,\ldots,x_n)x_1+\cdots+\Delta(x_1,\ldots,x_{n-1},x)x_n\]
Viewing this as a function of \(x_1,\ldots,x_n\) (that is, a set map from \(E^n\to L(E;E)\)), it is obviously multilinear and skew symmetric (by Proposition~I(ii)). Therefore if \(x_1,\ldots,x_n\) are linearly dependent, it is zero (by Proposition~I(iii)). If \(x_1,\ldots,x_n\) are linearly independent (and hence a basis), then viewing it as a function of~\(x\), its value at~\(x_i\) is just \(\Delta(x_1,\ldots,x_n)x_i\) (by Proposition~I(ii)), so it agrees on a basis with \(\Delta(x_1,\ldots,x_n)x\) and hence is equal to it.
\end{rmk}

\begin{rmk}
Let \(E\)~be a vector space with \(\dim E=n>1\) and \(E_1\)~a subspace with \(\dim E_1=1\). Let \(\Delta\)~be a determinant function in~\(E\) with \(\Delta(e_1,\ldots,e_n)=1\) where \(e_1\in E_1\). Then \(\Delta\)~induces a determinant function~\(\Delta_1\) in~\(E/E_1\) by
\[\Delta_1(\,\overline{x_2},\ldots,\overline{x_n}\,)=\Delta(e_1,x_2,\ldots,x_n)\]
with \(\Delta_1(\,\overline{e_2},\ldots,\overline{e_n}\,)=1\). Define \(D:E^n\to\Gamma\) by
\[D(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{j-1}\pi_1(x_j)\Delta_1(\,\overline{x_1},\ldots,\widehat{\overline{x_j}},\ldots,\overline{x_n}\,)\]
where \(\pi_1:E\to\Gamma\) is the coordinate function for~\(e_1\). Then \(D\)~is skew symmetric and \(n\)-linear with \(D(e_1,\ldots,e_n)=1\), so \(D=\Delta\) by uniqueness (Proposition~III). Therefore
\[\boxed{\Delta(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{j-1}\pi_1(x_j)\Delta_1(\,\overline{x_1},\ldots,\widehat{\overline{x_j}},\ldots,\overline{x_n}\,)}\]
This result expresses a fundamental relationship between an \(n\)-dimensional determinant function and an \((n-1)\)-dimensional one. The cofactor expansion formulas for the determinant (subsection 4.15) follow immediately. Note that this relationship can also be exploited to recursively \emph{define} an \(n\)-dimensional determinant function in terms of an \((n-1)\)-dimensional one.

The geometrical fact that the volume of an \(n\)-dimensional parallelepiped is equal to the product of the volume of any \((n-1)\)-dimensional ``base'' and the corresponding ``height'' (subsection 7.15) is closely related to this result.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In subsection~4.6, we want a transformation~\(\psi\) with \(\psi\varphi=(\det\varphi)\iota\). We can choose a basis \(x_1,\ldots,x_n\) in~\(E\) with \(\Delta(x_1,\ldots,x_n)=1\), for which we want
\begin{align*}
(\psi\varphi)x_i=\psi(\varphi x_i)&=(\det\varphi)x_i\\
	&=(\det\varphi)\Delta(x_1,\ldots,x_n)x_i\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_n)x_i
\end{align*}
To obtain this, we can define
\[\psi(x)=\sum_{j=1}^n\Delta(\varphi x_1,\ldots,x,\ldots,\varphi x_n)x_j\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{See the remark on Proposition~IV above.} Then \(\psi\)~obviously satisfies the above properties, by multilinearity and skew symmetry of~\(\Delta\).

To obtain~\(\psi\) in a ``coordinate-free''  manner (without choosing a basis), we observe that the construction on the right is multilinear and skew symmetric in \(x_1,\ldots,x_n\) when viewed as a mapping \(\Phi:E^n\to L(E;E)\). By the universal property of~\(\Delta\) (Proposition~III), there is a unique \(\psi\in L(E;E)\) satisfying the above; this~\(\psi\) is also seen to be independent of the choice of~\(\Delta\).
\end{rmk}

\begin{rmk}
In subsection~4.7, observe that
\[\Delta(x_1,\ldots,x_p,y_1,\ldots,y_q)\]
induces a determinant function on~\(E_2\) when \(x_1,\ldots,x_p\in E\) are fixed, and induces a determinant function on~\(E_1\) when \(y_1,\ldots,y_q\in E\) are fixed. Now let \(a_1,\ldots,a_p\) be a basis of~\(E_1\), so \(a_1,\ldots,a_p,b_1,\ldots,b_q\) is a basis of~\(E\). Then by~(4.8),
\begin{align*}
\det\varphi\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)&=\Delta(\varphi_1 a_1,\ldots,\varphi_1 a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\Delta(a_1,\ldots,a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\det\varphi_2\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)
\end{align*}
Since \(\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)\ne0\), it follows that \(\det\varphi=\det\varphi_1\mult\det\varphi_2\). Note this result shows that
\[\det(\varphi_1\dsum\varphi_2)=\det\varphi_1\mult\det\varphi_2\]
\end{rmk}

\begin{exer}[2]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\overline{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\det\varphi=\det\varphi_1\mult\det\overline{\varphi}\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) where \(e_1,\ldots,e_p\) is a basis of~\(E_1\). Let \(\Delta\ne 0\) be a determinant function in~\(E\). First observe that
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,\varphi e_{p+1},\ldots,\varphi e_n)\tag{1}\]
is a determinant function in~\(E_1\) and
\[\Delta_2(\,\overline{x_{p+1}},\ldots,\overline{x_n}\,)=\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)\tag{2}\]
is a well-defined determinant function in~\(E/E_1\). Now
\[\det\overline{\varphi}\mult\Delta_2(\,\overline{x_{p+1}},\ldots,\overline{x_n}\,)=\Delta_2(\,\overline{\varphi}\ \overline{x_{p+1}},\ldots,\overline{\varphi}\ \overline{x_n}\,)=\Delta_2(\,\overline{\varphi x_{p+1}},\ldots,\overline{\varphi x_n}\,)\tag{3}\]
It follows from (2) and~(3) that
\[\det\overline{\varphi}\mult\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)=\Delta(e_1,\ldots,e_p,\varphi x_{p+1},\ldots,\varphi x_n)\tag{4}\]
Now
\begin{align*}
\det\varphi\mult\Delta(e_1,\ldots,e_n)&=\Delta(\varphi e_1,\ldots,\varphi e_n)&&\\
	&=\Delta_1(\varphi_1 e_1,\ldots,\varphi_1 e_p)&&\text{by~(1)}\\
	&=\det\varphi_1\mult\Delta_1(e_1,\ldots,e_p)&&\\
	&=\det\varphi_1\mult\det\overline{\varphi}\mult\Delta(e_1,\ldots,e_n)&&\text{by (1), (4)}
\end{align*}
Since \(\Delta(e_1,\ldots,e_n)\ne 0\), the result follows.
\end{proof}

\subsection*{\S~4}
\begin{rmk}
If \(A\)~is an \(n\times n\) matrix of the form
\[A=\begin{pmatrix}
A_1&\\
*&A_2
\end{pmatrix}\]
where \(A_1\)~is \(p\times p\) and \(A_2\)~is \((n-p)\times(n-p)\), then
\[\det A=\det A_1\mult\det A_2\tag{1}\]
Indeed, let \(E\)~be an \(n\)-dimensional vector space and \(\varphi:E\to E\) be defined by \(M(\varphi;e_1,\ldots,e_n)=A\), so \(\det A=\det\varphi\). Let \(E_1=\gen{e_1,\ldots,e_p}\) and \(E_2=\gen{e_{p+1},\ldots,e_n}\). Then \(E=E_1\dsum E_2\) and \(E_1\)~is stable under~\(\varphi\). If \(\varphi_1:E_1\to E_1\) is the induced map, then \(A_1=M(\varphi_1)\), so \(\det A_1=\det\varphi_1\). Dually, \(E^*=E_1^*\dsum E_2^*\) where \(E_1^*=\gen{e_1^*,\ldots,e_p^*}\) and \(E_2^*=\gen{e_{p+1}^*,\ldots,e_n^*}\), and \(E_2^*\)~is stable under~\(\varphi^*\) since
\[M(\varphi^*;e_1^*,\ldots,e_n^*)=A^*=\begin{pmatrix}
A_1^*&*\\
&A_2^*
\end{pmatrix}\]
If \(\varphi_2^*:E_2^*\to E_2^*\) is the induced map, then \(A_2^*=M(\varphi_2^*)\) and \(\det A_2=\det A_2^*=\det\varphi_2^*\). So we must prove that \(\det\varphi=\det\varphi_1\mult\det\varphi_2^*\).

Let \(\Delta\ne0\) be a determinant function in~\(E\) and \(\Delta^*\)~its dual in~\(E^*\). We claim
\begin{multline*}
\Delta^*(e_1^*,\ldots,e_n^*)\mult\Delta(e_1,\ldots,e_p,\varphi e_{p+1},\ldots,\varphi e_n)\\=\Delta(e_1,\ldots,e_n)\mult\Delta^*(e_1^*,\ldots,e_p^*,\varphi^* e_{p+1}^*,\ldots,\varphi^* e_n^*)\tag{2}
\end{multline*}
Indeed, by~(4.26) the left side of~(2) is a determinant of the form
\[\begin{vmatrix}
J&\\
*&B
\end{vmatrix}\]
where \(J\)~is the \(p\times p\) identity matrix and \(B=(\beta_i^j)\) with \(\beta_i^j=\sprod{e_j^*}{\varphi e_i}=\sprod{\varphi^* e_j^*}{e_i}\). However, since the determinant is multilinear in its rows,\footnote{See subsection~4.9, item~4.} it is equal to
\[\begin{vmatrix}
J&\\
&B
\end{vmatrix}\]
A similar argument shows that the same is true of the right side of~(2). Now
\begin{align*}
\det\varphi&=\det\varphi\mult\Delta(e_1,\ldots,e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\Delta(\varphi_1e_1,\ldots,\varphi_1e_p,\varphi e_{p+1},\ldots,\varphi e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\det\varphi_1\mult\Delta(e_1,\ldots,e_p,\varphi e_{p+1},\ldots,\varphi e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\det\varphi_1\mult\Delta^*(e_1^*,\ldots,e_p^*,\varphi_2^* e_{p+1}^*,\ldots,\varphi_2^* e_n^*)\mult\Delta(e_1,\ldots,e_n)&\text{by~(2)}\\
	&=\det\varphi_1\mult\det\varphi_2^*\mult\Delta^*(e_1^*,\ldots,e_n^*)\mult\Delta(e_1,\ldots,e_n)\\
	&=\det\varphi_1\mult\det\varphi_2^*
\end{align*}
The same result~(1) holds when \(A\)~has the form
\[A=\begin{pmatrix}
A_1&*\\
&A_2
\end{pmatrix}\]
Indeed, by the above,
\[\det A=\det A^*=\det A_1^*\mult\det A_2^*=\det A_1\mult\det A_2\]
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Recall that the system (4.39) is equivalent to \(\varphi x=y\) where \(\varphi:\Gamma^n\to\Gamma^n\) is defined by \(M(\varphi)=(\alpha^j_k)=A\), \(x=(\xi^i)\), and \(y=(\eta^j)\). If \(\det A\ne0\), then \(\varphi\)~is invertible and
\[x=\varphi^{-1}y=\frac{1}{\det A}\ad(\varphi)(y)\]
It follows from the analysis of the adjoint matrix in subsection~4.13 that
\[\xi^i=\frac{1}{\det A}\sum_j\cof(\alpha^j_i)\eta^j\]
Moreover, it follows from~(4.38) that \(\sum_j\cof(\alpha^j_i)\eta^j=\det A_i\) where \(A_i\)~is the matrix obtained from~\(A\) by replacing the \(i\)-th row with~\(y\).\footnote{The cofactors of \(A_i\) and~\(A\) along the \(i\)-th row agree since \(A_i\) and~\(A\) agree on the other rows.} Therefore
\[\xi^i=\frac{\det A_i}{\det A}\]
\end{rmk}

\begin{rmk}
In subsection~4.14, \(\det B^j_i=\det S^j_i\) does \emph{not} follow from~(4.38), which only tells us that \(\det B^j_i=\det B^j_i\). However, it follows from~(4.16), or from our remarks in~\S~4 above.
\end{rmk}

\subsection*{\S~6}
\begin{exer}[5]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\chi(\varphi_1\dsum\varphi_2)=\chi(\varphi_1)\mult\chi(\varphi_2)\]
where \(\chi(\varphi)\)~denotes the characteristic polynomial of~\(\varphi\).
\end{exer}
\begin{proof}
This follows from the result in subsection~4.7 and the fact that
\[\varphi_1\dsum\varphi_2-\lambda\iota=(\varphi_1-\lambda\iota_{E_1})\dsum(\varphi_2-\lambda\iota_{E_2})\qedhere\]
\end{proof}

\begin{exer}[6]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\overline{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\chi(\varphi)=\chi(\varphi_1)\mult\chi(\overline{\varphi})\]
\end{exer}
\begin{proof}
This follows from problem~2 in \S~2, the fact that \(\varphi-\lambda\iota_E\) restricted to~\(E_1\) is just \(\varphi_1-\lambda\iota_{E_1}\), and \(\overline{\varphi-\lambda\iota_E}=\overline{\varphi}-\lambda\iota_{E/E_1}\).
\end{proof}
\begin{rmk}
Taking \(E_1=\ker\varphi\), we have \(\chi(\varphi_1)=\chi(0_{E_1})=(-\lambda)^p\) where \(p=\dim E_1\), so \(\chi(\varphi)=(-\lambda)^p\chi(\overline{\varphi})\).
\end{rmk}

\begin{exer}[7]
A linear map \(\varphi:E\to E\) is nilpotent if and only if \(\chi(\varphi)=(-\lambda)^n\).
\end{exer}
\begin{proof}
If \(\varphi\)~is nilpotent, we proceed by induction on~\(k\) least such that \(\varphi^k=0\). If \(k=1\), the result is trivial. If \(k>1\), let \(E_1=\ker\varphi\) and \(\overline{\varphi}:E/E_1\to E/E_1\) the induced map. Then \(\overline{\varphi}^{k-1}=0\) since
\[\overline{\varphi}^{k-1}(\overline{x})=\overline{\varphi^{k-1}}(\overline{x})=\overline{\varphi^{k-1}(x)}=0\]
as \(\varphi^{k-1}(x)\in E_1\). By the induction hypothesis, \(\chi(\overline{\varphi})=(-\lambda)^{n-p}\) where \(p=\dim E_1\), so by the previous problem,
\[\chi(\varphi)=(-\lambda)^p(-\lambda)^{n-p}=(-\lambda)^n\]
Conversely, if \(\varphi\ne0\) and \(\chi(\varphi)=(-\lambda)^n\), then the constant term \(\det\varphi=0\), so \(p>0\) and by the previous problem \((-\lambda)^n=(-\lambda)^p\chi(\overline{\varphi})\), which implies \(\chi(\overline{\varphi})=(-\lambda)^{n-p}\). By induction, \(\overline{\varphi}\)~is nilpotent. If \(\overline{\varphi}^k=0\), then \(\varphi^{k+1}=0\), so \(\varphi\)~is nilpotent.
\end{proof}

\subsection*{\S~7}
\begin{exer}[12]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\), then
\[\tr(\varphi_1\dsum\varphi_2)=\tr\varphi_1+\tr\varphi_2\]
\end{exer}
\begin{proof}
Immediate since
\[M(\varphi_1\dsum\varphi_2)=\begin{pmatrix}
M(\varphi_1)&\\
&M(\varphi_2)
\end{pmatrix}\qedhere\]
\end{proof}

\subsection*{\S~8}
\begin{rmk}
In~(4.68), if instead we define
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,e_{p+1},\ldots,e_n)\]
then \(\Delta_1\)~represents the original orientation in~\(E_1\). Indeed, in this case
\[\Delta_1(e_1,\ldots,e_p)=\Delta(e_1,\ldots,e_p,e_{p+1},\ldots,e_n)=\Delta_2(e_{p+1},\ldots,e_n)>0\]
\end{rmk}

\section*{Chapter~V}
\subsection*{\S~1}
\begin{rmk}
An algebra~\(A\) is a \emph{zero algebra} if \(xy=0\) for all \(x,y\in A\); this is equivalent to \(A^2=0\). As an example, \emph{the zero algebra} is the algebra \(A=0\). A zero algebra is unital if and only if it is the zero algebra.
\end{rmk}

\begin{rmk}
Let \(\varphi:A\to B\) be a homomorphism of algebras. If \(A_1\)~is a subalgebra of~\(A\) and \(B_1\)~is a subalgebra of~\(B\) and \(\varphi(A_1)\subseteq B_1\), then the restriction \(\varphi_1:A_1\to B_1\) of~\(\varphi\) to \(A_1,B_1\) is a homomorphism.

If \(A_1\) and~\(B_1\) are \emph{ideals}, then the induced linear map \(\overline{\varphi}:A/A_1\to B/B_1\) is also a homomorphism since
\[\overline{\varphi}(\overline{x}\,\overline{y})=\overline{\varphi}(\overline{xy})=\overline{\varphi(xy)}=\overline{\varphi(x)\varphi(y)}=\overline{\varphi(x)}\,\overline{\varphi(y)}=\overline{\varphi}(\overline{x})\,\overline{\varphi}(\overline{y})\]
\end{rmk}

\noindent In the problems below, \(E\)~is a finite-dimensional vector space.
\begin{exer}[12]
The mapping
\[\Phi:A(E;E)\to\opp{A(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
\(\Phi\)~is a linear isomorphism by problem 9 of chapter II, \S~6, and preserves products since \((\varphi\after\psi)^*=\psi^*\after\varphi^*\).
\end{proof}

\begin{exer}[16]
Every algebra automorphism \(\Phi:A(E;E)\to A(E;E)\) is an \emph{inner} automorphism; that is, there exists \(\alpha\in GL(E)\) such that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{exer}
\begin{proof}
First, observe that every basis~\((e_i)\) of~\(E\) induces a basis~\((\varphi_{ij})\) of~\(A(E;E)\) defined by \(\varphi_{ij}(e_k)=\delta_{jk}e_i\). This basis satisfies
\[\varphi_{ij}\varphi_{lk}=\delta_{jl}\varphi_{ik}\qquad\text{and}\qquad\sum_i\varphi_{ii}=\iota\tag{1}\]
Conversely, every basis satisfying these properties is induced by a basis of~\(E\) in this manner (see problem~14). Moreover, any two of these bases are conjugate to each other via the change of basis transformation between their inducing bases of~\(E\) (see problem~15).

Now fix \((e_i)\) and~\((\varphi_{ij})\) as above. Since \(\Phi\)~is an automorphism, \((\Phi(\varphi_{ij}))\)~is also a basis of~\(A(E;E)\) which satisfies~(1), so there is \(\alpha\in GL(E)\) with \(\Phi(\varphi_{ij})=\alpha\varphi_{ij}\alpha^{-1}\) for all \(i,j\). It follows that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{proof}
\begin{rmk}
The result is true for any nonzero endomorphism~\(\Phi\), since \(A(E;E)\)~is simple (see subsection~5.12).
\end{rmk}

\section*{Chapter~VI}
\subsection*{\S~1}
\begin{rmk}
The space of polynomials in one variable is positively graded by the degrees of monomials. More generally, the space of polynomials in \(p\)~variables is \(p\)-graded by the multidegrees of monomials.
\end{rmk}

\begin{rmk}
If \(E=\sum_{k\in G}E_k\) is a \(G\)-graded space and \(F=\sum_{k\in G}F\sect E_k\) is a \(G\)-graded subspace, then \(E/F=\sum_{k\in G}E_k/(F\sect E_k)\) is the \(G\)-graded factor space.\footnote{See problem~2 in Chapter~II, \S~4.}
\end{rmk}

\begin{rmk}
The zero map between two \(G\)-graded vector spaces is homogeneous of every degree. A nonzero homogeneous map has a unique degree.
\end{rmk}

\begin{rmk}
Let \(E\) and~\(F\) be \(G\)-graded vector spaces. If \(\varphi:E\to F\) is linear and homogeneous of degree~\(k\) and \(\varphi x\) is homogeneous of degree~\(l\), then we may assume without loss of generality that \(x\)~is homogeneous of degree \(l-k\). Indeed, writing \(x=\sum_m x_m\) with \(\deg x_m=m\), we have \(\varphi x=\sum_m\varphi x_m\) with \(\deg(\varphi x_m)=m+k\). Since \(\deg(\varphi x)=l\), we must have \(\varphi x_m=0\) for \(m\ne l-k\), so \(\varphi x=\varphi x_{l-k}\).
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional \(G\)-graded vector space and \(\varphi:E\to E\) is linear and homogeneous with \(\deg\varphi\ne 0\), then \(\tr\varphi=0\).
\end{rmk}
\begin{proof}
Write \(E=E_{k_1}\dsum\cdots\dsum E_{k_n}\) with \(k_i\in G\) and \(d_i=\dim E_{k_i}<\infty\). Let \((e_{ij})\)~be a basis of~\(E\) such that for each \(1\le i\le n\), \((e_{ij})\)~is a basis of~\(E_{k_i}\) for \(1\le j\le d_i\). Let \(\Delta\)~be a determinant function in~\(E\) with \(\Delta(e_{ij})=1\). Then
\[\tr\varphi=\sum_{i,j}\Delta(e_{11},\ldots,e_{1d_1},\ldots,\varphi(e_{ij}),\ldots,e_{n1},\ldots,e_{n\,d_n})\]
By assumption, \(\varphi(e_{ij})\in E_{k_l}\) for some \(l\ne i\), so each term in this sum is zero, and hence \(\tr\varphi=0\).
\end{proof}
\noindent As an example, formal differentiation in the space of polynomials of degree at most~\(n\) (graded by the degrees of monomials) is homogeneous of degree~\(-1\), so has zero trace. This is also obvious from its matrix representation with respect to the standard basis.

\begin{exer}[6]
Let \(E,E^*\) and \(F,F^*\) be pairs of dual \(G\)-graded vector spaces and let \(\varphi:E\to F\) and \(\varphi^*:E^*\from F^*\) be dual linear maps. If \(\varphi\)~is homogeneous of degree~\(k\), then \(\varphi^*\)~is homogeneous of degree~\(-k\).
\end{exer}
\begin{proof}
We have direct sum decompositions
\[E=\sum_{m\in G}E_m\qquad E^*=\sum_{m\in G}E^{*m}\]
and
\[F=\sum_{n\in G}F_n\qquad F^*=\sum_{n\in G}F^{*n}\]
where the pairs \(E_m,E^{*m}\) and \(F_n,F^{*n}\) are dual for all \(m,n\) under the restrictions of the scalar products between \(E,E^*\) and \(F,F^*\), respectively (see subsection~6.5). We also have \(\varphi E_m\subseteq F_{m+k}\) for all~\(m\). We must prove \(\varphi^* F^{*n}\subseteq E^{*n-k}\) for all~\(n\).

Let \(y^*\in F^{*n}\) and \(x\in E\). Write \(x=\sum_m x_m\) where \(x_m\in E_m\). Then
\[\sprod{\varphi^* y^*}{x}=\sprod{y^*}{\varphi x}=\sum_m\sprod{y^*}{\varphi x_m}=\sprod{y^*}{\varphi x_{n-k}}=\sprod{\varphi^* y^*}{x_{n-k}}\]
which implies
\[\sprod{\varphi^* y^*}{x-\pi_{n-k}x}=0\tag{1}\]
where \(\pi_{n-k}:E\to E_{n-k}\) is the canonical projection. Now write \(\varphi^* y^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*m}=0\) for all \(m\ne n-k\). Indeed, for \(m\ne n-k\) and \(x\in E_m\) we have \(\pi_{n-k}x=0\), so by~(1)
\[\sprod{x^{*m}}{x}=\sum_p\sprod{x^{*p}}{x}=\sprod{\varphi^* y^*}{x}=0\]
Therefore \(x^{*m}=0\). It follows that \(\varphi^* y^*=x^{*n-k}\in E^{*n-k}\), as desired.
\end{proof}

\begin{exer}[8]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. If \(F\)~is a \(G\)-graded subspace of~\(E\), then \(\oc{F}\)~is a \(G\)-graded subspace of~\(E^*\) and \(\occ{F}=F\).
\end{exer}
\begin{proof}
We have direct sums \(E=\sum_{m\in G}E_m\) and \(E^*=\sum_{m\in G}E^{*m}\) where the pairs \(E_m,E^{*m}\) are dual under the restrictions of the scalar product between \(E,E^*\) and \(\dim E_m=\dim E^{*m}<\infty\) for all~\(m\). By assumption, \(F=\sum_{m\in G}F\sect E_m\).
We must prove
\[\oc{F}=\sum_{m\in G}\oc{F}\sect E^{*m}\tag{1}\]
Let \(x^*\in\oc{F}\) and write \(x^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*n}\in\oc{F}\) for all~\(n\). Indeed, if \(x\in F\), write \(x=\sum_m x_m\) where \(x_m\in F\sect E_m\). Then
\[\sprod{x^{*n}}{x}=\sum_m\sprod{x^{*n}}{x_m}=\sprod{x^{*n}}{x_n}=\sum_m\sprod{x^{*m}}{x_n}=\sprod{x^*}{x_n}=0\]
This establishes~(1). By symmetry, we have
\[\occ{F}=\sum_{m\in G}\occ{F}\sect E_m\tag{2}\]
We claim \(\occ{F}\sect E_n\subseteq F\sect E_n\) for all~\(n\). To prove this, we first show
\[\occ{F}\sect E_n\subseteq(F\sect E_n)^{\perp_n\perp_n}\tag{3}\]
where \(\perp_n\)~is taken relative to the scalar product between \(E_n,E^{*n}\). Indeed, let \(x\in\occ{F}\sect E_n\) and \(x^*\in(F\sect E_n)^{\perp_n}\subseteq E^{*n}\). If \(y\in F\), write \(y=\sum_m y_m\) where \(y_m\in F\sect E_m\). Then
\[\sprod{x^*}{y}=\sum_m\sprod{x^*}{y_m}=\sprod{x^*}{y_n}=0\]
This implies \(x^*\in\oc{F}\), which implies \(\sprod{x^*}{x}=0\), which in turn implies~(3). Now \((F\sect E_n)^{\perp_n\perp_n}=F\sect E_n\) since \(\dim E_n<\infty\), which establishes the claim. Finally, it follows from~(2) that \(\occ{F}=F\).
\end{proof}

\subsection*{\S~2}
\begin{exer}[1]
Let \(A\)~be a \(G\)-graded algebra. If \(x\in A\) is an invertible element homogeneous of degree~\(k\), then \(x^{-1}\)~is homogeneous of degree~\(-k\). If \(A\)~is nonzero and positively graded, then \(k=0\).
\end{exer}
\begin{proof}
Write \(A=\sum_{m\in G}A_m\) and \(x^{-1}=\sum_m x_m\) with \(x_m\in A_m\). Then
\[e=xx^{-1}=\sum_m xx_m\]
Since \(\deg e=0\) and \(\deg(xx_m)=m+k\), it follows that \(xx_m=0\) for all \(m\ne-k\). Therefore \(e=xx_{-k}\) and \(x^{-1}=x_{-k}\), so \(x^{-1}\)~is homogeneous of degree~\(-k\).

If \(A\ne 0\), then \(x\ne 0\) and \(x^{-1}\ne 0\), so \(A_k\ne 0\) and \(A_{-k}\ne 0\). If \(A\)~is positively graded, this forces \(k=0\).
\end{proof}

\begin{exer}[4]
Let \(E\)~be a \(G\)-graded vector space. Then the subspace \(A_G(E;E)\) of~\(A(E;E)\) generated by homogeneous linear transformations of~\(E\) forms a \(G\)-graded subalgebra of~\(A(E;E)\).
\end{exer}
\begin{proof}
First observe that \(A_G(E;E)\)~is naturally graded as a vector space by the degrees of homogeneous transformations (see problem~3). If \(\varphi,\psi\in A_G(E;E)\) are homogeneous with \(\deg\varphi=m\) and \(\deg\psi=n\), then it is obvious that \(\varphi\psi\)~is homogeneous with \(\deg(\varphi\psi)=m+n\). It follows from this that \(A_G(E;E)\)~is a \(G\)-graded subalgebra.
\end{proof}

\begin{exer}[7]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. Then the mapping
\[\Phi:A_G(E;E)\to\opp{A_G(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
\(\Phi\)~is well defined by problems 6 and~10 of~\S~1, and is an isomorphism by problem~12 of chapter~V, \S~1.
\end{proof}

\section*{Chapter~VII}
\textbf{In this chapter, all vector spaces are real.}

\subsection*{\S~1}
\begin{rmk}
The Riesz representation theorem shows that for a finite-dimensional \emph{inner product space}~\(E\), there is a \emph{natural} isomorphism between \(E\) and its dual space~\(L(E)\) given by \(x\mapsto\iprod{x}{-}\). This is unlike for a finite-dimensional vector space, where the isomorphism is in general non-natural, and means we may naturally \emph{identify} a vector with its dual vector.
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional inner product space and \(E_1\)~is a subspace of~\(E\), then \(E\)~induces an inner product in~\(E/E_1\) by
\[\iprod{\overline{x}}{\overline{y}}=\iprod{x}{y}\]
where \(x,y\) are the unique representatives of \(\overline{x},\overline{y}\) in~\(\oc{E_1}\) (see problem~5).
\end{rmk}
\begin{proof}
The only thing to check is bilinearity, which follows from the fact that \(\oc{E_1}\)~is a subspace of~\(E\).
\end{proof}

\begin{exer}[5]
If \(E\)~is a finite-dimensional inner product space and \(E_1\)~is a subspace of~\(E\), then every element of~\(E/E_1\) has exactly one representative in~\(\oc{E_1}\).
\end{exer}
\begin{proof}
By duality, \(\dim E=\dim E_1+\dim\oc{E_1}\), and by definiteness of the inner product, \(E_1\sect\oc{E_1}=0\), so \(E=E_1\dsum\oc{E_1}\). For \(x+E_1\in E/E_1\), let \(y=\pi(x)-x\) where \(\pi\)~is the canonical projection onto~\(\oc{E_1}\). Then \(y\in E_1\) so \(x+y\in x+E_1\) and \(x+y\in\oc{E_1}\), as desired. If \(z\in E_1\) and \(x+z\in\oc{E_1}\), then
\[y-z=(y+x)-(x+z)\in E_1\sect\oc{E_1}=0\]
so \(y=z\), establishing uniqueness.
\end{proof}

\subsection*{\S~2}
\begin{rmk}
Let \(E\)~be an inner product space of finite dimension~\(n\). We provide an inductive proof of the existence of an orthonormal basis in~\(E\).

For \(n=0,1\), the result is trivial. For \(n>1\), fix a unit vector \(e_1\in E\) and let \(E_1=\gen{e_1}\). By induction, there is an orthonormal basis \(\overline{e_2},\ldots,\overline{e_n}\) in the induced inner product space \(E/E_1\) (see above). Letting \(e_2,\ldots,e_n\) be the representatives in~\(\oc{E_1}\), it follows that \(e_1,\ldots,e_n\) is an orthonormal basis in~\(E\).
\end{rmk}

\begin{rmk}
In the Gram-Schmidt process, we can just let \(e_1=a_1/\norm{a_1}\) and
\[e_k=\frac{a_k-\iprod{a_k}{e_1}e_1-\cdots-\iprod{a_k}{e_{k-1}}e_{k-1}}{\norm{a_k-\iprod{a_k}{e_1}e_1-\cdots-\iprod{a_k}{e_{k-1}}e_{k-1}}}\qquad(k=2,\ldots,n)\]
At each step, we compute the difference between the current vector~\(a_k\) and its orthogonal projection onto the subspace generated by the previous vectors, then normalize (compare (7.19)).
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional inner product space and \(\varphi:E\to E\) is linear, then the dual transformation \(\varphi^*:E\to E\) satisfies
\[\iprod{\varphi^* x}{y}=\iprod{x}{\varphi y}\]
If \(\varphi\)~preserves inner products, then it also preserves orthonormal bases and is invertible. In this case, \(\varphi^{-1}\)~also preserves inner products, so
\[\iprod{\varphi^{-1}x}{y}=\iprod{\varphi^{-1}x}{\varphi^{-1}\varphi y}=\iprod{x}{\varphi y}\]
and it follows that \(\varphi^{-1}=\varphi^*\). Conversely if \(\varphi^{-1}=\varphi^*\), then
\[\iprod{\varphi x}{\varphi y}=\iprod{\varphi^*\varphi x}{y}=\iprod{x}{y}\]
so \(\varphi\)~preserves inner products.

Such a~\(\varphi\) is called an \emph{orthogonal} transformation. Note \(\varphi\)~is orthogonal if and only if \(M(\varphi)\)~is an orthogonal matrix relative to an orthonormal basis.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
A determinant function~\(\Delta\) in an inner product space~\(E\) is normed if and only if \(\abs{\Delta(e_1,\ldots,e_n)}=1\) for any orthonormal basis \(e_1,\ldots,e_n\). If \(E\)~is oriented and \(\Delta\)~is the normed determinant function representing the orientation, then \(\Delta(e_1,\ldots,e_n)=1\) if \(e_1,\ldots,e_n\) is positive. Geometrically, this is just \emph{the} oriented volume function in~\(E\). By comparing (4.26) and~(7.23), we see that \emph{the normed determinant functions are precisely the self-dual determinant functions}.

If \(\Delta_0\ne 0\) is a determinant function in~\(E\) and \(\Delta_0^*\)~is its dual, then \(\Delta_0^*=\alpha\Delta_0\) for some real number~\(\alpha\) by uniqueness of~\(\Delta_0\), so
\[\alpha\Delta_0(x_1,\ldots,x_n)\Delta_0(y_1,\ldots,y_n)=\det\iprod{x_i}{y_j}\qquad(x_i,y_i\in E)\]
Substituting \(x_i=y_i=e_i\) shows that \(\alpha>0\). It follows that \(\Delta_1=\pm\sqrt{\alpha}\mult\Delta_0\) is a normed determinant function.
\end{rmk}

\begin{rmk}
It follows from~(7.37) that if \(x\) and~\(y\) are linearly independent, then
\[\norm{x\cross y}=\Delta(x,y,z)\]
where \(z=(x\cross y)/\norm{x\cross y}\). Since \(z\)~is a unit vector orthogonal to \(x\) and~\(y\), it follows (subsection 7.15) that \(\Delta(x,y,z)\)~is just the area of the parallelogram determined by \(x\) and~\(y\). In other words,
\[\norm{x\cross y}=\norm{x}\norm{y}\sin\theta\]
where \(0\le\theta\le\pi\) is the angle between \(x\) and~\(y\). This last equation obviously still holds if \(x\) and~\(y\) are linearly dependent but nonzero.
\end{rmk}

\begin{exer}[12]
For vectors \(x_1,\ldots,x_p\),
\[G(x_1,\ldots,x_p)\le\norm{x_1}^2\cdots\norm{x_p}^2\tag{1}\]
Additionally
\[\begin{vmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nn}
\end{vmatrix}^2\le\sum_{k=1}^n\abs{a_{1k}}^2\cdots\sum_{k=1}^n\abs{a_{nk}}^2\tag{2}\]
\end{exer}
\begin{proof}
By induction with the ``base times height'' rule for volume (subsection 7.15), \(V(u_1,\ldots,u_p)\le 1\) for linearly independent \emph{unit} vectors \(u_1,\ldots,u_p\).

Now if \(x_1,\ldots,x_p\) are linearly dependent, then \(G(x_1,\ldots,x_p)=0\) and (1)~holds trivially. Otherwise, setting \(u_i=x_i/\norm{x_i}\) we have
\[\sqrt{G(x_1,\ldots,x_p)}=V(x_1,\ldots,x_p)=\norm{x_1}\cdots\norm{x_p}\mult V(u_1,\ldots,u_p)\le\norm{x_1}\cdots\norm{x_p}\]
Squaring both sides yields~(1). Since the determinant of a matrix is a (normed) determinant function of the rows, (2)~follows.
\end{proof}
\begin{rmk}
These results both simply express an upper bound for the volume of a parallelepiped in terms of the lengths of the edges.
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The covariant components of a vector are just its coordinates in the dual space, which are also just the entries of its matrix (as a linear function). They are called ``covariant'' because they vary in the same way as basis vectors in the original space under a change of basis (see subsections 3.13--14). This is unlike the regular components of the vector, which vary inversely to the basis vectors and may be called the \emph{contravariant components}.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
In subsection~7.22, observe that \(Q\)~is just the set of unit vectors under the norm defined in example~3 in subsection~7.20. Since norm functions are continuous under the natural topology, \(Q\)~is closed under this topology, and \(Q\)~is also clearly bounded under this topology. It follows that \(Q\)~is compact under this topology since \(E\)~is finite-dimensional.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(x=\lambda e+x_1\) and \(y=\mu e+y_1\) are quaternions (\(\lambda,\mu\in\R\) and \(x_1,y_1\in E_1\)), then by the definition of quaternion multiplication
\[xy=\bigl(\lambda\mu-\iprod{x_1}{y_1}\bigr)e+\lambda y_1+\mu x_1+x_1\cross y_1\]
\end{rmk}

\begin{rmk}
In subsection~7.24, in the proof of Lemma~I, observe that the result holds trivially if \(y=\pm x\) by taking \(\lambda=\mp 1\). If \(y\ne\pm x\), then \(e\ne 0\). Suppose
\[\alpha x+\beta y+\gamma e=0\qquad(\alpha,\beta,\gamma\in\R)\]
If \(\alpha\ne 0\), then \(x=\beta_1y+\gamma_1e\) for \(\beta_1,\gamma_1\in\R\), so
\[-e=x^2=(\beta_1y+\gamma_1e)^2=2\beta_1\gamma_1y+(\gamma_1^2-\beta_1^2)e\]
which implies
\[2\beta_1\gamma_1y=(\beta_1^2-\gamma_1^2-1)e\]
If \(\beta_1=0\), it follows that \(\gamma_1^2=-1\), which is impossible. If \(\gamma_1=0\), it follows that \(\beta_1=\pm 1\), so \(x=\pm y\) contrary to assumption. Therefore \(\beta_1\gamma_1\ne 0\), so \(y=\delta e\) for \(\delta\in\R\). But then \(-e=y^2=\delta^2 e\), so \(\delta^2=-1\), which is impossible. It follows that \(\alpha=0\). Similarly \(\beta=0\). Finally, \(\gamma=0\). This result shows that the vectors \(x,y,e\) are linearly independent.

Now \(x+y\) and \(x-y\) are roots of polynomials of degree \(1\) or~\(2\), but the linear independence of \(x,y,e\) implies that these polynomials must have degree~\(2\), so (7.60) and~(7.61) follow.
\end{rmk}

\section*{Chapter~VIII}
\textbf{In this chapter, all vector spaces are real and finite-dimensional.}

\begin{rmk}
In this chapter, it is useful to think intuitively of transformations like complex numbers, which induce transformations of the complex plane through multiplication. Under this analogy, adjoints correspond to complex conjugates; self-adjoint transformations, to real numbers; positive transformations, to nonnegative real numbers; and isometries, to complex numbers on the unit circle.
\end{rmk}

\subsection*{\S~1}
\begin{rmk}
In subsection~8.2, if the bases \((x_{\nu})\) and~\((y_{\mu})\) are orthonormal, then they are self-dual,\footnote{See problem~2 in Chapter~VII, \S~2.} so \(M(\adj{\varphi},y_{\mu},x_{\nu})=M(\varphi,x_{\nu},y_{\mu})^*\) by~(3.4). It follows that
\[\tilde{\alpha}^{\varrho}_{\mu}=M(\adj{\varphi})^{\varrho}_{\mu}=M(\varphi)^{\mu}_{\varrho}=\alpha^{\mu}_{\varrho}\]
\end{rmk}

\begin{rmk}
Recall from subsection~7.18 the natural isomorphism \(\tau:E\to L(E)\) given by \(x\mapsto\iprod{x}{-}\), which maps each vector to its dual vector. In subsection~8.4, observe that there is a natural isomorphism
\[B(E,E)\iso L(E;L(E))\]
given by \(\Phi\mapsto(x\mapsto(y\mapsto\Phi(x,y)))\). For a linear transformation \(\varphi:E\to E\) and the bilinear function~\(\Phi\) defined by \(\Phi(x,y)=\iprod{\varphi x}{y}\), the linear map corresponding to~\(\Phi\) under this isomorphism is~\(\tau\varphi\):
\begin{diagram}[nohug]
x	&\rTo^{\varphi}		&\varphi x\\
	&\rdTo<{\Phi}		&\dTo>{\tau}\\
	&					&\iprod{\varphi x}{-}
\end{diagram}
In this sense, \(\Phi\)~is naturally dual to~\(\varphi\). Properties of~\(\Phi\) naturally correspond to those of~\(\varphi\); for example, \(\Phi\)~is symmetric if and only if \(\varphi\) is symmetric (self-adjoint), and \(\Phi\)~is skew symmetric if and only if \(\varphi\)~is skew symmetric.
\end{rmk}

\begin{rmk}
In subsection~8.5, observe for a direct sum \(E=\sum E_i\) and \(\varphi=\sum\varphi_i\) with \(\varphi_i:E_i\to E_i\), if \(E_i\)~is stable under~\(\adj{\varphi}\) then the restriction of~\(\adj{\varphi}\) to~\(E_i\) is the adjoint of~\(\varphi_i\). In other words, if \(\adj{\varphi}_i\)~denotes the restriction, then
\[\adj{\varphi}_i=\adj{\varphi_i}\]
Indeed, for \(x,y\in E_i\),
\[\iprod{\adj{\varphi}_i x}{y}=\iprod{\adj{\varphi}x}{y}=\iprod{x}{\varphi y}=\iprod{x}{\varphi_i y}\]
If additionally the \(E_i\)~are orthogonal, we see that \(\varphi\)~is normal if and only if each \(\varphi_i\)~is normal.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In subsection~8.6, consider \(E=\R^n\). Since \(e_1\)~minimizes the quadratic form \(q(x)=\iprod{x}{\varphi x}\) subject to the constraint \(h(x)=1-\iprod{x}{x}=0\), there exists a Lagrange multiplier \(\lambda\in\R\) such that
\[\grad q(e_1)+\lambda\grad h(e_1)=0\]
Now \(\grad h(e_1)=-2e_1\), and since \(\varphi\)~is self-adjoint it follows that \(\grad q(e_1)=2\varphi e_1\). Therefore \(\varphi e_1=\lambda e_1\), so \(e_1\)~is an eigenvector of~\(\varphi\) with eigenvalue~\(\lambda\).
\end{rmk}

\begin{exer}[5]
Every positive transformation~\(\varphi\) has a unique positive square root (that is, a positive transformation~\(\psi\) such that \(\psi^2=\varphi\)).
\end{exer}
\begin{proof}
There is an orthonormal basis \(e_1,\ldots,e_n\) of eigenvectors of~\(\varphi\), so that \(\varphi e_i=\lambda_i e_i\) for \(\lambda_i\in\R\). Now \(\lambda_i=\iprod{e_i}{\varphi e_i}\ge 0\), so setting \(\psi e_i=\sqrt{\lambda_i}e_i\) it follows that \(\psi\)~is positive with \(\psi^2=\varphi\).

If \(\psi_1\)~is positive with \(\psi_1^2=\varphi\), then the eigenvalues of~\(\psi_1\) must be \(\sqrt{\lambda_i}\). Also, if \(\psi_1x=\sqrt{\lambda_i}x\), then \(\varphi x=\lambda_i x\), so \(E_{\psi_1}(\sqrt{\lambda_i})\subseteq E_{\varphi}(\lambda_i)\). By~(8.21), it follows that \(E_{\psi_1}(\sqrt{\lambda_i})=E_{\varphi}(\lambda_i)\), so \(\psi_1 e_i=\sqrt{\lambda_i}e_i\) and \(\psi_1=\psi\).
\end{proof}

\subsection*{\S~3}
\begin{rmk}
In subsection~2.19, we see that a linear transformation~\(\pi\) is a projection operator (that is, \(\pi^2=\pi\)) if and only if \(\pi=0_{\ker\pi}\dsum\iota_{\im\pi}\). In subsection~8.11, we see that \(\pi\)~is additionally an \emph{orthogonal} projection if and only if \(\ker\pi\orth\im\pi\).
\end{rmk}

\subsection*{\S~4}
\begin{exer}[2 - Skew transformations of 3-space]
Let \(E\)~be an oriented Euclidean 3-space.
\begin{enumerate}
\item[(i)] For \(a\in E\), \(\varphi_a(x)=a\cross x\) is a skew transformation of~\(E\).
\item[(ii)] For \(a,b\in E\), \(\varphi_{a\cross b}=\varphi_a\varphi_b-\varphi_b\varphi_a\).
\item[(iii)] If \(\varphi\)~is a skew transformation of~\(E\), then \(\varphi=\varphi_a\) for a unique \(a\in E\).
\item[(iv)] The vector~\(a\) in~(iii) is given by
\[a=\alpha_{23}e_1+\alpha_{31}e_2+\alpha_{12}e_3\]
where \(e_1,e_2,e_3\) is a positive orthonormal basis of~\(E\) and \((\alpha_{ij})=M(\varphi;e_i)\).
\item[(v)] If \(a\ne 0\), then \(\ker\varphi_a=\gen{a}\) and \(\oc{a}\)~is stable under~\(\varphi_a\).
\item[(vi)] If \(e_1,e_2\) are normal such that \(e_1,e_2,a\) is a positive orthogonal basis of~\(E\), then
\[M(\varphi_a;e_1,e_2,a)=\begin{pmatrix}
0&\norm{a}&0\\
-\norm{a}&0&0\\
0&0&0
\end{pmatrix}\]
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] It is a transformation since the cross product is bilinear, and it is skew since \(\iprod{x}{a\cross x}=0\) for all \(x\in E\).
\item[(ii)] By the vector triple product formula~(7.41),
\[(a\cross b)\cross x=a\cross(b\cross x)-b\cross(a\cross x)\]
\item[(iii)] For uniqueness, note that if \(\varphi_a=\varphi_b\), then \(a\cross x=b\cross x\) for all \(x\in E\), so \((a-b)\cross x=0\) for all \(x\in E\), so \(a-b=0\) and \(a=b\).

For existence, note that the bilinear function \(\iprod{\varphi x}{y}\) is skew symmetric, and hence a determinant function in any plane in~\(E\). Define
\[\Phi(x,y,z)=\iprod{\varphi y}{z}x+\iprod{\varphi z}{x}y+\iprod{\varphi x}{y}z\]
Then \(\Phi\)~is clearly trilinear and skew symmetric. By the universal property of determinants if \(\Delta\)~is the normed determinant function representing the orientation in~\(E\) and \(e_1,e_2,e_3\) is an orthonormal basis with \(\Delta(e_1,e_2,e_3)=1\), then \(\Phi=\Delta a\) where \(a=\Phi(e_1,e_2,e_3)\).\footnote{See Proposition~III in subsection~4.3.} By direct computation it is easily verified that \(a\cross e_i=\varphi e_i\), so \(\varphi=\varphi_a\).
\item[(iv)] By the proof of~(iii), noting that \(a=\Phi(e_1,e_2,e_3)\) and \(\alpha_{ij}=\iprod{\varphi e_i}{e_j}\).
\item[(v)] By the fact that \(a\cross x=0\) if and only if \(a\) and~\(x\) are linearly dependent.
\item[(vi)] By direct computation.\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
The definition of~\(\Phi\) in~(iii) is motivated by the fundamental relationship between \(n\)-dimensional and \((n-1)\)-dimensional determinant functions discussed in \S~1 of Chapter~IV above. If \(\varphi\)~is skew, then its dual bilinear function \(\Delta_1(x,y)=\iprod{\varphi x}{y}\) is a determinant function in any plane in~\(E\). If \(\varphi=\varphi_a\), then
\[\Delta_1(x,y)=\iprod{a\cross x}{y}=\Delta(a,x,y)\]
where \(\Delta\)~is the normed determinant function representing the orientation in~\(E\). In other words, \(\Delta_1\)~is just the 2-dimensional area function induced by the 3-dimensional volume function~\(\Delta\) and the vector~\(a\). Therefore if we \emph{define} a 3-dimensional volume map~\(\Phi\) in terms of~\(\Delta_1\), we should expect that \(\Phi=\Delta a\).

This result, which shows that any skew linear \emph{transformation} of the space can be represented by a unique vector under the \emph{cross} product, is analogous to the Riesz representation theorem, which shows that any linear \emph{function} of the space can be represented by a unique vector under the \emph{inner} product.
\end{rmk}

\begin{exer}[3]
If \(\varphi\ne 0\) and~\(\psi\) are skew transformations of an oriented Euclidean 3-space with \(\ker\varphi\subseteq\ker\psi\), then \(\psi=\lambda\varphi\) for some \(\lambda\in\R\).
\end{exer}
\begin{proof}
By the previous problem, we can write \(\varphi(x)=a\cross x\) and \(\psi(x)=b\cross x\) with \(a\ne 0\). By assumption, \(a\) and~\(b\) are orthogonal to the same plane, so \(b=\lambda a\) for some \(\lambda\in\R\) and hence \(\psi=\lambda\varphi\).
\end{proof}

\begin{exer}[4]
\[(a_1\cross a_2)\cross a_3=a_2(a_1,a_3)-a_1(a_2,a_3)\]
\end{exer}
\begin{proof}
Without loss of generality, we may assume that \(a_1,a_2\) are orthonormal. In particular, \(a_1\cross a_2\ne 0\). Define
\[\varphi(x)=(a_1\cross a_2)\cross x\qquad\text{and}\qquad\psi(x)=a_2(a_1,x)-a_1(a_2,x)\]
Then \(\varphi\ne 0\) is skew, and \(\psi\)~is skew since \(\iprod{x}{\psi x}=0\) for all~\(x\). The kernel of~\(\varphi\) is the line determined by~\(a_1\cross a_2\), which is killed by~\(\psi\). By the previous problem, \(\psi=\lambda\varphi\) for some \(\lambda\in\R\). Substituting \(x=a_1+a_2\) into this equation, it follows that \(a_2-a_1=\lambda(a_2-a_1)\), so \(\lambda=1\) and \(\psi=\varphi\).
\end{proof}

\begin{exer}[5]
A linear transformation~\(\varphi\) satisfies \(\adj{\varphi}=\lambda\varphi\) for \(\lambda\in\R\) if and only if \(\varphi\)~is self-adjoint or skew.
\end{exer}
\begin{proof}
If \(\varphi\ne 0\) and \(\adj{\varphi}=\lambda\varphi\) for some \(\lambda\in\R\), then there exist vectors \(x,y\) such that \(\iprod{\varphi x}{y}\ne 0\) and
\[\iprod{\varphi x}{y}=\lambda\iprod{x}{\varphi y}=\lambda^2\iprod{\varphi x}{y}\]
so \(\lambda=\pm 1\) and \(\varphi\)~is self-adjoint or skew, respectively. The rest is obvious.
\end{proof}

\begin{exer}[6]
If \(\Phi\)~is a skew symmetric bilinear function in an oriented Euclidean 3-space, then there is a unique vector~\(a\) such that
\[\Phi(x,y)=\iprod{a}{x\cross y}\]
\end{exer}
\begin{proof}
By problem~2, the skew transformation~\(\varphi\) dual to~\(\Phi\) can be written in the form \(\varphi(x)=a\cross x\) for some vector~\(a\). Then
\[\Phi(x,y)=\iprod{\varphi x}{y}=\iprod{a\cross x}{y}=\iprod{a}{x\cross y}\]
Uniqueness is obvious.
\end{proof}

\subsection*{\S~5}
\begin{exer}[2]
A linear transformation~\(\varphi\) is regular and preserves orthogonality (that is, \(\iprod{\varphi x}{\varphi y}=0\) whenever \(\iprod{x}{y}=0\)) if and only if \(\varphi=\lambda\tau\) where \(\lambda\ne 0\) and \(\tau\)~is a rotation.
\end{exer}
\begin{proof}
For the forward direction, let \(e_1,\ldots,e_n\) be an orthonormal basis. Then \(\varphi e_1,\ldots,\varphi e_n\) is an orthogonal basis. Also
\[\norm{\varphi e_i}^2-\norm{\varphi e_j}^2=\iprod{\varphi e_i-\varphi e_j}{\varphi e_i+\varphi e_j}=\iprod{\varphi(e_i-e_j)}{\varphi(e_i+e_j)}=0\]
since
\[\iprod{e_i-e_j}{e_i+e_j}=1-1=0\]
for all~\(i,j\). Let \(\lambda=\norm{\varphi e_i}>0\). Then clearly \(\tau=\lambda^{-1}\varphi\) is a rotation.

The reverse direction is trivial.
\end{proof}
\begin{rmk}
This proof is motivated by the geometrical fact that a rectangle is a square if and only if its diagonals are orthogonal.
\end{rmk}

\begin{exer}[5]
If \(\varphi:E\to E\) is a mapping such that \(\varphi(0)=0\) and
\[\norm{\varphi x-\varphi y}=\norm{x-y}\]
for all \(x,y\in E\), then \(\varphi\)~is linear.
\end{exer}
\begin{proof}
Substituting \(y=0\), we have \(\norm{\varphi x}=\norm{x}\) for all~\(x\), so
\[\iprod{\varphi x-\varphi y}{\varphi x-\varphi y}=\norm{x}^2-2\iprod{\varphi x}{\varphi y}+\norm{y}^2\]
On the other hand,
\[\iprod{\varphi x-\varphi y}{\varphi x-\varphi y}=\norm{\varphi x-\varphi y}^2=\norm{x-y}^2=\norm{x}^2-2\iprod{x}{y}+\norm{y}^2\]
Therefore
\[\iprod{\varphi x}{\varphi y}=\iprod{x}{y}\]
It now follows that
\[\norm{\varphi(x+y)-\varphi x-\varphi y}^2=\iprod{\varphi(x+y)-\varphi x-\varphi y}{\varphi(x+y)-\varphi x-\varphi y}=0\]
so \(\varphi(x+y)=\varphi x+\varphi y\). Similarly \(\varphi(\lambda x)=\lambda\varphi x\).
\end{proof}

\subsection*{\S~6}
\begin{rmk}
In subsection~8.21, \(j\)~is called the canonical ``complex structure'' on~\(E\) because it induces a complex vector space structure on the underlying set of~\(E\) in which scalar multiplication is defined by
\[(\alpha+\beta i)\mult x=\alpha x+\beta jx\qquad\alpha,\beta\in\R\]
\end{rmk}

\begin{rmk}
In subsection~8.21, to derive~(8.40) from (8.39) and~(8.41), first observe that \(j\varphi=\varphi j\). Indeed, for \(z\ne 0\), \(\iprod{jz}{z}=0\), so \(\iprod{\varphi jz}{\varphi z}=0\). On the other hand, \(\iprod{j\varphi z}{\varphi z}=0\). Since \(E\)~is a plane, it follows that \(j\varphi z=\lambda\varphi jz\) for some \(\lambda\in\R\). But
\begin{align*}
\lambda\norm{z}^2&=\lambda\norm{\varphi jz}^2\\
	&=\lambda\iprod{\varphi jz}{\varphi jz}\\
	&=\iprod{j\varphi z}{\varphi jz}\\
	&=\Delta(\varphi z,\varphi jz)\\
	&=\Delta(z,jz)\\
	&=\iprod{jz}{jz}\\
	&=\iprod{z}{z}=\norm{z}^2
\end{align*}
so \(\lambda=1\) and \(j\varphi z=\varphi j z\).

From this and~(8.39), it follows that
\[\varphi^{-1}x=x\mult\cos(-\Theta)+jx\mult\sin(-\Theta)=x\mult\cos\Theta-jx\mult\sin\Theta\]
where \(x\) and~\(\Theta\) are as in~(8.39). In~(8.41),
\[\Delta(x,\varphi y)=\iprod{jx}{\varphi y}=\iprod{\varphi^{-1}jx}{y}=\iprod{j\varphi^{-1}x}{y}=\Delta(\varphi^{-1}x,y)\]
so
\[\Delta(\varphi x,y)+\Delta(x,\varphi y)=\Delta(\varphi x+\varphi^{-1}x,y)=2\cos\Theta\mult\Delta(x,y)\]
and it follows that \(\cos\Theta=\tfrac{1}{2}\tr\varphi\). Similar reasoning shows that \(\sin\Theta=-\tfrac{1}{2}\tr(j\varphi)\), contrary to what the book says.
\end{rmk}

\begin{rmk}
In subsection~8.21, to make sense of the ``definition'' in~(8.43), fix \(x\ne 0\) and let \(0\le\Theta\le\pi\) be the angle between \(x\) and~\(\varphi x\). Fix an orientation in~\(E\) so that \(\Theta\)~is the \emph{oriented} angle between \(x\) and~\(\varphi x\). Then (8.40)~holds, so \(\cos\Theta=\tfrac{1}{2}\tr\varphi\). Clearly \(\Theta\)~is independent of~\(x\) and depends only on~\(\varphi\), so can be written as~\(\Theta(\varphi)\) and satisfies~(8.43).
\end{rmk}

\begin{rmk}
In subsection~8.24, for the proof of the first part of Proposition~I, let \(q_i=p_i-\lambda_i e\) where \(\lambda_i=\iprod{p_i}{e}\) for \(i=1,2\). Since \(q_1\) and~\(q_2\) each generate the same axis of rotation, either \(q_1=0=q_2\) in which case \(p_1=\pm e\) and \(p_2=\pm e\) and the result holds, or else \(q_1\ne 0\) and \(q_2=\alpha q_1\) for some \(\alpha\ne 0\). If \(\alpha>0\), then \(q_1\) and~\(q_2\) induce the same orientation in their orthogonal plane (in~\(E_1\)), so the oriented angle~\(\Theta\) of rotation in that plane is the same. It follows that
\[\lambda_1=\cos\frac{\Theta}{2}=\lambda_2\qquad\text{and}\qquad\norm{q_1}=\sin\frac{\Theta}{2}=\norm{q_2}\]
The second equation implies that \(\alpha=1\), so \(q_1=q_2\) and \(p_1=p_2\). On the other hand if \(\alpha<0\), then \(-p_2\)~is a unit quaternion inducing the same rotation as~\(p_1\) and its pure part~\(-q_2\) satisfies \(-q_2=(-\alpha)q_1\) with \(-\alpha>0\), so by the previous case \(p_1=-p_2\).
\end{rmk}

\begin{exer}[16]
If \(p\ne\pm e\) is a unit quaternion, then the rotation vector induced by~\(p\) is
\[u=2\lambda(p-\lambda e)\qquad\lambda=\iprod{p}{e}\]
\end{exer}
\begin{proof}
Let \(q=p-\lambda e\). By assumption, \(q\ne 0\) and \(u=\alpha q\) since \(q\) and~\(u\) both lie on the axis of rotation. If \(\alpha>0\), then \(q\) and~\(u\) both induce the same orientation on the orthogonal plane and
\[\alpha\norm{q}=\norm{u}=\sin\Theta=2\lambda\norm{q}\]
where \(\Theta\)~is the oriented angle of rotation. Therefore \(\alpha=2\lambda\) and the result holds. If \(\alpha<0\), then \(-p\)~induces the same rotation vector and satisfies the hypotheses of the previous case since \(u=(-\alpha)(-q)\) with \(-\alpha>0\), so \(-\alpha=2(-\lambda)\) and \(\alpha=2\lambda\) and the result holds. If \(\alpha=0\), then \(u=0\) and \(\Theta=\pi\), so \(\lambda=\cos(\pi/2)=0\) and the result holds.
\end{proof}

% References
\begin{thebibliography}{0}
\bibitem{greub} Greub, W. \textit{Linear Algebra}, 4th~ed. Springer, 1975.
\end{thebibliography}
\end{document}