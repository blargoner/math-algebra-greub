% Notes and exercises from Linear Algebra and Multilinear Algebra by Greub
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier,diagrams,stmaryrd}
\usepackage[hidelinks]{hyperref}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\from}{\leftarrow}
\newcommand{\iso}{\cong}
\renewcommand{\equiv}{\sim}
\newcommand{\orth}{\perp}
\newcommand{\divides}{\mid}

\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\tr}{tr}

\newcommand{\union}{\cup}
\newcommand{\sect}{\cap}
\newcommand{\after}{\circ}
\newcommand{\meet}{\wedge}
\newcommand{\dsum}{\oplus}
\newcommand{\bigdsum}{\bigoplus}
\newcommand{\mult}{\cdot}
\newcommand{\cross}{\times}
\newcommand{\grad}{\nabla}
\newcommand{\tprod}{\otimes}
\newcommand{\bigtprod}{\bigotimes}
\newcommand{\medtprod}{{\textstyle\bigtprod}}
\newcommand{\eprod}{\wedge}
\newcommand{\bigeprod}{\bigwedge}
\newcommand{\medeprod}{{\textstyle\bigeprod}}
\newcommand{\fprod}{\bullet}
\newcommand{\adot}{\bullet}

\newcommand{\delete}{\widehat}
\newcommand{\gen}[1]{\langle#1\rangle}
\newcommand{\pair}[2]{\langle#1,#2\rangle}
\newcommand{\sprod}[2]{\langle#1,#2\rangle}
\newcommand{\bigsprod}[2]{\big\langle#1,#2\big\rangle}
\newcommand{\oc}[1]{#1^{\perp}}
\newcommand{\occ}[1]{#1^{\perp\perp}}
\newcommand{\opp}[1]{#1^{\mathrm{opp}}}
\newcommand{\iprod}[2]{(#1,#2)}
\newcommand{\norm}[1]{|#1|}
\newcommand{\abs}[1]{|#1|}
\newcommand{\adj}[1]{\widetilde{#1}}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\proj}[1]{\overline{#1}}
\newcommand{\multi}[4]{#2_{#3}#1\cdots#1#2_{#4}}
\newcommand{\timess}[3]{\multi{\cross}{#1}{#2}{#3}}
\newcommand{\tprods}[3]{\multi{\tprod}{#1}{#2}{#3}}
\newcommand{\eprods}[3]{\multi{\eprod}{#1}{#2}{#3}}
\newcommand{\fprods}[3]{\multi{\fprod}{#1}{#2}{#3}}
\newcommand{\circled}[1]{\text{\scriptsize\textcircled{\textit{#1}}}}
\newcommand{\sign}[1]{\varepsilon_{#1}}

\newarrow{Dashto}{}{dash}{}{dash}>

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{warn}{Warning}

% Meta
\title{Notes and exercises from\\\textit{Linear Algebra} and \textit{Multilinear Algebra}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

% Intro
\section*{Introduction}
This document contains notes and exercises from \cite{greub1} and~\cite{greub2}.

\bigskip
\noindent
{\boldmath\textbf{Unless otherwise stated, \(\Gamma\)~denotes a field of characteristic~\(0\) over which all vector spaces are defined.}}

% Linear algebra
\newpage
\part*{Linear Algebra}
\section*{Chapter~I}
\subsection*{\S~1}
\begin{rmk}
The free vector space~\(C(X)\) is intuitively the space of all ``formal linear combinations'' of \(x\in X\).
\end{rmk}

\subsection*{\S~2}
\begin{exer}[5 - Universal property of~\(C(X)\)]
Let \(X\)~be a set and \(C(X)\)~the free vector space on~\(X\) (subsection~1.7). Recall
\[C(X)=\{\,f:X\to\Gamma\mid f(x)=0\text{ for all but finitely many }x\in X\,\}\]
The inclusion map \(i_X:X\to C(X)\) is defined by \(a\mapsto f_a\) where \(f_a\)~is the ``characteristic function'' of~\(a\): \(f_a(a)=1\) and \(f_a(x)=0\) for all \(x\ne a\). For \(f\in C(X)\), \(f=\sum_{a\in X}f(a)f_a\).
\begin{enumerate}
\item[(i)] If \(F\)~is a vector space and \(f:X\to F\), there is a unique \emph{linear} \(\varphi:C(X)\to F\) ``extending~\(f\)'' in the sense that \(\varphi\after i_X=f\):
\begin{diagram}[nohug]
X	&\rTo^{i_X}	&C(X)\\
	&\rdTo_f	&\dDashto>{\varphi}\\
	&			&F
\end{diagram}

\item[(ii)] If \(\alpha:X\to Y\), there is a unique \emph{linear} \(\alpha_*:C(X)\to C(Y)\) which makes the following diagram commute:
\begin{diagram}
X			&\rTo^{\alpha}			&Y\\
\dTo<{i_X}	&						&\dTo>{i_Y}\\
C(X)		&\rDashto_{\alpha_*}	&C(Y)
\end{diagram}
If \(\beta:Y\to Z\), then \((\beta\after\alpha)_*=\beta_*\after\alpha_*\).

\item[(iii)] If \(E\)~is a vector space, there is a unique linear map \(\pi_E:C(E)\to E\) such that \(\pi_E\after i_E=\iota_E\) (where \(\iota_E:E\to E\) is the identity map):
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&C(E)\\
	&\rdTo_{\iota_E}	&\dDashto>{\pi_E}\\
	&			&E
\end{diagram}

\item[(iv)] If \(E\) and~\(F\) are vector spaces and \(\varphi:E\to F\), then \(\varphi\)~is linear if and only if \(\pi_F\after\varphi_*=\varphi\after\pi_E\):
\begin{diagram}
E				&\rTo^{\varphi}		&F\\
\uTo<{\pi_E}	&					&\uTo>{\pi_F}\\
C(E)			&\rTo_{\varphi_*}	&C(F)
\end{diagram}

\item[(v)] Let \(E\)~be a vector space and \(N(E)\)~the subspace of~\(C(E)\) generated by all elements of the form
\[f_{\lambda a+\mu b}-\lambda f_a-\mu f_b\qquad(a,b\in E\text{ and }\lambda,\mu\in\Gamma)\]
Then \(\ker\pi_E=N(E)\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] By Proposition~II, since \(i_X(X)\)~is a basis of~\(C(X)\).

\item[(ii)] By~(i), applied to~\(i_Y\after\alpha\). Note \(\beta_*\after\alpha_*\)~is linear such that
\[(\beta_*\after\alpha_*)\after i_X=i_Z\after(\beta\after\alpha)\]
so \(\beta_*\after\alpha_*=(\beta\after\alpha)_*\) by uniqueness:
\begin{diagram}
X			&\rTo^{\alpha}		&Y			&\rTo^{\beta}	&Z\\
\dTo<{i_X}	&					&\dTo>{i_Y}	&				&\dTo>{i_Z}\\
C(X)		&\rTo_{\alpha_*}	&C(Y)		&\rTo_{\beta_*}	&C(Z)
\end{diagram}

\item[(iii)] By~(i), applied to~\(\iota_E\).

\item[(iv)] If \(\varphi\)~is linear, then \(\varphi\after\pi_E:C(E)\to F\) is linear and extends~\(\varphi\) in the sense that \(\varphi\after\pi_E\after i_E=\varphi\after\iota_E=\varphi\). However, \(\pi_F\after\varphi_*:C(E)\to F\) is also linear and extends~\(\varphi\) since
\[\pi_F\after\varphi_*\after i_E=\pi_F\after i_F\after\varphi=\iota_F\after\varphi=\varphi\]
By uniqueness, these two maps must be equal. Conversely, if these two maps are equal, then \(\varphi\)~is linear since \(\pi_F\after\varphi_*\)~is linear and \(\pi_E\)~is surjective.

\item[(v)] By~(iii),
\begin{align*}
\pi_E(f_{\lambda a+\mu b}-\lambda f_a-\mu f_b)&=\pi_E(f_{\lambda a+\mu b})-\lambda\pi_E(f_a)-\mu\pi_E(f_b)\\
	&=\lambda a+\mu b-\lambda a-\mu b\\
	&=0
\end{align*}
for all \(a,b\in E\) and \(\lambda,\mu\in\Gamma\). It follows that \(N(E)\subseteq\ker\pi_E\) since \(N(E)\)~is the \emph{smallest} subspace containing these elements and \(\ker\pi_E\)~is a subspace.

On the other hand, it follows from the fact that \(N(E)\)~is a subspace that
\[\sum\lambda_i f_{a_i}-f_{\;\sum\lambda_i a_i}\in N(E)\]
for all (finite) linear combinations. Now if \(g=\sum_{a\in E}g(a)f_a\in\ker\pi_E\), then
\[0=\pi_E(g)=\sum_{a\in E}g(a)\pi_E(f_a)=\sum_{a\in E}g(a)a\]
This implies \(f_{\;\sum_{a\in E}g(a)a}=f_0\in N(E)\). But by the above, \(g-f_0\in N(E)\), so \(g\in N(E)\). Therefore also \(\ker\pi_E\subseteq N(E)\).\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
Note (i)~shows that \(C(X)\)~is a universal (initial) object in the category of ``vector spaces with maps of~\(X\) into them''. In this category, the objects are maps \(X\to F\), for vector spaces~\(F\), and the arrows are \emph{linear} (i.e. structure-preserving) maps \(F\to G\) between the vector spaces which respect the mappings of~\(X\):
\begin{diagram}[nohug]
X	&\rTo	&F\\
	&\rdTo	&\dTo\\
	&		&G
\end{diagram}
By~(i), every object \(X\to F\) in this category can be obtained from the inclusion map \(X\to C(X)\) in a unique way. This is why \(C(X)\) is called ``universal''. This is only possible because \(C(X)\)~is free from any nontrivial relations among the elements of~\(X\), so any relations among the images of those elements in~\(F\) can be obtained starting from~\(C(X)\). This is why \(C(X)\)~is called ``free''. It is immediate from the universal property that \(C(X)\)~is unique up to isomorphism: if \(X\to U\) is also universal, then the composites \(\psi\after\varphi\) and~\(\varphi\after\psi\) of the induced linear maps \(\varphi:C(X)\to U\) and \(\psi:U\to C(X)\) are linear and extend the inclusion maps, so must be the identity maps on \(C(X)\) and~\(U\) by uniqueness; that is, \(\varphi\) and~\(\psi\) are mutually inverse and hence \emph{isomorphisms}. In fact they are also unique by the universal property.

Now (ii)~shows that we have a \emph{functor} from the category of sets into the category of vector spaces, which sends sets \(X\) and~\(Y\) to the vector spaces \(C(X)\) and~\(C(Y)\), and which sends a set map \(\alpha:X\to Y\) to the linear map \(\alpha_*:C(X)\to C(Y)\). The functor preserves the category structure of composites of arrows.

In~(iii), we are ``forgetting'' the linear structure of~\(E\) when forming~\(C(E)\). For example, if \(E=\R^2\), then \(\pair{1}{1}=\pair{1}{0}+\pair{0}{1}\) in~\(E\), but \emph{not} in~\(C(E)\). The ``formal'' linear combination
\[\pair{1}{1}-\pair{1}{0}-\pair{0}{1}\]
is not zero in~\(C(E)\) because the pairs are unrelated elements (symbols) which are \emph{linearly independent}. Note \(\pi_E\)~is surjective (since \(\iota_E\)~is), so \(E\)~is a projection of~\(C(E)\). In~(iv), we see that \(\varphi:E\to F\) is linear if and only if it is a ``projection'' of \(\varphi_*:C(E)\to C(F)\).

In~(v), we see that \(\pi_E\)~just recalls the linear structure of~\(E\) that was forgotten in~\(C(E)\). In particular, \(C(E)/N(E)\iso E\). In other words, if you start with~\(E\), then forget about its linear structure, then recall that linear structure, you just get \(E\)~again.
\end{rmk}

\subsection*{\S~4}
\begin{exer}[11]
Let \(E\)~be a real vector space and \(E_1\)~a vector hyperplane in~\(E\) (that is, a subspace of codimension~\(1\)). Define an equivalence relation on \(E^1=E-E_1\) as follows: for \(x,y\in E^1\), \(x\equiv y\) if the segment
\[x(t)=(1-t)x+ty\qquad(0\le t\le 1)\]
is disjoint from~\(E_1\). Then there are precisely two equivalence classes.
\end{exer}
\begin{proof}
Fix \(e\in E^1\) with \(E=E_1\dsum\gen{e}\) and define \(\alpha:E\to\R\) by \(x-\alpha(x)e\in E_1\) for all \(x\in E\). It is clear that \(\alpha\)~is linear, and \(x\in E_1\) if and only if \(\alpha(x)=0\). For \(x,y\in E^1\), it follows that \(x\equiv y\) if and only if
\[0\ne\alpha(x(t))=\alpha((1-t)x+ty)=(1-t)\alpha(x)+t\alpha(y)\]
for all \(0\le t\le 1\). But this is just equivalent to \(\alpha(x)\alpha(y)>0\).

Now if \(x\in E^1\), then \(\alpha(x)\ne0\), so \(\alpha(x)^2>0\) and \(x\equiv x\). If \(x\equiv y\), then \(\alpha(y)\alpha(x)=\alpha(x)\alpha(y)>0\), so \(y\equiv x\). If also \(y\equiv z\), then \(\alpha(y)\alpha(z)>0\), so \(\alpha(x)\alpha(z)>0\) and \(x\equiv z\). In other words, this is indeed an equivalence relation.

Note there are at least two equivalence classes since \(\alpha(e)=1\) and \(\alpha(-e)=-1\), so \(\alpha(e)\alpha(-e)=-1<0\) and \(e\not\equiv -e\). On the other hand, there are at most two classes since if \(x\in E^1\), then either \(\alpha(x)>0\) and \(x\equiv e\) or \(\alpha(x)<0\) and \(x\equiv -e\).
\end{proof}
\begin{rmk}
This result shows that the hyperplane separates the vector space into two disjoint half-spaces.
\end{rmk}

\section*{Chapter~II}
\subsection*{\S~2}
\begin{rmk}
In subsection~2.11, in the second part of the proof of Proposition~I, just let \(\psi:E\from F\) be any linear mapping extending \(\varphi_1^{-1}:E\from\im\varphi\).\footnote{See Corollary~I to Proposition~I in subsection~1.15.}
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The direct sum \(E\dsum F\) is a coproduct in the category of vector spaces in the following sense: if \(\varphi:E\to G\) and \(\psi:F\to G\) are linear maps, there is a unique linear map \(\chi:E\dsum F\to G\) such that \(\varphi=\chi\after i_E\) and \(\psi=\chi\after i_F\), where \(i_E\) and~\(i_F\) are the canonical injections:
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&E\dsum F		&\lTo^{i_F}		&F\\
	&\rdTo<{\varphi}	&\dDashto{\chi}	&\ldTo>{\psi}	&\\
	&					&G				&				&
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x+y)=\varphi(x)+\psi(y)\) for \(x\in E\), \(y\in F\). It is the unique linear map ``extending'' both \(\varphi\) and~\(\psi\). This property makes \(E\dsum F\) unique up to a unique isomorphism.

Dually, \(E\dsum F\) is a product in the following sense: if \(\varphi:G\to E\) and \(\psi:G\to F\) are linear maps, there is a unique linear map \(\chi:G\to E\dsum F\) such that \(\varphi=\pi_E\after\chi\) and \(\psi=\pi_F\after\chi\):
\begin{diagram}[nohug]
	&					&G				&				&\\
	&\ldTo<{\varphi}	&\dDashto{\chi}	&\rdTo>{\psi}	&\\
E	&\lTo^{\pi_E}		&E\dsum F		&\rTo^{\pi_F}	&F
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x)=\varphi(x)+\psi(x)\), and ``combines'' \(\varphi\) and~\(\psi\). This property also makes \(E\dsum F\) unique up to a unique isomorphism. An infinite direct sum is also a coproduct, but \emph{not} a product, essentially because it has no infinite sums of elements.

In the proof of Proposition~I, \(\sigma\)~is the product map and \(\tau\)~is the coproduct map. If \(\varphi_1:E_1\to F_1\) and \(\varphi_2:E_2\to F_2\) are linear maps, then \(\varphi=\varphi_1\dsum\varphi_2\) is both a coproduct and product map:
\begin{diagram}
E_1				&\pile{\rTo\\\lTo}	&E_1\dsum E_2	&\pile{\lTo\\\rTo}	&E_2\\
\dTo<{\varphi_1}&					&\dTo>{\varphi}	&					&\dTo>{\varphi_2}\\
F_1				&\pile{\rTo\\\lTo}	&F_1\dsum F_2	&\pile{\lTo\\\rTo}	&F_2
\end{diagram}
The structure of~\(\varphi\) is completely determined by the structures of \(\varphi_1\) and~\(\varphi_2\). In particular, \(\varphi\)~is injective (surjective, bijective) if and only if \(\varphi_1\) and~\(\varphi_2\) are.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
The definition of dual space is fundamentally \emph{symmetrical} between \(E\) and~\(E^*\), as is the definition of dual mapping between \(\varphi\) and~\(\varphi^*\). This symmetry often allows us to use bidirectional reasoning and derive two theorems from one proof. For example, (2.48) actually follows from~(2.47) by symmetry of \(\varphi\) and~\(\varphi^*\). The proof of Proposition~I in subsection~2.23 exploits symmetry, as do other proofs in the book. Many other books simply \emph{define} the dual space of~\(E\) to be~\(L(E)\) (no doubt in light of Proposition~I of this subsection), at the expense of this symmetry.
\end{rmk}

\begin{rmk}
The results in subsection~2.23 show that quotient spaces are dual to subspaces.
\end{rmk}

\begin{rmk}
If \(E,E^*\) and \(F,F^*\) are pairs of dual spaces and \(\varphi:E\to F\) is linear, then \(\varphi^*:E^*\from F^*\) is dual to~\(\varphi\) if and only if the following diagram commutes:
\begin{diagram}
F^*\times E&\rTo^{\varphi^*\times\iota_E}&E^*\times E\\
\dTo<{\iota_{F^*}\times\varphi}&		&\dTo>{\sprod{\ }{\ }}\\
F^*\times F&\rTo_{\sprod{\ }{\ }}&\Gamma
\end{diagram}
\end{rmk}

\begin{rmk}
Let \(E\)~be a vector space and \((x_{\alpha})_{\alpha\in A}\) be a basis of~\(E\). For each \(x\in E\), write \(x=\sum_{\alpha\in A}f_{\alpha}(x)x_{\alpha}\). Then \(f_{\alpha}\in L(E)\) for each \(\alpha\in A\). The function \(f_{\alpha}\) is called the \emph{\(\alpha\)-th coordinate function} for the basis.

Coordinate functions can be used in an alternative proof of Proposition~IV. If \(E_1\)~is a subspace of~\(E\), let \(B_1\)~be a basis of~\(E_1\) and extend it to a basis \(B\) of~\(E\). For each \(x_{\alpha}\in B-B_1\), we have \(f_{\alpha}\in\oc{E_1}\). If \(x\in\occ{E_1}\), then \(f_{\alpha}(x)=\sprod{f_{\alpha}}{x}=0\) for all such \(\alpha\), so \(x\in E_1\). In other words, \(\occ{E_1}\subseteq E_1\).
\end{rmk}

\begin{rmk}
In the corollary to Proposition~V, for \(f\in L(E)\) let \(f_k=f\after i_k\after \pi_k\) where \(i_k:E_k\to E\) is the \(k\)-th canonical injection and \(\pi_k:E\to E_k\) is the \(k\)-th canonical projection. Then \(f=\sum_k f_k\) and \(f_k\in\oc{F_k}\) for all~\(k\), so \(L(E)=\sum_k\oc{F_k}\). The sum is direct since if \(f\in\oc{F_k}\sect\sum_{j\ne k}\oc{F_j}\), then \(f\)~kills \(\sum_{j\ne k}E_j\) and~\(E_k\), so \(f=0\). A scalar product is induced between \(E_k,\oc{F_k}\) since \(E_k\sect F_k=0\) and \(\oc{F_k}\sect\oc{E_k}=0\).\footnote{See subsection~2.23.} The induced injection \(\oc{F_k}\to L(E_k)\) is surjective since every linear function on~\(E_k\) can be extended to a linear function on~\(E\) which kills~\(F_k\).
\end{rmk}

\begin{rmk}
For \(\varphi:E\to F\) a linear map, let \(L(\varphi):L(E)\from L(F)\) be the dual map given by \(L(\varphi)(f)=f\after\varphi\) (2.50). Then \(L\)~linearly embeds \(L(E;F)\) in \(L(L(F);L(E))\), by (2.43) and~(2.44). Also, \(L(\psi\after\varphi)=L(\varphi)\after L(\psi)\) and \(L(\iota_{E})=\iota_{L(E)}\). This shows that \(L\)~is a contravariant functor in the category of vector spaces. This functor preserves exactness of sequences (see~2.29), and finite direct sums, which are just (co)products in the category (see~2.30), among other things.
\end{rmk}

\begin{exer}[10]
If \(\varphi:E\to F\) is a linear map with restriction \(\varphi_1:E_1\to F_1\) and dual map \(\varphi^*:E^*\from F^*\), then \(\varphi^*\)~can be restricted to \(\oc{F_1},\oc{E_1}\) and the induced map \(\proj{\varphi^*}:E^*/\oc{E_1}\from F^*/\oc{F_1}\) is dual to~\(\varphi_1\).
\end{exer}
\begin{proof}
If \(y^*\in\oc{F_1}\) and \(x\in E_1\), then
\[\sprod{\varphi^*y^*}{x}=\sprod{y^*}{\varphi x}=0\]
so \(\varphi^*\)~maps \(\oc{F_1}\) into~\(\oc{E_1}\). We know that the pairs \(E_1,E^*/\oc{E_1}\) and \(F_1,F^*/\oc{F_1}\) are dual under the induced scalar products. For \(\proj{y^*}\in F^*/\oc{F_1}\) and \(x\in E_1\),
\begin{align*}
\sprod{\proj{\varphi^*}\ \proj{y^*}}{x}&=\sprod{\proj{\varphi^* y^*}}{x}\\
	&=\sprod{\varphi^* y^*}{x}\\
	&=\sprod{y^*}{\varphi_1 x}\\
	&=\sprod{\proj{y^*}}{\varphi_1 x}\qedhere
\end{align*}
\end{proof}
\begin{rmk}
This result shows that quotient maps are dual to restriction maps. The examples in subsections 2.24 and~2.27 are special cases.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(E\)~is finite-dimensional, then every basis of a dual space~\(E^*\) is a dual basis. Indeed, if \(f_1,\ldots,f_n\) is a basis of~\(E^*\), let \(e_1,\ldots,e_n\) be its dual basis in~\(E\). Then \(\sprod{f_i}{e_j}=\delta_{ij}\) by~(2.62), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\), again by~(2.62).

Alternatively, with \(E^*=L(E)\), let \(f_1^*,\ldots,f_n^*\) be the dual basis of \(f_1,\ldots,f_n\) in \(E^{**}=L(L(E))\), so \(\sprod{f_j^*}{f_i}=\delta_{ij}\) by~(2.62). Let \(e_1,\ldots,e_n\in E\) be defined by \(\sprod{f_j^*}{f}=\sprod{f}{e_j}\) for all \(f\in E^*\) (see \S~5, problem~3). Then \(\sprod{f_i}{e_j}=\sprod{f_j^*}{f_i}=\delta_{ij}\), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\).

The first proof here uses the symmetry between \(E\) and~\(E^*\), while the second uses the natural isomorphism \(E\iso E^{**}\).
\end{rmk}

\begin{exer}[9]
If \(E\) and~\(F\) are finite-dimensional, then the mapping
\[\Phi:L(E;F)\to L(F^*;E^*)\]
defined by \(\varphi\mapsto\varphi^*\) is a linear isomorphism.
\end{exer}
\begin{proof}
By the remark in~\S~5 above, and the fact that \(\varphi^{**}=\varphi\).
\end{proof}

\section*{Chapter~III}
\begin{warn}
Greub's notational choices in this chapter are insane. In particular, although he uses left-hand mapping notation (writing \(\varphi x\) instead of~\(x\varphi\), and \(\varphi\psi\) to mean \emph{\(\varphi\) after \(\psi\)}), and follows the usual ``row-by-column'' convention for matrix multiplication, his convention for the matrix of a linear mapping is the transpose of that normally used in these circumstances. This has the following unfortunate consequences:
\begin{itemize}
\item The matrix of the linear mapping naturally associated with a system of linear equations has the coefficients from each equation appear \emph{vertically in columns}.
\item If \(M(x)\)~is the \emph{column vector} representing~\(x\), then \(M(\varphi x)=M(\varphi)^*M(x)\), and if \(M(x)\)~is the \emph{row vector} representing~\(x\), then \(M(\varphi x)=M(x)M(\varphi)\).
\item \(M(\varphi\psi)=M(\psi)M(\varphi)\)
\end{itemize}
Compounding the insanity, Greub (inspired by tensor notation) indexes over columns instead of rows when working in dual spaces. This further increases the risk of confusion and error, as we see below. Greub says that ``it would be very undesirable\dots to agree once and for all to always let the subscript count the rows'', but we couldn't disagree more in this context.
\end{warn}

\subsection*{\S~3}
\begin{rmk}
In subsection~3.13, although \(\beta^{\varrho}_{\nu}=\check{\alpha}_{\nu}^{\varrho}\), we must remember that \(\nu\)~indexes the \emph{columns} of the matrix of the dual basis transformation \(x^{*\nu}\mapsto\bar{x}^{*\nu}\) by~(3.24), whereas \(\nu\)~indexes the \emph{rows} of the matrix of the basis transformation \(x_{\nu}\mapsto\bar{x}_{\nu}\) by~(3.22). In other words, the matrix of the dual basis transformation \(x^{*\nu}\mapsto\bar{x}^{*\nu}\) is the \emph{transpose} of the inverse of the matrix of the basis transformation \(x_{\nu}\mapsto\bar{x}_{\nu}\), contrary to what the book says.\footnote{Compare this to the proof of equation~(3.4) in subsection~3.3.} It's easier to remember that the matrix of \(x^{*\nu}\mapsfrom\bar{x}^{*\nu}\) (arrow reversed!) is the transpose of the matrix of \(x_{\nu}\mapsto\bar{x}_{\nu}\).
\end{rmk}

\begin{rmk}
In subsection 3.13, we see that if a basis transformation is effected by~\(\tau\), then the corresponding \emph{coordinate} transformation is effected by~\(\tau^{-1}\). The coordinates of a vector are transformed ``exactly in the same way'' as the vectors of the dual basis, despite the previous remark, because of Greub's notational choices.
\end{rmk}

\section*{Chapter~IV}
\begin{rmk}
In this chapter, it is implicitly assumed that all vector spaces have dimension \(n\ge 1\), except in the definition of intersection number (subsection 4.31) where \(n=0\). Here we summarize results for the case \(n=0\):
\begin{itemize}
\item For a set~\(X\), \(X^0=\{\emptyset\}\). Therefore maps \(\Phi:X^0\to Y\) can be identified with elements of~\(Y\).
\item For vector spaces \(E\) and~\(F\), a map \(\Phi:E^0\to F\) is vacuously \(0\)-linear. Since the only permutation in~\(S_0\) is the identity \(\iota=\emptyset\), \(\Phi\)~is also trivially skew symmetric.
\end{itemize}
In particular if \(E=0\), the following results hold:
\begin{itemize}
\item Determinant functions in~\(E\) are just scalars in~\(\Gamma\), and dual determinant functions are just reciprocal scalars.
\item The only transformation of~\(E\) is the zero transformation, which is also the identity transformation. It has determinant~\(1\), trace~\(0\), and constant characteristic polynomial~\(1\). It has no eigenvalues or eigenvectors. Its adjoint is also the zero transformation. Its matrix on the empty basis is empty.
\item If \(E\)~is real (\(\Gamma=\R\)), the orientations in~\(E\) are represented by the scalars~\(\pm 1\), and determine whether the empty basis is positive or negative. The zero transformation is orientation preserving. The empty basis is deformable into itself.
\end{itemize}
\end{rmk}

\subsection*{\S~1}
\begin{rmk} To see why~(4.1) holds, observe by definition of~\(\tau(\sigma\Phi)\) that
\[(\tau(\sigma\Phi))(x_1,\ldots,x_p)=(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})\]
Let \(y_i=x_{\tau(i)}\). Then by definition of \(\sigma\Phi\) and~\((\tau\sigma)\Phi\),
\begin{align*}
(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})&=(\sigma\Phi)(y_1,\ldots,y_p)\\
	&=\Phi(y_{\sigma(1)},\ldots,y_{\sigma(p)})\\
	&=\Phi(x_{\tau(\sigma(1))},\ldots,x_{\tau(\sigma(p))})\\
	&=\Phi(x_{(\tau\sigma)(1)},\ldots,x_{(\tau\sigma)(p)})\\
	&=((\tau\sigma)\Phi)(x_1,\ldots,x_p)
\end{align*}
Therefore \(\tau(\sigma\Phi)=(\tau\sigma)\Phi\).
\end{rmk}

\begin{rmk}
By Proposition~I(iii) and Proposition~II, a determinant function \(\Delta\ne 0\) ``determines'' linear independence in the sense that \(\Delta(x_1,\ldots,x_n)\ne0\) if and only if \(x_1,\ldots,x_n\) are linearly independent. By~(4.8), it follows that \(\det\varphi\) ``determines'' whether a linear transformation~\(\varphi\) preserves linear independence, i.e. whether or not \(\varphi\)~is invertible.

Geometrically, \(\Delta(x_1,\ldots,x_n)\) measures the oriented (signed) volume of the \(n\)-dimensional parallelepiped determined by the vectors \(x_1,\ldots,x_n\). Therefore \(\det\varphi\) is the factor by which \(\varphi\)~changes oriented volume. Since a small change in the vectors \(x_1,\ldots,x_n\) results in a small change in the oriented volume, \(\Delta\)~is continuous.
\end{rmk}

\begin{rmk}
We provide an alternative proof of Proposition~IV. First note
\[(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)=\Delta(x_1,\ldots,x,\ldots,x_n)\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{\(\delete{x_j}\)~denotes deletion of \(x_j\)~from the sequence on the left.} Therefore
\[\sum_{j=1}^n(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)x_j=\Delta(x,x_2,\ldots,x_n)x_1+\cdots+\Delta(x_1,\ldots,x_{n-1},x)x_n\]
Viewing this as a function of \(x_1,\ldots,x_n\) (that is, a set map from \(E^n\to L(E;E)\)), it is obviously multilinear and skew symmetric (by Proposition~I(ii)). Therefore if \(x_1,\ldots,x_n\) are linearly dependent, it is zero (by Proposition~I(iii)). If \(x_1,\ldots,x_n\) are linearly independent (and hence a basis), then viewing it as a function of~\(x\), its value at~\(x_i\) is just \(\Delta(x_1,\ldots,x_n)x_i\) (by Proposition~I(ii)), so it agrees on a basis with \(\Delta(x_1,\ldots,x_n)x\) and hence is equal to it.
\end{rmk}

\begin{rmk}
Let \(E\)~be a vector space with \(\dim E=n>1\) and \(E_1\)~a subspace with \(\dim E_1=1\). Let \(\Delta\)~be a determinant function in~\(E\) with \(\Delta(e_1,\ldots,e_n)=1\) where \(e_1\in E_1\). Then \(\Delta\)~induces a determinant function~\(\Delta_1\) in~\(E/E_1\) by
\[\Delta_1(\,\proj{x_2},\ldots,\proj{x_n}\,)=\Delta(e_1,x_2,\ldots,x_n)\]
with \(\Delta_1(\,\proj{e_2},\ldots,\proj{e_n}\,)=1\). Define \(D:E^n\to\Gamma\) by
\[D(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{j-1}\Delta_1(\,\proj{x_1},\ldots,\widehat{\proj{x_j}},\ldots,\proj{x_n}\,)\mult\pi_1(x_j)\]
where \(\pi_1:E\to\Gamma\) is the coordinate function for~\(e_1\). Then \(D\)~is skew symmetric and \(n\)-linear with \(D(e_1,\ldots,e_n)=1\), so \(D=\Delta\) by uniqueness (Proposition~III). Therefore
\[\boxed{\Delta(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{j-1}\Delta_1(\,\proj{x_1},\ldots,\widehat{\proj{x_j}},\ldots,\proj{x_n}\,)\mult\pi_1(x_j)}\]
This result, which can also be obtained by applying~\(\pi_1\) to both sides of~(4.6) with \(x=e_1\), expresses a fundamental relationship between an \(n\)-dimensional determinant function and an \((n-1)\)-dimensional one. The cofactor expansion formulas for the determinant (subsection 4.15) follow immediately.

Note that this relationship can also be exploited in reverse to \emph{define} an \(n\)-dimensional determinant function in terms of an \((n-1)\)-dimensional one. In fact, if \(\Phi:E^{n-1}\to\Gamma\) is a skew symmetric \((n-1)\)-linear function in~\(E\), then there is a vector \(a\in E\) such that
\[\Phi(x_1,\ldots,x_{n-1})=\Delta(x_1,\ldots,x_{n-1},a)\tag{1}\]
Define \(\varphi:E^n\to E\) by
\[\varphi(x_1,\ldots,x_n)=\sum_{j=1}^n (-1)^{n-j}\Phi(x_1,\ldots,\widehat{x_j},\ldots,x_n)\mult x_j\]
Then \(\varphi\)~is skew symmetric and \(n\)-linear, so \(\varphi=\Delta a\) where \(a=\varphi(e_1,\ldots,e_n)\). This means
\[\Delta(x_1,\ldots,x_n)a=\sum_{j=1}^n (-1)^{n-j}\Phi(x_1,\ldots,\widehat{x_j},\ldots,x_n)\mult x_j\tag{2}\]
If \(x_1,\ldots,x_{n-1}\) are linearly dependent, then (1)~holds trivially, so we may assume that they are linearly independent. If \(a\)~is not in their span, then (1)~follows from~(2) with \(x_n=a\); if \(a\)~is in the span, then (1)~follows from~(2) with \(x_n\) chosen so that \(\Delta(x_1,\ldots,x_n)=1\).

Geometrically, this result expresses a relationship between \(n\)-dimensional volume and \((n-1)\)-dimensional volume. We also see from~(1) that the latter is just the former measured \emph{relative to a fixed vector}. The fact that the volume of an \(n\)-dimensional parallelepiped is equal to the product of the volume of any \((n-1)\)-dimensional ``base'' and the corresponding ``height'' (subsection 7.15) is a special case.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In subsection~4.6, we want a transformation~\(\psi\) with \(\psi\varphi=(\det\varphi)\iota\). We can choose a basis \(x_1,\ldots,x_n\) in~\(E\) with \(\Delta(x_1,\ldots,x_n)=1\), for which we want
\begin{align*}
(\psi\varphi)x_i=\psi(\varphi x_i)&=(\det\varphi)x_i\\
	&=(\det\varphi)\Delta(x_1,\ldots,x_n)x_i\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_n)x_i
\end{align*}
To obtain this, we can define
\[\psi(x)=\sum_{j=1}^n\Delta(\varphi x_1,\ldots,x,\ldots,\varphi x_n)x_j\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{See the remark on Proposition~IV above.} Then \(\psi\)~obviously satisfies the above properties, by multilinearity and skew symmetry of~\(\Delta\).

To obtain~\(\psi\) in a ``coordinate-free''  manner (without choosing a basis), we observe that the construction on the right is multilinear and skew symmetric in \(x_1,\ldots,x_n\) when viewed as a mapping \(\Phi:E^n\to L(E;E)\). By the universal property of~\(\Delta\) (Proposition~III), there is a unique \(\psi\in L(E;E)\) satisfying the above; this~\(\psi\) is also seen to be independent of the choice of~\(\Delta\).
\end{rmk}

\begin{rmk}
In subsection~4.7, observe that
\[\Delta(x_1,\ldots,x_p,y_1,\ldots,y_q)\]
induces a determinant function on~\(E_2\) when \(x_1,\ldots,x_p\in E\) are fixed, and induces a determinant function on~\(E_1\) when \(y_1,\ldots,y_q\in E\) are fixed. Now let \(a_1,\ldots,a_p\) be a basis of~\(E_1\), so \(a_1,\ldots,a_p,b_1,\ldots,b_q\) is a basis of~\(E\). Then by~(4.8),
\begin{align*}
\det\varphi\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)&=\Delta(\varphi_1 a_1,\ldots,\varphi_1 a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\Delta(a_1,\ldots,a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\det\varphi_2\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)
\end{align*}
Since \(\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)\ne0\), it follows that \(\det\varphi=\det\varphi_1\mult\det\varphi_2\). Note this result shows that
\[\det(\varphi_1\dsum\varphi_2)=\det\varphi_1\mult\det\varphi_2\]
\end{rmk}

\begin{exer}[2]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\det\varphi=\det\varphi_1\mult\det\proj{\varphi}\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) where \(e_1,\ldots,e_p\) is a basis of~\(E_1\). Let \(\Delta\ne 0\) be a determinant function in~\(E\). First observe that
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,\varphi e_{p+1},\ldots,\varphi e_n)\tag{1}\]
is a determinant function in~\(E_1\) and
\[\Delta_2(\,\proj{x_{p+1}},\ldots,\proj{x_n}\,)=\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)\tag{2}\]
is a well-defined determinant function in~\(E/E_1\). Now
\[\det\proj{\varphi}\mult\Delta_2(\,\proj{x_{p+1}},\ldots,\proj{x_n}\,)=\Delta_2(\,\proj{\varphi}\ \proj{x_{p+1}},\ldots,\proj{\varphi}\ \proj{x_n}\,)=\Delta_2(\,\proj{\varphi x_{p+1}},\ldots,\proj{\varphi x_n}\,)\tag{3}\]
It follows from (2) and~(3) that
\[\det\proj{\varphi}\mult\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)=\Delta(e_1,\ldots,e_p,\varphi x_{p+1},\ldots,\varphi x_n)\tag{4}\]
Now
\begin{align*}
\det\varphi\mult\Delta(e_1,\ldots,e_n)&=\Delta(\varphi e_1,\ldots,\varphi e_n)&&\\
	&=\Delta_1(\varphi_1 e_1,\ldots,\varphi_1 e_p)&&\text{by~(1)}\\
	&=\det\varphi_1\mult\Delta_1(e_1,\ldots,e_p)&&\\
	&=\det\varphi_1\mult\det\proj{\varphi}\mult\Delta(e_1,\ldots,e_n)&&\text{by (1), (4)}
\end{align*}
Since \(\Delta(e_1,\ldots,e_n)\ne 0\), the result follows.
\end{proof}

\subsection*{\S~4}
\begin{rmk}
If \(A\)~is an \(n\times n\) matrix of the form
\[A=\begin{pmatrix}
A_1&\\
*&A_2
\end{pmatrix}\]
where \(A_1\)~is \(p\times p\) and \(A_2\)~is \((n-p)\times(n-p)\), then
\[\det A=\det A_1\mult\det A_2\tag{1}\]
Indeed, let \(E\)~be an \(n\)-dimensional vector space and \(\varphi:E\to E\) be defined by \(M(\varphi;e_1,\ldots,e_n)=A\), so \(\det A=\det\varphi\). Let \(E_1=\gen{e_1,\ldots,e_p}\) and \(E_2=\gen{e_{p+1},\ldots,e_n}\). Then \(E=E_1\dsum E_2\) and \(E_1\)~is stable under~\(\varphi\). If \(\varphi_1:E_1\to E_1\) is the induced map, then \(A_1=M(\varphi_1)\), so \(\det A_1=\det\varphi_1\). Dually, \(E^*=E_1^*\dsum E_2^*\) where \(E_1^*=\gen{e_1^*,\ldots,e_p^*}\) and \(E_2^*=\gen{e_{p+1}^*,\ldots,e_n^*}\), and \(E_2^*\)~is stable under~\(\varphi^*\) since
\[M(\varphi^*;e_1^*,\ldots,e_n^*)=A^*=\begin{pmatrix}
A_1^*&*\\
&A_2^*
\end{pmatrix}\]
If \(\varphi_2^*:E_2^*\to E_2^*\) is the induced map, then \(A_2^*=M(\varphi_2^*)\) and \(\det A_2=\det A_2^*=\det\varphi_2^*\). So we must prove that \(\det\varphi=\det\varphi_1\mult\det\varphi_2^*\).

Let \(\Delta\ne0\) be a determinant function in~\(E\) and \(\Delta^*\)~its dual in~\(E^*\). We claim
\begin{multline*}
\Delta^*(e_1^*,\ldots,e_n^*)\mult\Delta(e_1,\ldots,e_p,\varphi e_{p+1},\ldots,\varphi e_n)\\=\Delta(e_1,\ldots,e_n)\mult\Delta^*(e_1^*,\ldots,e_p^*,\varphi^* e_{p+1}^*,\ldots,\varphi^* e_n^*)\tag{2}
\end{multline*}
Indeed, by~(4.26) the left side of~(2) is a determinant of the form
\[\begin{vmatrix}
J&\\
*&B
\end{vmatrix}\]
where \(J\)~is the \(p\times p\) identity matrix and \(B=(\beta_i^j)\) with \(\beta_i^j=\sprod{e_j^*}{\varphi e_i}=\sprod{\varphi^* e_j^*}{e_i}\). However, since the determinant is multilinear in its rows,\footnote{See subsection~4.9, item~4.} it is equal to
\[\begin{vmatrix}
J&\\
&B
\end{vmatrix}\]
A similar argument shows that the same is true of the right side of~(2). Now
\begin{align*}
\det\varphi&=\det\varphi\mult\Delta(e_1,\ldots,e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\Delta(\varphi_1e_1,\ldots,\varphi_1e_p,\varphi e_{p+1},\ldots,\varphi e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\det\varphi_1\mult\Delta(e_1,\ldots,e_p,\varphi e_{p+1},\ldots,\varphi e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\det\varphi_1\mult\Delta^*(e_1^*,\ldots,e_p^*,\varphi_2^* e_{p+1}^*,\ldots,\varphi_2^* e_n^*)\mult\Delta(e_1,\ldots,e_n)&\text{by~(2)}\\
	&=\det\varphi_1\mult\det\varphi_2^*\mult\Delta^*(e_1^*,\ldots,e_n^*)\mult\Delta(e_1,\ldots,e_n)\\
	&=\det\varphi_1\mult\det\varphi_2^*
\end{align*}
The same result~(1) holds when \(A\)~has the form
\[A=\begin{pmatrix}
A_1&*\\
&A_2
\end{pmatrix}\]
Indeed, by the above,
\[\det A=\det A^*=\det A_1^*\mult\det A_2^*=\det A_1\mult\det A_2\]
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Recall that the system (4.39) is equivalent to \(\varphi x=y\) where \(\varphi:\Gamma^n\to\Gamma^n\) is defined by \(M(\varphi)=(\alpha^j_k)=A\), \(x=(\xi^i)\), and \(y=(\eta^j)\). If \(\det A\ne0\), then \(\varphi\)~is invertible and
\[x=\varphi^{-1}y=\frac{1}{\det A}\ad(\varphi)(y)\]
It follows from the analysis of the adjoint matrix in subsection~4.13 that
\[\xi^i=\frac{1}{\det A}\sum_j\cof(\alpha^j_i)\eta^j\]
Moreover, it follows from~(4.38) that \(\sum_j\cof(\alpha^j_i)\eta^j=\det A_i\) where \(A_i\)~is the matrix obtained from~\(A\) by replacing the \(i\)-th row with~\(y\).\footnote{The cofactors of \(A_i\) and~\(A\) along the \(i\)-th row agree since \(A_i\) and~\(A\) agree on the other rows.} Therefore
\[\xi^i=\frac{\det A_i}{\det A}\]
\end{rmk}

\begin{rmk}
In subsection~4.14, \(\det B^j_i=\det S^j_i\) does \emph{not} follow from~(4.38), which only tells us that \(\det B^j_i=\det B^j_i\). However, it follows from~(4.16), or from our remarks in~\S~4 above.
\end{rmk}

\subsection*{\S~6}
\begin{exer}[5]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\chi(\varphi_1\dsum\varphi_2)=\chi(\varphi_1)\mult\chi(\varphi_2)\]
where \(\chi(\varphi)\)~denotes the characteristic polynomial of~\(\varphi\).
\end{exer}
\begin{proof}
This follows from the result in subsection~4.7 and the fact that
\[\varphi_1\dsum\varphi_2-\lambda\iota=(\varphi_1-\lambda\iota_{E_1})\dsum(\varphi_2-\lambda\iota_{E_2})\qedhere\]
\end{proof}

\begin{exer}[6]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\chi(\varphi)=\chi(\varphi_1)\mult\chi(\proj{\varphi})\]
\end{exer}
\begin{proof}
This follows from problem~2 in \S~2, the fact that \(\varphi-\lambda\iota_E\) restricted to~\(E_1\) is just \(\varphi_1-\lambda\iota_{E_1}\), and \(\proj{\varphi-\lambda\iota_E}=\proj{\varphi}-\lambda\iota_{E/E_1}\).
\end{proof}
\begin{rmk}
Taking \(E_1=\ker\varphi\), we have \(\chi(\varphi_1)=\chi(0_{E_1})=(-\lambda)^p\) where \(p=\dim E_1\), so \(\chi(\varphi)=(-\lambda)^p\chi(\proj{\varphi})\).
\end{rmk}

\begin{exer}[7]
A linear map \(\varphi:E\to E\) is nilpotent if and only if \(\chi(\varphi)=(-\lambda)^n\).
\end{exer}
\begin{proof}
If \(\varphi\)~is nilpotent, we proceed by induction on~\(k\) least such that \(\varphi^k=0\). If \(k=1\), the result is trivial. If \(k>1\), let \(E_1=\ker\varphi\) and \(\proj{\varphi}:E/E_1\to E/E_1\) the induced map. Then \(\proj{\varphi}^{k-1}=0\) since
\[\proj{\varphi}^{k-1}(\proj{x})=\proj{\varphi^{k-1}}(\proj{x})=\proj{\varphi^{k-1}(x)}=0\]
as \(\varphi^{k-1}(x)\in E_1\). By the induction hypothesis, \(\chi(\proj{\varphi})=(-\lambda)^{n-p}\) where \(p=\dim E_1\), so by the previous problem,
\[\chi(\varphi)=(-\lambda)^p(-\lambda)^{n-p}=(-\lambda)^n\]
Conversely, if \(\varphi\ne0\) and \(\chi(\varphi)=(-\lambda)^n\), then the constant term \(\det\varphi=0\), so \(p>0\) and by the previous problem \((-\lambda)^n=(-\lambda)^p\chi(\proj{\varphi})\), which implies \(\chi(\proj{\varphi})=(-\lambda)^{n-p}\). By induction, \(\proj{\varphi}\)~is nilpotent. If \(\proj{\varphi}^k=0\), then \(\varphi^{k+1}=0\), so \(\varphi\)~is nilpotent.
\end{proof}

\subsection*{\S~7}
\begin{exer}[12]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\tr(\varphi_1\dsum\varphi_2)=\tr\varphi_1+\tr\varphi_2\]
\end{exer}
\begin{proof}
Immediate since
\[M(\varphi_1\dsum\varphi_2)=\begin{pmatrix}
M(\varphi_1)&\\
&M(\varphi_2)
\end{pmatrix}\qedhere\]
\end{proof}

\subsection*{\S~8}
\begin{rmk}
In~(4.68), if instead we define
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,e_{p+1},\ldots,e_n)\]
then \(\Delta_1\)~represents the original orientation in~\(E_1\). Indeed, in this case
\[\Delta_1(e_1,\ldots,e_p)=\Delta(e_1,\ldots,e_p,e_{p+1},\ldots,e_n)=\Delta_2(e_{p+1},\ldots,e_n)>0\]
\end{rmk}

\section*{Chapter~V}
\subsection*{\S~1}
\begin{rmk}
An algebra~\(A\) is a \emph{zero algebra} if \(xy=0\) for all \(x,y\in A\); this is equivalent to \(A^2=0\). As an example, \emph{the zero algebra} is the algebra \(A=0\). A zero algebra is unital if and only if it is the zero algebra.
\end{rmk}

\begin{rmk}
Let \(\varphi:A\to B\) be a homomorphism of algebras. If \(A_1\)~is a subalgebra of~\(A\) and \(B_1\)~is a subalgebra of~\(B\) and \(\varphi(A_1)\subseteq B_1\), then the restriction \(\varphi_1:A_1\to B_1\) of~\(\varphi\) to \(A_1,B_1\) is a homomorphism.

If \(A_1\) and~\(B_1\) are \emph{ideals}, then the induced linear map \(\proj{\varphi}:A/A_1\to B/B_1\) is also a homomorphism since
\[\proj{\varphi}(\proj{x}\,\proj{y})=\proj{\varphi}(\proj{xy})=\proj{\varphi(xy)}=\proj{\varphi(x)\varphi(y)}=\proj{\varphi(x)}\,\proj{\varphi(y)}=\proj{\varphi}(\proj{x})\,\proj{\varphi}(\proj{y})\]
\end{rmk}

\noindent In the problems below, \(E\)~is a finite-dimensional vector space.
\begin{exer}[12]
The mapping
\[\Phi:A(E;E)\to\opp{A(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
\(\Phi\)~is a linear isomorphism by problem 9 of chapter II, \S~6, and preserves products since \((\varphi\after\psi)^*=\psi^*\after\varphi^*\).
\end{proof}

\begin{exer}[16]
Every algebra automorphism \(\Phi:A(E;E)\to A(E;E)\) is an \emph{inner} automorphism; that is, there exists \(\alpha\in GL(E)\) such that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{exer}
\begin{proof}
First, observe that every basis~\((e_i)\) of~\(E\) induces a basis~\((\varphi_{ij})\) of~\(A(E;E)\) defined by \(\varphi_{ij}(e_k)=\delta_{jk}e_i\). This basis satisfies
\[\varphi_{ij}\varphi_{lk}=\delta_{jl}\varphi_{ik}\qquad\text{and}\qquad\sum_i\varphi_{ii}=\iota\tag{1}\]
Conversely, every basis satisfying these properties is induced by a basis of~\(E\) in this manner (see problem~14). Moreover, any two of these bases are conjugate to each other via the change of basis transformation between their inducing bases of~\(E\) (see problem~15).

Now fix \((e_i)\) and~\((\varphi_{ij})\) as above. Since \(\Phi\)~is an automorphism, \((\Phi(\varphi_{ij}))\)~is also a basis of~\(A(E;E)\) which satisfies~(1), so there is \(\alpha\in GL(E)\) with \(\Phi(\varphi_{ij})=\alpha\varphi_{ij}\alpha^{-1}\) for all \(i,j\). It follows that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{proof}
\begin{rmk}
The result is true for any nonzero endomorphism~\(\Phi\), since \(A(E;E)\)~is simple (see subsection~5.12).
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
We see the following examples of change of coefficient field of a vector space:
\begin{itemize}
\item Taking the underlying real space of a complex space (5.16): for example changing from \(\C\) over~\(\C\) to \(\C\) over~\(\R\). In this case the dimension is doubled. Moreover, the underlying real space can be decomposed into ``real'' and ``imaginary'' parts of equal dimension (11.7).
\item Complexifying a real space (2.16): for example changing from \(\R\) over~\(\R\) to \(\R^2\) over~\(\C\). In this case the dimension is preserved.
\item Inducing complex structure on a real space (8.21): for example changing from \(\R^2\) over~\(\R\) to \(\R^2\) over~\(\C\). In this case the dimension is halved.
\end{itemize}
\end{rmk}

\section*{Chapter~VI}
\subsection*{\S~1}
\begin{rmk}
The space of polynomials in one variable is positively graded by the degrees of monomials. More generally, the space of polynomials in \(p\)~variables is \(p\)-graded by the multidegrees of monomials.
\end{rmk}

\begin{rmk}
If \(E=\sum_{k\in G}E_k\) is a \(G\)-graded space and \(F=\sum_{k\in G}F\sect E_k\) is a \(G\)-graded subspace, then \(E/F=\sum_{k\in G}E_k/(F\sect E_k)\) is the \(G\)-graded factor space.\footnote{See problem~2 in chapter~II, \S~4.}
\end{rmk}

\begin{rmk}
The zero map between two \(G\)-graded vector spaces is homogeneous of every degree. A nonzero homogeneous map has a unique degree.
\end{rmk}

\begin{rmk}
Let \(E\) and~\(F\) be \(G\)-graded vector spaces. If \(\varphi:E\to F\) is linear and homogeneous of degree~\(k\) and \(\varphi x\) is homogeneous of degree~\(l\), then we may assume without loss of generality that \(x\)~is homogeneous of degree \(l-k\). Indeed, writing \(x=\sum_m x_m\) with \(\deg x_m=m\), we have \(\varphi x=\sum_m\varphi x_m\) with \(\deg(\varphi x_m)=m+k\). Since \(\deg(\varphi x)=l\), we must have \(\varphi x_m=0\) for \(m\ne l-k\), so \(\varphi x=\varphi x_{l-k}\).
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional \(G\)-graded vector space and \(\varphi:E\to E\) is linear and homogeneous with \(\deg\varphi\ne 0\), then \(\tr\varphi=0\).
\end{rmk}
\begin{proof}
Write \(E=E_{k_1}\dsum\cdots\dsum E_{k_n}\) with \(k_i\in G\) and \(d_i=\dim E_{k_i}<\infty\). Let \((e_{ij})\)~be a basis of~\(E\) such that for each \(1\le i\le n\), \((e_{ij})\)~is a basis of~\(E_{k_i}\) for \(1\le j\le d_i\). Let \(\Delta\)~be a determinant function in~\(E\) with \(\Delta(e_{ij})=1\). Then
\[\tr\varphi=\sum_{i,j}\Delta(e_{11},\ldots,e_{1d_1},\ldots,\varphi(e_{ij}),\ldots,e_{n1},\ldots,e_{n\,d_n})\]
By assumption, \(\varphi(e_{ij})\in E_{k_l}\) for some \(l\ne i\), so each term in this sum is zero, and hence \(\tr\varphi=0\).
\end{proof}
\noindent As an example, formal differentiation in the space of polynomials of degree at most~\(n\) (graded by the degrees of monomials) is homogeneous of degree~\(-1\), so has zero trace. This is also obvious from its matrix representation with respect to the standard basis.

\begin{exer}[6]
Let \(E,E^*\) and \(F,F^*\) be pairs of dual \(G\)-graded vector spaces and let \(\varphi:E\to F\) and \(\varphi^*:E^*\from F^*\) be dual linear maps. If \(\varphi\)~is homogeneous of degree~\(k\), then \(\varphi^*\)~is homogeneous of degree~\(-k\).
\end{exer}
\begin{proof}
We have direct sum decompositions
\[E=\sum_{m\in G}E_m\qquad E^*=\sum_{m\in G}E^{*m}\]
and
\[F=\sum_{n\in G}F_n\qquad F^*=\sum_{n\in G}F^{*n}\]
where the pairs \(E_m,E^{*m}\) and \(F_n,F^{*n}\) are dual for all \(m,n\) under the restrictions of the scalar products between \(E,E^*\) and \(F,F^*\), respectively (see subsection~6.5). We also have \(\varphi E_m\subseteq F_{m+k}\) for all~\(m\). We must prove \(\varphi^* F^{*n}\subseteq E^{*n-k}\) for all~\(n\).

Let \(y^*\in F^{*n}\) and \(x\in E\). Write \(x=\sum_m x_m\) where \(x_m\in E_m\). Then
\[\sprod{\varphi^* y^*}{x}=\sprod{y^*}{\varphi x}=\sum_m\sprod{y^*}{\varphi x_m}=\sprod{y^*}{\varphi x_{n-k}}=\sprod{\varphi^* y^*}{x_{n-k}}\]
which implies
\[\sprod{\varphi^* y^*}{x-\pi_{n-k}x}=0\tag{1}\]
where \(\pi_{n-k}:E\to E_{n-k}\) is the canonical projection. Now write \(\varphi^* y^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*m}=0\) for all \(m\ne n-k\). Indeed, for \(m\ne n-k\) and \(x\in E_m\) we have \(\pi_{n-k}x=0\), so by~(1)
\[\sprod{x^{*m}}{x}=\sum_p\sprod{x^{*p}}{x}=\sprod{\varphi^* y^*}{x}=0\]
Therefore \(x^{*m}=0\). It follows that \(\varphi^* y^*=x^{*n-k}\in E^{*n-k}\), as desired.
\end{proof}

\begin{exer}[8]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. If \(F\)~is a \(G\)-graded subspace of~\(E\), then \(\oc{F}\)~is a \(G\)-graded subspace of~\(E^*\) and \(\occ{F}=F\).
\end{exer}
\begin{proof}
We have direct sums \(E=\sum_{m\in G}E_m\) and \(E^*=\sum_{m\in G}E^{*m}\) where the pairs \(E_m,E^{*m}\) are dual under the restrictions of the scalar product between \(E,E^*\) and \(\dim E_m=\dim E^{*m}<\infty\) for all~\(m\). By assumption, \(F=\sum_{m\in G}F\sect E_m\).
We must prove
\[\oc{F}=\sum_{m\in G}\oc{F}\sect E^{*m}\tag{1}\]
Let \(x^*\in\oc{F}\) and write \(x^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*n}\in\oc{F}\) for all~\(n\). Indeed, if \(x\in F\), write \(x=\sum_m x_m\) where \(x_m\in F\sect E_m\). Then
\[\sprod{x^{*n}}{x}=\sum_m\sprod{x^{*n}}{x_m}=\sprod{x^{*n}}{x_n}=\sum_m\sprod{x^{*m}}{x_n}=\sprod{x^*}{x_n}=0\]
This establishes~(1). By symmetry, we have
\[\occ{F}=\sum_{m\in G}\occ{F}\sect E_m\tag{2}\]
We claim \(\occ{F}\sect E_n\subseteq F\sect E_n\) for all~\(n\). To prove this, we first show
\[\occ{F}\sect E_n\subseteq(F\sect E_n)^{\perp_n\perp_n}\tag{3}\]
where \(\perp_n\)~is taken relative to the scalar product between \(E_n,E^{*n}\). Indeed, let \(x\in\occ{F}\sect E_n\) and \(x^*\in(F\sect E_n)^{\perp_n}\subseteq E^{*n}\). If \(y\in F\), write \(y=\sum_m y_m\) where \(y_m\in F\sect E_m\). Then
\[\sprod{x^*}{y}=\sum_m\sprod{x^*}{y_m}=\sprod{x^*}{y_n}=0\]
This implies \(x^*\in\oc{F}\), which implies \(\sprod{x^*}{x}=0\), which in turn implies~(3). Now \((F\sect E_n)^{\perp_n\perp_n}=F\sect E_n\) since \(\dim E_n<\infty\), which establishes the claim. Finally, it follows from~(2) that \(\occ{F}=F\).
\end{proof}

\subsection*{\S~2}
\begin{exer}[1]
Let \(A\)~be a \(G\)-graded algebra. If \(x\in A\) is an invertible element homogeneous of degree~\(k\), then \(x^{-1}\)~is homogeneous of degree~\(-k\). If \(A\)~is nonzero and positively graded, then \(k=0\).
\end{exer}
\begin{proof}
Write \(A=\sum_{m\in G}A_m\) and \(x^{-1}=\sum_m x_m\) with \(x_m\in A_m\). Then
\[e=xx^{-1}=\sum_m xx_m\]
Since \(\deg e=0\) and \(\deg(xx_m)=m+k\), it follows that \(xx_m=0\) for all \(m\ne-k\). Therefore \(e=xx_{-k}\) and \(x^{-1}=x_{-k}\), so \(x^{-1}\)~is homogeneous of degree~\(-k\).

If \(A\ne 0\), then \(x\ne 0\) and \(x^{-1}\ne 0\), so \(A_k\ne 0\) and \(A_{-k}\ne 0\). If \(A\)~is positively graded, this forces \(k=0\).
\end{proof}

\begin{exer}[4]
Let \(E\)~be a \(G\)-graded vector space. Then the subspace \(A_G(E;E)\) of~\(A(E;E)\) generated by homogeneous linear transformations of~\(E\) forms a \(G\)-graded subalgebra of~\(A(E;E)\).
\end{exer}
\begin{proof}
First observe that \(A_G(E;E)\)~is naturally graded as a vector space by the degrees of homogeneous transformations (see problem~3). If \(\varphi,\psi\in A_G(E;E)\) are homogeneous with \(\deg\varphi=m\) and \(\deg\psi=n\), then it is obvious that \(\varphi\psi\)~is homogeneous with \(\deg(\varphi\psi)=m+n\). It follows from this that \(A_G(E;E)\)~is a \(G\)-graded subalgebra.
\end{proof}

\begin{exer}[7]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. Then the mapping
\[\Phi:A_G(E;E)\to\opp{A_G(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
\(\Phi\)~is well defined by problems 6 and~10 of~\S~1, and is an isomorphism by problem~12 of chapter~V, \S~1.
\end{proof}

\section*{Chapter~VII}
\textbf{In this chapter, all vector spaces are real.}

\subsection*{\S~1}
\begin{rmk}
The Riesz representation theorem shows that for a finite-dimensional \emph{inner product space}~\(E\), there is a \emph{natural} isomorphism between \(E\) and its dual space~\(L(E)\) given by \(x\mapsto\iprod{x}{-}\). This is unlike for a finite-dimensional vector space, where the isomorphism is in general non-natural, and means we may naturally \emph{identify} a vector with its dual vector.
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional inner product space and \(E_1\)~is a subspace of~\(E\), then \(E\)~induces an inner product in~\(E/E_1\) by
\[\iprod{\proj{x}}{\proj{y}}=\iprod{x}{y}\]
where \(x,y\) are the unique representatives of \(\proj{x},\proj{y}\) in~\(\oc{E_1}\) (see problem~5).
\end{rmk}
\begin{proof}
The only thing to check is bilinearity, which follows from the fact that \(\oc{E_1}\)~is a subspace of~\(E\).
\end{proof}

\begin{exer}[5]
If \(E\)~is a finite-dimensional inner product space and \(E_1\)~is a subspace of~\(E\), then every element of~\(E/E_1\) has exactly one representative in~\(\oc{E_1}\).
\end{exer}
\begin{proof}
By duality, \(\dim E=\dim E_1+\dim\oc{E_1}\), and by definiteness of the inner product, \(E_1\sect\oc{E_1}=0\), so \(E=E_1\dsum\oc{E_1}\). For \(x+E_1\in E/E_1\), let \(y=\pi(x)-x\) where \(\pi\)~is the canonical projection onto~\(\oc{E_1}\). Then \(y\in E_1\) so \(x+y\in x+E_1\) and \(x+y\in\oc{E_1}\), as desired. If \(z\in E_1\) and \(x+z\in\oc{E_1}\), then
\[y-z=(y+x)-(x+z)\in E_1\sect\oc{E_1}=0\]
so \(y=z\), establishing uniqueness.
\end{proof}

\subsection*{\S~2}
\begin{rmk}
Let \(E\)~be an inner product space of finite dimension~\(n\). We provide an inductive proof of the existence of an orthonormal basis in~\(E\).

For \(n=0,1\), the result is trivial. For \(n>1\), fix a unit vector \(e_1\in E\) and let \(E_1=\gen{e_1}\). By induction, there is an orthonormal basis \(\proj{e_2},\ldots,\proj{e_n}\) in the induced inner product space \(E/E_1\) (see above). Letting \(e_2,\ldots,e_n\) be the representatives in~\(\oc{E_1}\), it follows that \(e_1,\ldots,e_n\) is an orthonormal basis in~\(E\).
\end{rmk}

\begin{rmk}
In the Gram-Schmidt process, we can just let \(e_1=a_1/\norm{a_1}\) and
\[e_k=\frac{a_k-\iprod{a_k}{e_1}e_1-\cdots-\iprod{a_k}{e_{k-1}}e_{k-1}}{\norm{a_k-\iprod{a_k}{e_1}e_1-\cdots-\iprod{a_k}{e_{k-1}}e_{k-1}}}\qquad(k=2,\ldots,n)\]
At each step, we compute the difference between the current vector~\(a_k\) and its orthogonal projection onto the subspace generated by the previous vectors, then normalize (compare (7.19)).
\end{rmk}

\begin{rmk}
If \(E\)~is a finite-dimensional inner product space and \(\varphi:E\to E\) is linear, then the dual transformation \(\varphi^*:E\to E\) satisfies
\[\iprod{\varphi^* x}{y}=\iprod{x}{\varphi y}\]
If \(\varphi\)~preserves inner products, then it also preserves orthonormal bases and is invertible. In this case, \(\varphi^{-1}\)~also preserves inner products, so
\[\iprod{\varphi^{-1}x}{y}=\iprod{\varphi^{-1}x}{\varphi^{-1}\varphi y}=\iprod{x}{\varphi y}\]
and it follows that \(\varphi^{-1}=\varphi^*\). Conversely if \(\varphi^{-1}=\varphi^*\), then
\[\iprod{\varphi x}{\varphi y}=\iprod{\varphi^*\varphi x}{y}=\iprod{x}{y}\]
so \(\varphi\)~preserves inner products.

Such a~\(\varphi\) is called an \emph{orthogonal} transformation. Note \(\varphi\)~is orthogonal if and only if \(M(\varphi)\)~is an orthogonal matrix relative to an orthonormal basis.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
A determinant function~\(\Delta\) in an inner product space~\(E\) is normed if and only if \(\abs{\Delta(e_1,\ldots,e_n)}=1\) for any orthonormal basis \(e_1,\ldots,e_n\). If \(E\)~is oriented and \(\Delta\)~is the normed determinant function representing the orientation, then \(\Delta(e_1,\ldots,e_n)=1\) if \(e_1,\ldots,e_n\) is positive. Geometrically, this is just \emph{the} oriented volume function in~\(E\). By comparing (4.26) and~(7.23), we see that \emph{the normed determinant functions are precisely the self-dual determinant functions}.

If \(\Delta_0\ne 0\) is a determinant function in~\(E\) and \(\Delta_0^*\)~is its dual, then \(\Delta_0^*=\alpha\Delta_0\) for some real number~\(\alpha\) by uniqueness of~\(\Delta_0\), so
\[\alpha\Delta_0(x_1,\ldots,x_n)\Delta_0(y_1,\ldots,y_n)=\det\iprod{x_i}{y_j}\qquad(x_i,y_i\in E)\]
Substituting \(x_i=y_i=e_i\) shows that \(\alpha>0\). It follows that \(\Delta_1=\pm\sqrt{\alpha}\mult\Delta_0\) is a normed determinant function.
\end{rmk}

\begin{rmk}
It follows from~(7.37) that if \(x\) and~\(y\) are linearly independent, then
\[\norm{x\cross y}=\Delta(x,y,z)\]
where \(z=(x\cross y)/\norm{x\cross y}\). Since \(z\)~is a unit vector orthogonal to \(x\) and~\(y\), it follows (subsection 7.15) that \(\Delta(x,y,z)\)~is just the area of the parallelogram determined by \(x\) and~\(y\). In other words,
\[\norm{x\cross y}=\norm{x}\norm{y}\sin\theta\]
where \(0\le\theta\le\pi\) is the angle between \(x\) and~\(y\). This last equation obviously still holds if \(x\) and~\(y\) are linearly dependent but nonzero.
\end{rmk}

\begin{exer}[12]
For vectors \(x_1,\ldots,x_p\),
\[G(x_1,\ldots,x_p)\le\norm{x_1}^2\cdots\norm{x_p}^2\tag{1}\]
Additionally
\[\begin{vmatrix}
a_{11}&\cdots&a_{1n}\\
\vdots&\ddots&\vdots\\
a_{n1}&\cdots&a_{nn}
\end{vmatrix}^2\le\sum_{k=1}^n\abs{a_{1k}}^2\cdots\sum_{k=1}^n\abs{a_{nk}}^2\tag{2}\]
\end{exer}
\begin{proof}
By induction with the ``base times height'' rule for volume (subsection 7.15), \(V(u_1,\ldots,u_p)\le 1\) for linearly independent \emph{unit} vectors \(u_1,\ldots,u_p\).

Now if \(x_1,\ldots,x_p\) are linearly dependent, then \(G(x_1,\ldots,x_p)=0\) and (1)~holds trivially. Otherwise, setting \(u_i=x_i/\norm{x_i}\) we have
\[\sqrt{G(x_1,\ldots,x_p)}=V(x_1,\ldots,x_p)=\norm{x_1}\cdots\norm{x_p}\mult V(u_1,\ldots,u_p)\le\norm{x_1}\cdots\norm{x_p}\]
Squaring both sides yields~(1). Since the determinant of a matrix is a (normed) determinant function of the rows, (2)~follows.
\end{proof}
\begin{rmk}
These results both simply express an upper bound for the volume of a parallelepiped in terms of the lengths of the edges.
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The covariant components of a vector are just its coordinates in the dual space, which are also just the entries of its matrix (as a linear function). They are called ``covariant'' because they vary in the same way as basis vectors in the original space under a change of basis (see subsections 3.13--14). This is unlike the regular components of the vector, which vary inversely to the basis vectors and may be called the \emph{contravariant components}.

To see why the covariant components of a vector are its inner products with the basis vectors, observe from (7.42) and~(7.49) that
\[\iprod{x}{e_{\nu}}=\sprod{\tau x}{e_{\nu}}=\sum_{\lambda}\xi_{\lambda}\sprod{e^{*\lambda}}{e_{\nu}}=\xi_{\nu}\]
Dually, the contravariant components of a vector are its inner products with the dual basis vectors:
\[\iprod{\tau x}{e^{*\nu}}=\xi^{\nu}\]
\end{rmk}

\begin{rmk}
If \(e_{\nu}\)~is an orthonormal basis of~\(E\), then the dual basis \(e^{*\nu}=\tau e_{\nu}\) is an orthonormal basis of~\(E^*\).
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
In subsection~7.22, observe that \(Q\)~is just the set of unit vectors under the norm defined in example~3 in subsection~7.20. Since norm functions are continuous under the natural topology, \(Q\)~is closed under this topology, and \(Q\)~is also clearly bounded under this topology. It follows that \(Q\)~is compact under this topology since \(E\)~is finite-dimensional.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(x=\lambda e+x_1\) and \(y=\mu e+y_1\) are quaternions (\(\lambda,\mu\in\R\) and \(x_1,y_1\in E_1\)), then by the definition of quaternion multiplication
\[xy=\bigl(\lambda\mu-\iprod{x_1}{y_1}\bigr)e+\lambda y_1+\mu x_1+x_1\cross y_1\]
\end{rmk}

\begin{rmk}
In subsection~7.24, in the proof of Lemma~I, observe that the result holds trivially if \(y=\pm x\) by taking \(\lambda=\mp 1\). If \(y\ne\pm x\), then \(e\ne 0\). Suppose
\[\alpha x+\beta y+\gamma e=0\qquad(\alpha,\beta,\gamma\in\R)\]
If \(\alpha\ne 0\), then \(x=\beta_1y+\gamma_1e\) for \(\beta_1,\gamma_1\in\R\), so
\[-e=x^2=(\beta_1y+\gamma_1e)^2=2\beta_1\gamma_1y+(\gamma_1^2-\beta_1^2)e\]
which implies
\[2\beta_1\gamma_1y=(\beta_1^2-\gamma_1^2-1)e\]
If \(\beta_1=0\), it follows that \(\gamma_1^2=-1\), which is impossible. If \(\gamma_1=0\), it follows that \(\beta_1=\pm 1\), so \(x=\pm y\) contrary to assumption. Therefore \(\beta_1\gamma_1\ne 0\), so \(y=\delta e\) for \(\delta\in\R\). But then \(-e=y^2=\delta^2 e\), so \(\delta^2=-1\), which is impossible. It follows that \(\alpha=0\). Similarly \(\beta=0\). Finally, \(\gamma=0\). This result shows that the vectors \(x,y,e\) are linearly independent.

Now \(x+y\) and \(x-y\) are roots of polynomials of degree \(1\) or~\(2\), but the linear independence of \(x,y,e\) implies that these polynomials must have degree~\(2\), so (7.60) and~(7.61) follow.
\end{rmk}

\section*{Chapter~VIII}
\textbf{In this chapter, all vector spaces are real and finite-dimensional.}

\begin{rmk}
In this chapter, it is useful to think intuitively of transformations like complex numbers, which induce transformations of the complex plane through multiplication. Under this analogy, adjoints correspond to complex conjugates; self-adjoint transformations, to real numbers; positive transformations, to nonnegative real numbers; skew transformations, to purely imaginary numbers; and isometries, to complex numbers on the unit circle.
\end{rmk}

\subsection*{\S~1}
\begin{rmk}
In subsection~8.2, if the bases \((x_{\nu})\) and~\((y_{\mu})\) are orthonormal, then they are self-dual,\footnote{See problem~2 in chapter~VII, \S~2.} so \(M(\adj{\varphi},y_{\mu},x_{\nu})=M(\varphi,x_{\nu},y_{\mu})^*\) by~(3.4). It follows that
\[\tilde{\alpha}^{\varrho}_{\mu}=M(\adj{\varphi})^{\varrho}_{\mu}=M(\varphi)^{\mu}_{\varrho}=\alpha^{\mu}_{\varrho}\]
\end{rmk}

\begin{rmk}
Recall from subsection~7.18 the natural isomorphism \(\tau:E\to L(E)\) given by \(x\mapsto\iprod{x}{-}\), which maps each vector to its dual vector. In subsection~8.4, observe that there is a natural isomorphism
\[B(E,E)\iso L(E;L(E))\]
given by \(\Phi\mapsto(x\mapsto(y\mapsto\Phi(x,y)))\). For a linear transformation \(\varphi:E\to E\) and the bilinear function~\(\Phi\) defined by \(\Phi(x,y)=\iprod{\varphi x}{y}\), the linear map corresponding to~\(\Phi\) under this isomorphism is~\(\tau\varphi\):
\begin{diagram}[nohug]
x	&\rTo^{\varphi}		&\varphi x\\
	&\rdTo<{\Phi}		&\dTo>{\tau}\\
	&					&\iprod{\varphi x}{-}
\end{diagram}
In this sense, \(\Phi\)~is naturally dual to~\(\varphi\). Properties of~\(\Phi\) naturally correspond to those of~\(\varphi\); for example, \(\Phi\)~is symmetric if and only if \(\varphi\) is symmetric (self-adjoint), and \(\Phi\)~is skew symmetric if and only if \(\varphi\)~is skew symmetric. Since
\[\Phi(x,y)=\iprod{\varphi x}{y}=\iprod{x}{\adj{\varphi}y}\]
we see that \(\varphi\)~is ``left-dual'' to~\(\Phi\), while \(\adj{\varphi}\)~is ``right-dual'' to~\(\Phi\). It might be said that \(\Phi\), \(\varphi\), and \(\adj{\varphi}\) enter into a holy trinity.\footnote{No one says this.}
\end{rmk}

\begin{rmk}
In subsection~8.5, observe for a direct sum \(E=\sum E_i\) and \(\varphi=\sum\varphi_i\) with \(\varphi_i:E_i\to E_i\), if \(E_i\)~is stable under~\(\adj{\varphi}\) then the restriction of~\(\adj{\varphi}\) to~\(E_i\) is the adjoint of~\(\varphi_i\). In other words, if \(\adj{\varphi}_i\)~denotes the restriction, then
\[\adj{\varphi}_i=\adj{\varphi_i}\]
Indeed, for \(x,y\in E_i\),
\[\iprod{\adj{\varphi}_i x}{y}=\iprod{\adj{\varphi}x}{y}=\iprod{x}{\varphi y}=\iprod{x}{\varphi_i y}\]
It follows that \(\adj{\varphi}=\sum\adj{\varphi_i}\). If additionally the \(E_i\)~are orthogonal, we see that \(\varphi\)~is normal if and only if each \(\varphi_i\)~is normal.
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
Geometrically, a self-adjoint transformation just independently scales the axes in some system of orthogonal axes for the space.
\end{rmk}

\begin{rmk}
For any transformation~\(\varphi\), the transformations
\[\varphi+\adj{\varphi}\qquad\varphi\adj{\varphi}\qquad\adj{\varphi}\varphi\]
are self-adjoint.
\end{rmk}

\begin{rmk}
If \(E=E_1\dsum E_2\) with \(E_1\orth E_2\), and \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are self-adjoint, then \(\varphi=\varphi_1\dsum\varphi_2\) is self-adjoint.
\end{rmk}
\begin{proof}
For \(x=x_1+x_2\) and \(y=y_1+y_2\) with \(x_i,y_i\in E_i\),
\begin{align*}
\iprod{\varphi x}{y}&=\iprod{\varphi_1 x_1+\varphi_2 x_2}{y_1+y_2}\\
	&=\iprod{\varphi_1 x_1}{y_1}+\iprod{\varphi_2 x_2}{y_2}\\
	&=\iprod{x_1}{\varphi_1 y_1}+\iprod{x_2}{\varphi_2 y_2}\\
	&=\iprod{x_1+x_2}{\varphi_1 y_1+\varphi_2 y_2}\\
	&=\iprod{x}{\varphi y}\qedhere
\end{align*}
\end{proof}

\begin{rmk}
In subsection~8.6, consider \(E=\R^n\). Since \(e_1\)~minimizes the quadratic form \(q(x)=\iprod{x}{\varphi x}\) subject to the constraint \(h(x)=1-\iprod{x}{x}=0\), there exists a Lagrange multiplier \(\lambda\in\R\) such that
\[\grad q(e_1)+\lambda\grad h(e_1)=0\]
Now \(\grad h(e_1)=-2e_1\), and since \(\varphi\)~is self-adjoint it follows that \(\grad q(e_1)=2\varphi e_1\). Therefore \(\varphi e_1=\lambda e_1\), so \(e_1\)~is an eigenvector of~\(\varphi\) with eigenvalue~\(\lambda\).
\end{rmk}

\begin{exer}[5]
Every positive transformation~\(\varphi\) has a unique positive square root (that is, a positive transformation~\(\psi\) such that \(\psi^2=\varphi\)).
\end{exer}
\begin{proof}
There is an orthonormal basis \(e_1,\ldots,e_n\) of eigenvectors of~\(\varphi\), so that \(\varphi e_i=\lambda_i e_i\) for \(\lambda_i\in\R\). Now \(\lambda_i=\iprod{e_i}{\varphi e_i}\ge 0\), so setting \(\psi e_i=\sqrt{\lambda_i}e_i\) it follows that \(\psi\)~is positive with \(\psi^2=\varphi\).

If \(\psi_1\)~is positive with \(\psi_1^2=\varphi\), then the eigenvalues of~\(\psi_1\) must be \(\sqrt{\lambda_i}\). Also, if \(\psi_1x=\sqrt{\lambda_i}x\), then \(\varphi x=\lambda_i x\), so \(E_{\psi_1}(\sqrt{\lambda_i})\subseteq E_{\varphi}(\lambda_i)\). By~(8.21), it follows that \(E_{\psi_1}(\sqrt{\lambda_i})=E_{\varphi}(\lambda_i)\), so \(\psi_1 e_i=\sqrt{\lambda_i}e_i\) and \(\psi_1=\psi\).
\end{proof}

\subsection*{\S~3}
\begin{rmk}
In subsection~2.19, we see that a linear transformation~\(\pi\) is a projection operator (that is, \(\pi^2=\pi\)) if and only if \(\pi=0_{\ker\pi}\dsum\iota_{\im\pi}\). In subsection~8.11, we see that \(\pi\)~is additionally an \emph{orthogonal} projection if and only if \(\ker\pi\orth\im\pi\). In summary, for a projection operator~\(\pi\), the following are equivalent:
\begin{itemize}[itemsep=0pt]
\item \(\ker\pi\orth\im\pi\).
\item \(\pi\)~is self-adjoint.
\item \(\pi\)~is normal.
\end{itemize}
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
Geometrically, a skew transformation, apart from possibly killing off part of the space, induces scaled 90-degree rotations\footnote{No one calls these ``scrotations''.} on orthogonal stable planes in the space.
\end{rmk}

\begin{rmk}
For any transformation~\(\varphi\), the transformation \(\varphi-\adj{\varphi}\) is skew. Also
\[\varphi=\tfrac{1}{2}(\varphi+\adj{\varphi})+\tfrac{1}{2}(\varphi-\adj{\varphi})\]
uniquely represents~\(\varphi\) as a sum of self-adjoint and skew transformations.
\end{rmk}

\begin{rmk}
If \(\varphi\)~is a skew transformation of~\(E\) and \(F\)~is stable subspace of~\(E\), then \(\oc{F}\)~is also stable.
\end{rmk}
\begin{proof}
If \(x\in F\) and \(y\in\oc{F}\), then \(\iprod{x}{\varphi y}=-\iprod{\varphi x}{y}=0\).
\end{proof}

\begin{rmk}
In subsection~8.16, the proof of the normal form~(8.35) is incorrect because it is not true in general that the~\(a_n\) defined form an orthonormal basis of the space. For example in~\(\R^4\), if we define the transformation \(\psi\) by
\[e_1\mapsto e_2\qquad e_2\mapsto -e_1\qquad e_3\mapsto e_4\qquad e_4\mapsto -e_3\]
where \(e_i\)~is the \(i\)-th standard basis vector, then \(\psi\)~is skew and \(\varphi=\psi^2=-\iota\) is diagonalized by the standard basis. If we follow the proof for this example, we get \(a_1=e_1\), \(a_2=\psi e_1=e_2\), \(a_3=e_2\), and \(a_4=\psi e_2=-e_1\), so the~\(a_n\) do not form a basis of~\(\R^4\).

To prove the result, let \(\lambda_1,\ldots,\lambda_r\) be the distinct eigenvalues of \(\varphi=\psi^2\) and write the orthogonal decomposition~(8.21)
\[E=E_1\dsum\cdots\dsum E_r\]
where \(E_i\)~is the eigenspace of~\(\varphi\) corresponding to~\(\lambda_i\). Observe that \(E_i\)~is stable under~\(\psi\), for if \(x\in E_i\) then
\[\varphi(\psi x)=\psi(\varphi x)=\psi(\lambda_i x)=\lambda_i(\psi x)\]
so \(\psi x\in E_i\). As in the book, the~\(\lambda_i\) are negative or zero. If \(\lambda_i<0\), construct a basis for \(F=E_i\) in the following way: let \(a_1\)~be an arbitrary unit vector in~\(F\) and \(a_2=\kappa_i^{-1}\psi a_1\) where \(\kappa_i=\sqrt{-\lambda_i}\). It is immediate that \(a_2\)~is a unit vector in~\(F\) orthogonal to~\(a_1\) and \(H=\gen{a_1,a_2}\) is stable under~\(\psi\). Let \(\oc{H}\)~be the orthogonal complement of~\(H\) in~\(F\). If \(\oc{H}=0\), take the basis \(a_1,a_2\). Otherwise, adjoin to \(a_1,a_2\) the result of applying this procedure recursively to \(F=\oc{H}\) (recall that \(\oc{H}\)~is stable under~\(\psi\) by the remark above). If \(\lambda_i=0\), assume that \(i=r\) and choose any orthonormal basis of~\(E_r\), noting that \(\psi\)~is zero on~\(E_r\) since
\[\norm{\psi x}^2=\iprod{\psi x}{\psi x}=-\iprod{x}{\varphi x}=0\]
for \(x\in E_r\). Combine the resulting bases of the~\(E_i\) to obtain an orthonormal basis of~\(E\) with respect to which the matrix of~\(\psi\) has the form~(8.35).\footnote{See \url{https://math.stackexchange.com/q/3402347}.}
\end{rmk}

\begin{exer}[1]
If \(\varphi\)~is a skew transformation of a plane, then
\[\iprod{\varphi x}{\varphi y}=\det\varphi\mult\iprod{x}{y}\]
\end{exer}
\begin{proof}
If \(\varphi=0\), then the result is trivial; if \(\varphi\ne 0\), then \(\varphi\)~is an automorphism since it has even rank. Now \(\Delta(x,y)=\iprod{\varphi x}{y}\) is a determinant function, so
\[\iprod{\varphi^2 x}{\varphi y}=\Delta(\varphi x,\varphi y)=\det\varphi\mult\Delta(x,y)=\det\varphi\mult\iprod{\varphi x}{y}\]
Substituting \(\varphi^{-1}x\) for~\(x\) yields the result.
\end{proof}

\begin{exer}[2 - Skew transformations of 3-space]
Let \(E\)~be an oriented Euclidean 3-space.
\begin{enumerate}
\item[(i)] For \(a\in E\), \(\varphi_a(x)=a\cross x\) is a skew transformation of~\(E\).
\item[(ii)] For \(a,b\in E\), \(\varphi_{a\cross b}=\varphi_a\varphi_b-\varphi_b\varphi_a\).
\item[(iii)] If \(\varphi\)~is a skew transformation of~\(E\), then \(\varphi=\varphi_a\) for a unique \(a\in E\).
\item[(iv)] The vector~\(a\) in~(iii) is given by
\[a=\alpha_{23}e_1+\alpha_{31}e_2+\alpha_{12}e_3\]
where \(e_1,e_2,e_3\) is a positive orthonormal basis of~\(E\) and \((\alpha_{ij})=M(\varphi;e_i)\).
\item[(v)] If \(a\ne 0\), then \(\ker\varphi_a=\gen{a}\) and \(\oc{a}\)~is stable under~\(\varphi_a\).
\item[(vi)] If \(e_1,e_2\) are normal such that \(e_1,e_2,a\) is a positive orthogonal basis of~\(E\), then
\[M(\varphi_a;e_1,e_2,a)=\begin{pmatrix}
0&\norm{a}&0\\
-\norm{a}&0&0\\
0&0&0
\end{pmatrix}\]
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] It is a transformation since the cross product is bilinear, and it is skew since \(\iprod{x}{a\cross x}=0\) for all \(x\in E\).
\item[(ii)] By the vector triple product formula~(7.41),
\[(a\cross b)\cross x=a\cross(b\cross x)-b\cross(a\cross x)\]
\item[(iii)] For uniqueness, note that if \(\varphi_a=\varphi_b\), then \(a\cross x=b\cross x\) for all \(x\in E\), so \((a-b)\cross x=0\) for all \(x\in E\), so \(a-b=0\) and \(a=b\).

For existence, note that the bilinear function \(\iprod{\varphi x}{y}\) is skew symmetric, and hence a determinant function in any plane in~\(E\). Define
\[\Phi(x,y,z)=\iprod{\varphi y}{z}x+\iprod{\varphi z}{x}y+\iprod{\varphi x}{y}z\]
Then \(\Phi\)~is clearly trilinear and skew symmetric. By the universal property of determinants if \(\Delta\)~is the normed determinant function representing the orientation in~\(E\) and \(e_1,e_2,e_3\) is an orthonormal basis with \(\Delta(e_1,e_2,e_3)=1\), then \(\Phi=\Delta a\) where \(a=\Phi(e_1,e_2,e_3)\).\footnote{See Proposition~III in subsection~4.3.} By direct computation it is easily verified that \(a\cross e_i=\varphi e_i\), so \(\varphi=\varphi_a\).
\item[(iv)] By the proof of~(iii), noting that \(a=\Phi(e_1,e_2,e_3)\) and \(\alpha_{ij}=\iprod{\varphi e_i}{e_j}\).
\item[(v)] By the fact that \(a\cross x=0\) if and only if \(a\) and~\(x\) are linearly dependent.
\item[(vi)] By direct computation.\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
The definition of~\(\Phi\) in~(iii) is motivated by the fundamental relationship between \(n\)-dimensional and \((n-1)\)-dimensional determinant functions discussed in \S~1 of chapter~IV above. If \(\varphi\)~is skew, then its dual bilinear function \(\Delta_1(x,y)=\iprod{\varphi x}{y}\) is a determinant function in any plane in~\(E\). If \(\varphi=\varphi_a\), then
\[\Delta_1(x,y)=\iprod{a\cross x}{y}=\Delta(a,x,y)\]
where \(\Delta\)~is the normed determinant function representing the orientation in~\(E\). In other words, \(\Delta_1\)~is just the 2-dimensional area function induced by the 3-dimensional volume function~\(\Delta\) and the vector~\(a\). Therefore if we \emph{define} a 3-dimensional volume map~\(\Phi\) in terms of~\(\Delta_1\), we should expect that \(\Phi=\Delta a\).

This result, which shows that any skew linear \emph{transformation} of the space can be represented by a unique vector under the \emph{cross} product, is analogous to the Riesz representation theorem, which shows that any linear \emph{function} of the space can be represented by a unique vector under the \emph{inner} product.
\end{rmk}

\begin{exer}[3]
If \(\varphi\ne 0\) and~\(\psi\) are skew transformations of an oriented Euclidean 3-space with \(\ker\varphi\subseteq\ker\psi\), then \(\psi=\lambda\varphi\) for some \(\lambda\in\R\).
\end{exer}
\begin{proof}
By the previous problem, we can write \(\varphi(x)=a\cross x\) and \(\psi(x)=b\cross x\) with \(a\ne 0\). By assumption, \(a\) and~\(b\) are orthogonal to the same plane, so \(b=\lambda a\) for some \(\lambda\in\R\) and hence \(\psi=\lambda\varphi\).
\end{proof}

\begin{exer}[4]
\[(a_1\cross a_2)\cross a_3=a_2(a_1,a_3)-a_1(a_2,a_3)\]
\end{exer}
\begin{proof}
Without loss of generality, we may assume that \(a_1,a_2\) are orthonormal. In particular, \(a_1\cross a_2\ne 0\). Define
\[\varphi(x)=(a_1\cross a_2)\cross x\qquad\text{and}\qquad\psi(x)=a_2(a_1,x)-a_1(a_2,x)\]
Then \(\varphi\ne 0\) is skew, and \(\psi\)~is skew since \(\iprod{x}{\psi x}=0\) for all~\(x\). The kernel of~\(\varphi\) is the line determined by~\(a_1\cross a_2\), which is killed by~\(\psi\). By the previous problem, \(\psi=\lambda\varphi\) for some \(\lambda\in\R\). Substituting \(x=a_1+a_2\) into this equation, it follows that \(a_2-a_1=\lambda(a_2-a_1)\), so \(\lambda=1\) and \(\psi=\varphi\).
\end{proof}

\begin{exer}[5]
A linear transformation~\(\varphi\) satisfies \(\adj{\varphi}=\lambda\varphi\) for \(\lambda\in\R\) if and only if \(\varphi\)~is self-adjoint or skew.
\end{exer}
\begin{proof}
If \(\varphi\ne 0\) and \(\adj{\varphi}=\lambda\varphi\) for some \(\lambda\in\R\), then there exist vectors \(x,y\) such that \(\iprod{\varphi x}{y}\ne 0\) and
\[\iprod{\varphi x}{y}=\lambda\iprod{x}{\varphi y}=\lambda^2\iprod{\varphi x}{y}\]
so \(\lambda=\pm 1\) and \(\varphi\)~is self-adjoint or skew, respectively. The rest is obvious.
\end{proof}

\begin{exer}[6]
If \(\Phi\)~is a skew symmetric bilinear function in an oriented Euclidean 3-space, then there is a unique vector~\(a\) such that
\[\Phi(x,y)=\iprod{a}{x\cross y}\]
\end{exer}
\begin{proof}
By problem~2, the skew transformation~\(\varphi\) dual to~\(\Phi\) can be written in the form \(\varphi(x)=a\cross x\) for some vector~\(a\). Then
\[\Phi(x,y)=\iprod{\varphi x}{y}=\iprod{a\cross x}{y}=\iprod{a}{x\cross y}\]
Uniqueness is obvious.
\end{proof}
\begin{rmk}
This result shows that any function measuring 2-dimensional area in planes in 3-space actually measures 3-dimensional volume relative to some fixed vector in the space.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Geometrically, a rotation (and more generally an isometry) preserves length and angle. A proper rotation additionally preserves orientation, whereas an improper rotation reverses it.
\end{rmk}

\begin{exer}[2]
A linear transformation~\(\varphi\) is regular and preserves orthogonality (that is, \(\iprod{\varphi x}{\varphi y}=0\) whenever \(\iprod{x}{y}=0\)) if and only if \(\varphi=\lambda\tau\) where \(\lambda\ne 0\) and \(\tau\)~is a rotation.
\end{exer}
\begin{proof}
For the forward direction, let \(e_1,\ldots,e_n\) be an orthonormal basis. Then \(\varphi e_1,\ldots,\varphi e_n\) is an orthogonal basis. Also
\[\norm{\varphi e_i}^2-\norm{\varphi e_j}^2=\iprod{\varphi e_i-\varphi e_j}{\varphi e_i+\varphi e_j}=\iprod{\varphi(e_i-e_j)}{\varphi(e_i+e_j)}=0\]
since
\[\iprod{e_i-e_j}{e_i+e_j}=1-1=0\]
for all~\(i,j\). Let \(\lambda=\norm{\varphi e_i}>0\). Then clearly \(\tau=\lambda^{-1}\varphi\) is a rotation.

The reverse direction is trivial.
\end{proof}
\begin{rmk}
This proof is motivated by the geometrical fact that a rectangle is a square if and only if its diagonals are orthogonal.
\end{rmk}

\begin{exer}[5]
If \(\varphi:E\to E\) is a mapping such that \(\varphi(0)=0\) and
\[\norm{\varphi x-\varphi y}=\norm{x-y}\]
for all \(x,y\in E\), then \(\varphi\)~is linear.
\end{exer}
\begin{proof}
Substituting \(y=0\), we have \(\norm{\varphi x}=\norm{x}\) for all~\(x\), so
\[\iprod{\varphi x-\varphi y}{\varphi x-\varphi y}=\norm{x}^2-2\iprod{\varphi x}{\varphi y}+\norm{y}^2\]
On the other hand,
\[\iprod{\varphi x-\varphi y}{\varphi x-\varphi y}=\norm{\varphi x-\varphi y}^2=\norm{x-y}^2=\norm{x}^2-2\iprod{x}{y}+\norm{y}^2\]
Therefore
\[\iprod{\varphi x}{\varphi y}=\iprod{x}{y}\]
It now follows that
\[\norm{\varphi(x+y)-\varphi x-\varphi y}^2=\iprod{\varphi(x+y)-\varphi x-\varphi y}{\varphi(x+y)-\varphi x-\varphi y}=0\]
so \(\varphi(x+y)=\varphi x+\varphi y\). Similarly \(\varphi(\lambda x)=\lambda\varphi x\).
\end{proof}

\subsection*{\S~6}
\begin{rmk}
In subsection~8.21, \(j\)~is called the canonical ``complex structure'' on~\(E\) because it induces a complex vector space structure on the underlying set of~\(E\) in which scalar multiplication is defined by
\[(\alpha+\beta i)\mult x=\alpha x+\beta jx\qquad\alpha,\beta\in\R\]
\end{rmk}

\begin{rmk}
In subsection~8.21, to derive~(8.40) from (8.39) and~(8.41), first observe that \(j\varphi=\varphi j\). Indeed, for \(z\ne 0\), \(\iprod{jz}{z}=0\), so \(\iprod{\varphi jz}{\varphi z}=0\). On the other hand, \(\iprod{j\varphi z}{\varphi z}=0\). Since \(E\)~is a plane, it follows that \(j\varphi z=\lambda\varphi jz\) for some \(\lambda\in\R\). But
\begin{align*}
\lambda\norm{z}^2&=\lambda\norm{\varphi jz}^2\\
	&=\lambda\iprod{\varphi jz}{\varphi jz}\\
	&=\iprod{j\varphi z}{\varphi jz}\\
	&=\Delta(\varphi z,\varphi jz)\\
	&=\Delta(z,jz)\\
	&=\iprod{jz}{jz}\\
	&=\iprod{z}{z}=\norm{z}^2
\end{align*}
so \(\lambda=1\) and \(j\varphi z=\varphi j z\).

From this and~(8.39), it follows that
\[\varphi^{-1}x=x\mult\cos(-\Theta)+jx\mult\sin(-\Theta)=x\mult\cos\Theta-jx\mult\sin\Theta\]
where \(x\) and~\(\Theta\) are as in~(8.39). In~(8.41),
\[\Delta(x,\varphi y)=\iprod{jx}{\varphi y}=\iprod{\varphi^{-1}jx}{y}=\iprod{j\varphi^{-1}x}{y}=\Delta(\varphi^{-1}x,y)\]
so
\[\Delta(\varphi x,y)+\Delta(x,\varphi y)=\Delta(\varphi x+\varphi^{-1}x,y)=2\cos\Theta\mult\Delta(x,y)\]
and it follows that \(\cos\Theta=\tfrac{1}{2}\tr\varphi\). Similar reasoning shows that \(\sin\Theta=-\tfrac{1}{2}\tr(j\varphi)\), contrary to what the book says.
\end{rmk}

\begin{rmk}
In subsection~8.21, to make sense of the ``definition'' in~(8.43), fix \(x\ne 0\) and let \(0\le\Theta\le\pi\) be the angle between \(x\) and~\(\varphi x\). Fix an orientation in~\(E\) so that \(\Theta\)~is the \emph{oriented} angle between \(x\) and~\(\varphi x\). Then (8.40)~holds, so \(\cos\Theta=\tfrac{1}{2}\tr\varphi\). Clearly \(\Theta\)~is independent of~\(x\) and depends only on~\(\varphi\), so can be written as~\(\Theta(\varphi)\) and satisfies~(8.43).
\end{rmk}

\begin{rmk}
In subsection~8.22, why would we expect the rotation vector~\(u\) in~(8.45) to lie on the rotation axis? Since \(\psi\)~is skew, \(\Psi(x,y)=\iprod{\psi x}{y}\) measures oriented area in planes in~\(E\), and since \(\psi\)~kills off~\(E_1\), \(\Psi(x,y)=0\) if \(x\in E_1\) or \(y\in E_1\). But we know from previous results that \(\Psi\)~actually measures oriented \emph{volume} relative to~\(u\), so it follows that \(u\)~must lie in~\(E_1\).

Note that \(\psi\)~stabilizes \(F=\oc{E_1}\), and if \(\psi_1\)~denotes the restriction, then
\[\psi_1=\tfrac{1}{2}(\varphi_1-\adj{\varphi_1})=\tfrac{1}{2}(\varphi_1-\varphi_1^{-1})\]
where \(\varphi_1\)~denotes the restriction of~\(\varphi\). If \(F\)~is oriented by~\(E\) and by \(u\ne 0\), and \(j\)~is the induced canonical complex structure on~\(F\), then
\[\psi_1=j\mult\sin\Theta\]
where \(0<\Theta<\pi\) is the oriented angle of rotation. If \(x\in F\) is any unit vector, then
\[\norm{u}=\norm{u\cross x}=\norm{\psi_1 x}=\norm{jx}\sin\Theta=\sin\Theta\]
as expected.
\end{rmk}

\begin{rmk}
In subsection~8.24, for the proof of the first part of Proposition~I, let \(q_i=p_i-\lambda_i e\) where \(\lambda_i=\iprod{p_i}{e}\) for \(i=1,2\). Since \(q_1\) and~\(q_2\) each generate the same axis of rotation, either \(q_1=0=q_2\) in which case \(p_1=\pm e\) and \(p_2=\pm e\) and the result holds, or else \(q_1\ne 0\) and \(q_2=\alpha q_1\) for some \(\alpha\ne 0\). If \(\alpha>0\), then \(q_1\) and~\(q_2\) induce the same orientation in their orthogonal plane (in~\(E_1\)), so the oriented angle~\(\Theta\) of rotation in that plane is the same. It follows that
\[\lambda_1=\cos\frac{\Theta}{2}=\lambda_2\qquad\text{and}\qquad\norm{q_1}=\sin\frac{\Theta}{2}=\norm{q_2}\]
The second equation implies that \(\alpha=1\), so \(q_1=q_2\) and \(p_1=p_2\). On the other hand if \(\alpha<0\), then \(-p_2\)~is a unit quaternion inducing the same rotation as~\(p_1\) and its pure part~\(-q_2\) satisfies \(-q_2=(-\alpha)q_1\) with \(-\alpha>0\), so by the previous case \(p_1=-p_2\).
\end{rmk}

\begin{exer}[16]
If \(p\ne\pm e\) is a unit quaternion, then the rotation vector induced by~\(p\) is
\[u=2\lambda(p-\lambda e)\qquad\lambda=\iprod{p}{e}\]
\end{exer}
\begin{proof}
Let \(q=p-\lambda e\). By assumption, \(q\ne 0\) and \(u=\alpha q\) since \(q\) and~\(u\) both lie on the axis of rotation. If \(\alpha>0\), then \(q\) and~\(u\) both induce the same orientation on the orthogonal plane and
\[\alpha\norm{q}=\norm{u}=\sin\Theta=2\lambda\norm{q}\]
where \(\Theta\)~is the oriented angle of rotation. Therefore \(\alpha=2\lambda\) and the result holds. If \(\alpha<0\), then \(-p\)~induces the same rotation vector and satisfies the hypotheses of the previous case since \(u=(-\alpha)(-q)\) with \(-\alpha>0\), so \(-\alpha=2(-\lambda)\) and \(\alpha=2\lambda\) and the result holds. If \(\alpha=0\), then \(u=0\) and \(\Theta=\pi\), so \(\lambda=\cos(\pi/2)=0\) and the result holds.
\end{proof}

\begin{exer}[17]
Let \(p\ne\pm e\) be a unit quaternion. If \(F\)~denotes the plane generated by \(e\) and~\(p\), then the rotations \(\varphi x=px\) and \(\psi x=xp\) agree on~\(F\) and stabilize \(F\) and~\(\oc{F}\).
\end{exer}
\begin{proof}
The rotations agree on~\(e\) and~\(p\), hence on~\(F\). By definition of quaternion multiplication, \(p^2\in F\), so they also stabilize \(F\) and~\(\oc{F}\).
\end{proof}

\section*{Chapter~XI}
\begin{rmk}
In parts of this chapter, it is implicitly assumed that unitary spaces have dimension \(n\ge 1\).
\end{rmk}

\begin{rmk}
A map \(\varphi:E\to F\) of complex vector spaces \(E,F\) is \emph{conjugate-linear} if
\[\varphi(\lambda x+\mu y)=\conj{\lambda}\varphi x+\conj{\mu}\varphi y\]
for all \(x,y\in E\) and \(\lambda,\mu\in\C\), where \(\conj{\lambda},\conj{\mu}\) denote the complex conjugates of \(\lambda,\mu\) respectively.
\end{rmk}

\subsection*{\S~1}
\begin{rmk}
A sesquilinear function \(\Phi(x,y)\) is linear in~\(x\) and conjugate-linear in~\(y\).
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
The identity map \(\kappa:F\to\conj{F}\) is conjugate-linear. A map \(\varphi:E\to F\) is conjugate-linear if and only if \(\kappa\varphi:E\to\conj{F}\) is linear. This yields a natural bijective correspondence between conjugate-linear maps \(E\to F\) and linear maps \(E\to\conj{F}\).
\end{rmk}

\begin{rmk}
If \(z\mapsto\conj{z}\) is a conjugation in~\(E\), then for \(\lambda=\alpha+i\beta\) with \(\alpha,\beta\in\R\),
\[\conj{\lambda z}=\conj{\alpha z+i\beta z}=\alpha\conj{z}-i\beta\,\conj{z}=\conj{\lambda}\,\conj{z}\]
Therefore a conjugation is just a conjugate-linear involution. By the previous remark, a conjugation can also be viewed as a linear map \(E\to\conj{E}\).
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
If \(E\)~is a unitary space, then there exists a conjugation in~\(E\). In fact, if \(z_1,\ldots,z_n\) is a basis of~\(E\) and \(F\)~is the real span of \(z_1,\ldots,z_n\), then \(F\)~is a real form of~\(E\) and the map \(z\mapsto\conj{z}\) defined by \(x+iy\mapsto x-iy\) for \(x,y\in F\) is a conjugation in the \emph{vector space}~\(E\). It is also a conjugation in the \emph{unitary space}~\(E\) since for \(z_1=x_1+iy_1\) and \(z_2=x_2+iy_2\) with \(x_j,y_j\in F\),
\[\iprod{z_1}{z_2}=\iprod{x_1}{x_2}+\iprod{y_1}{y_2}+i\left[\iprod{y_1}{x_2}-\iprod{x_1}{y_2}\right]\]
and
\[\iprod{\,\conj{z_1}}{\conj{z_2}\,}=\iprod{x_1}{x_2}+\iprod{y_1}{y_2}-i\left[\iprod{y_1}{x_2}-\iprod{x_1}{y_2}\right]\]
so \(\iprod{\,\conj{z_1}}{\conj{z_2}\,}=\conj{\iprod{z_1}{z_2}}\) as required.\footnote{See problem~2(i) in~\S~2.}
\end{rmk}

\begin{rmk}
If \(E,F\) are unitary spaces and \(\conj{E},\conj{F}\) the corresponding conjugate spaces, then \(E\)~is dual to~\(\conj{E}\) under the scalar product
\[\sprod{x}{x^*}=\iprod{x}{\kappa_E^{-1}x^*}\]
where \(\kappa_E:E\to\conj{E}\) is the identity map; similarly \(F\)~is dual to~\(\conj{F}\) under the scalar product
\[\sprod{y}{y^*}=\iprod{y}{\kappa_F^{-1}y^*}\]
where \(\kappa_F:F\to\conj{F}\) is the identity map.

If \(\varphi:E\to F\) is a linear map, then the dual map \(\varphi^*:\conj{E}\from\conj{F}\) satisfies
\[\iprod{\varphi x}{\kappa_F^{-1}y^*}=\sprod{\varphi x}{y^*}=\sprod{x}{\varphi^* y^*}=\iprod{x}{\kappa_E^{-1}\varphi^* y^*}\]
Taking \(y^*=\kappa_F y\) yields
\[\iprod{\varphi x}{y}=\iprod{x}{\kappa_E^{-1}\varphi^*\kappa_F y}\]
Therefore \(\adj{\varphi}=\kappa_E^{-1}\varphi^*\kappa_F\):
\begin{diagram}
E				&\rTo^{\varphi}		&F\\
\dTo<{\kappa_E}	&					&\dTo>{\kappa_F}\\
\conj{E}		&\lTo_{\varphi^*}	&\conj{F}
\end{diagram}
This construction of the adjoint avoids using conjugations in \(E,F\) by using the conjugate spaces \(\conj{E},\conj{F}\) instead.

If \(E=F\), \(\Delta\ne 0\) is a determinant function in~\(E\), and \(\Delta^*\)~is the corresponding determinant function in~\(\conj{E}\) (11.6), then
\begin{align*}
\det\adj{\varphi}\mult\Delta(x_1,\ldots,x_n)&=\Delta(\adj{\varphi}x_1,\ldots,\adj{\varphi}x_n)\\
	&=\Delta(\kappa^{-1}\varphi^*\kappa x_1,\ldots,\kappa^{-1}\varphi^*\kappa x_n)\\
	&=\conj{\Delta^*(\varphi^*\kappa x_1,\ldots,\varphi^*\kappa x_n)}\\
	&=\conj{\det\varphi^*}\mult\conj{\Delta^*(\kappa x_1,\ldots,\kappa x_n)}\\
	&=\conj{\det\varphi}\mult\Delta(x_1,\ldots,x_n)
\end{align*}
where \(\kappa=\kappa_E\), so \(\det\adj{\varphi}=\conj{\det\varphi}\). This derivation of~(11.21) similarly avoids using conjugations in~\(E\).
\end{rmk}

\begin{rmk}
The mapping \(\varphi\mapsto\adj{\varphi}\) is conjugate-linear.
\end{rmk}

\begin{rmk}
If \(z\mapsto\conj{z}\) is a conjugation in~\(E\) and \(\Delta\)~is a determinant function in~\(E\), then
\[\conj{\Delta}(z_1,\ldots,z_n)=\conj{\Delta(\conj{z_1},\ldots,\conj{z_n})}\]
is also a determinant function in~\(E\), called the \emph{conjugate determinant function}. The proof of~(11.21) in the book uses one of these.
\end{rmk}

\begin{exer}[1]
If \(E\)~is a unitary space and \(\varphi:E\to E\) is linear, then
\[\Phi(x,y)=\iprod{\varphi x}{y}\]
is sesquilinear. Conversely, every sesquilinear function in~\(E\) can be uniquely represented in this way. The adjoint~\(\adj{\varphi}\) represents the adjoint~\(\adj{\Phi}\).
\end{exer}
\begin{proof}
The forward direction is trivial. If \(\Phi\)~is sesquilinear, then for any fixed vector~\(x\) the function
\[y\mapsto\conj{\Phi(x,y)}\]
is linear, so by the Riesz theorem (11.5) there is a unique vector~\(\varphi x\) such that
\[\conj{\Phi(x,y)}=\iprod{y}{\varphi x}=\conj{\iprod{\varphi x}{y}}\]
and therefore
\[\Phi(x,y)=\iprod{\varphi x}{y}\]
Clearly \(\varphi\)~is linear and is uniquely determined by~\(\Phi\). Also
\[\iprod{\adj{\varphi}x}{y}=\conj{\iprod{y}{\adj{\varphi}x}}=\conj{\iprod{\varphi y}{x}}=\conj{\Phi(y,x)}=\adj{\Phi}(x,y)\qedhere\]
\end{proof}
\begin{rmk}
This result shows that the linear transformation \(\varphi\) is naturally left-dual to the sesquilinear function~\(\Phi\). Observe that \(\Phi\)~is Hermitian if and only if \(\varphi\)~is Hermitian (self-adjoint) and \(\Phi\)~is skew-Hermitian if and only if \(\varphi\)~is skew-Hermitian.
\end{rmk}

\section*{Chapter~XII}
\subsection*{\S~1}
\begin{exer}[2]
Define the linear mapping \(\int:\Gamma[t]\to\Gamma[t]\) by
\[\int t^p=\frac{t^{p+1}}{p+1}\qquad(p\ge 0)\]
Then \(\int\)~is homogeneous (of degree~1) with \(d\after\int=\iota\), and is unique with these properties.
\end{exer}
\begin{proof}
If \(\int\)~satisfies the properties, then we must have
\[\int t^p=\frac{t^{p+1}}{p+1}+\alpha_p\qquad(\alpha_p\in\Gamma)\]
since \(d\after\int=\iota\). But \(\alpha_p=0\) for all~\(p\) since \(\int\)~is homogeneous. Finally, \(\int\)~is uniquely determined on~\(\Gamma[t]\) since \(\int\)~is linear and the~\(t^p\) (\(p\ge 0\)) form a basis of~\(\Gamma[t]\).
\end{proof}

\begin{exer}[3]
If \(\int:\Gamma[t]\to\Gamma[t]\) is the integration operator, then \(\int\after d=\iota-\varrho\), where \(\varrho:\Gamma[t]\to\Gamma\) is the scalar projection \(\varrho f=f(0)\). It follows that
\[\int fg'=fg-\varrho(fg)-\int gf'\]
\end{exer}
\begin{proof}
The first part is easily verified on the basis~\(t^p\) (\(p\ge 0\)). The second part then follows since
\[fg-\varrho(fg)=\int(fg)'=\int(f'g+fg')=\int f'g+\int fg'\qedhere\]
\end{proof}

\subsection*{\S~3}
\begin{rmk}
If \(A=0\), then the minimum polynomial of \(a=0\) is \(\mu=1\).
\end{rmk}

\section*{Chapter~XIII}
\subsection*{\S~1}
In the following remarks, \(f\in\Gamma[t]\).
\begin{rmk}
If \(\varphi:E\to E\) is linear and \(E_1\)~is stable under~\(\varphi\), then \(E_1\)~is also stable under~\(f(\varphi)\) and
\[f(\varphi)_{E_1}=f(\varphi_{E_1})\]
Dually,
\[\proj{f(\varphi)}=f(\proj{\varphi})\]
\end{rmk}
\begin{proof}
Stabilization of~\(E_1\) is preserved by addition, scalar multiplication, and composition, so is preserved by~\(f\). It follows that \(f(\varphi)_{E_1}\) and~\(f(\varphi_{E_1})\) are both transformations of~\(E_1\) with the same action, so are equal. Finally,
\[\proj{f(\varphi)}(\proj{x})=\proj{f(\varphi)(x)}=f(\proj{\varphi})(\proj{x})\qedhere\]
\end{proof}

\begin{rmk}
It follows that if \(K_1(f)=\ker f(\varphi_{E_1})\), then
\[K_1(f)=K(f)\sect E_1\]
Dually, if \(\proj{K}(f)=\ker f(\proj{\varphi})\), then
\[\proj{K}(f)\supseteq\proj{K(f)}\]
The reverse inclusion does not hold in general. For a counterexample, consider \(f=t\) with \(\varphi:\R^2\to\R^2\) defined by \(\varphi(x,y)=(y,0)\) and \(E_1=\R\times 0\).
\end{rmk}

\begin{rmk}
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[f(\varphi_1\dsum\varphi_2)=f(\varphi_1)\dsum f(\varphi_2)\]
\end{rmk}
\begin{proof}
Write \(\varphi=\varphi_1\dsum\varphi_2\), so \(\varphi_{E_1}=\varphi_1\) and \(\varphi_{E_2}=\varphi_2\). Then
\[f(\varphi)=f(\varphi)_{E_1}\dsum f(\varphi)_{E_2}=f(\varphi_1)\dsum f(\varphi_2)\qedhere\]
\end{proof}

\begin{rmk}
If \(E,E^*\) are dual spaces and \(\varphi:E\to E\) and \(\varphi^*:E^*\from E^*\) are dual transformations, then
\[f(\varphi)^*=f(\varphi^*)\]
\end{rmk}
\begin{proof}
By the results of subsection~2.25.
\end{proof}

\begin{rmk}
In particular, if \(E\)~is an inner product space or a unitary space, then
\[\adj{f(\varphi)}=\conj{f}(\adj{\varphi})\]
where \(\conj{f}\)~is obtained from~\(f\) by conjugating the coefficients.\footnote{In the case of an inner product space, \(\conj{f}=f\).}
\end{rmk}
\begin{proof}
Let \(\conj{E}\)~be the conjugate space and \(\kappa:E\to\conj{E}\) the conjugate-linear identity map. Then
\[\adj{f(\varphi)}=\kappa^{-1}f(\varphi)^*\kappa=\kappa^{-1}f(\varphi^*)\kappa=\conj{f}(\kappa^{-1}\varphi^*\kappa)=\conj{f}(\adj{\varphi})\qedhere\]
\end{proof}

\begin{rmk}
It follows that \(f(\varphi)=0\) if and only if \(\conj{f}(\adj{\varphi})=0\), so if \(\mu\)~is the minimum polynomial of~\(\varphi\) then \(\conj{\mu}\)~is the minimum polynomial of~\(\adj{\varphi}\).
\end{rmk}

\begin{exer}[3]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\mu_{\varphi_1\dsum\varphi_2}=\mu_{\varphi_1}\meet\mu_{\varphi_2}\]
\end{exer}
\begin{proof}
Write \(\varphi=\varphi_1\dsum\varphi_2\). By a remark above, \(f(\varphi)=f(\varphi_1)\dsum f(\varphi_2)\), so \(f(\varphi)=0\) if and only if \(f(\varphi_1)=0\) and \(f(\varphi_2)=0\). It follows that \(\mu_{\varphi}\)~is the least common multiple of \(\mu_{\varphi_1}\) and~\(\mu_{\varphi_2}\).
\end{proof}

\begin{exer}[5]
If \(\varphi:E\to E\) stabilizes~\(E_1\), and \(\varphi_1:E_1\to E_1\) and \(\proj{\varphi}:E/E_1\to E/E_1\) are the induced transformations, then
\[\proj{\mu}\meet\mu_1\divides\mu\divides\proj{\mu}\mu_1\]
where \(\mu=\mu_{\varphi}\), \(\mu_1=\mu_{\varphi_1}\), and \(\proj{\mu}=\mu_{\proj{\varphi}}\).
\end{exer}
\begin{proof}
By a remark above, \(\mu(\varphi_1)=0\) and \(\mu(\proj{\varphi})=0\), so \(\mu_1\divides\mu\) and \(\proj{\mu}\divides\mu\). On the other hand,
\[\proj{\mu}\mu_1(\varphi)(x)=\mu_1(\varphi)(\proj{\mu}(\varphi)(x))\]
But \(\proj{\mu}(\varphi)(x)\)~is in~\(E_1\) since
\[\proj{\proj{\mu}(\varphi)(x)}=\proj{\proj{\mu}(\varphi)}(\proj{x})=\proj{\mu}(\proj{\varphi})(\proj{x})=0\]
so
\[\mu_1(\varphi)(\proj{\mu}(\varphi)(x))=\mu_1(\varphi_1)(\proj{\mu}(\varphi)(x))=0\]
It follows that \(\proj{\mu}\mu_1(\varphi)=0\), so \(\mu\divides\proj{\mu}\mu_1\).
\end{proof}

\begin{exer}[9]
The minimum polynomial of a self-adjoint transformation of a unitary space has real coefficients.
\end{exer}
\begin{proof}
By a remark above, \(\conj{\mu}=\mu\).
\end{proof}

\subsection*{\S~2}
\begin{rmk}
In Proposition~I, if \(\pi_i\) and~\(\pi^*_i\) are the projection operators associated with the generalized eigenspace decompositions in \(E\) and~\(E^*\), then
\[(\pi_i)^*=\pi^*_i\]
\end{rmk}
\begin{proof}
From~(13.24), it follows that
\[\sprod{\pi^*_i x^*}{x}=\sprod{\pi^*_i x^*}{\pi_i x}=\sprod{x^*}{\pi_i x}\qedhere\]
\end{proof}
\begin{rmk}
If \(g_i\in\Gamma[t]\) with \(g_i(\varphi)=\pi_i\) (subsection~13.5), then
\[g_i(\varphi^*)=g_i(\varphi)^*=(\pi_i)^*=\pi^*_i\]
Alternatively, the fact that \(g_i(\varphi^*)=\pi^*_i\) follows from the fact that \(g_i\)~depends only on the common minimum polynomial of~\(\varphi^*\) and~\(\varphi\), and the argument above can be run in reverse to obtain~(13.24).
\end{rmk}

\begin{exer}[1]
The minimum polynomial~\(\mu\) of \(\varphi:E\to E\) is completely reducible if and only if every nonzero subspace of~\(E\) stable under~\(\varphi\) contains an eigenvector of~\(\varphi\).
\end{exer}
\begin{proof}
If \(F\ne 0\) is stable, then by~(13.22)
\[F=\sum_i F\sect E_i\]
where the~\(E_i\) are the generalized eigenspaces of~\(\varphi\). Fix~\(i\) with \(F\sect E_i\ne 0\). If \(\mu\)~is completely reducible, then \(E_i=K((t-\lambda)^k)\) for some \(\lambda\in\Gamma\) and \(k\ge 1\). Choose \(x\in F\sect E_i\) with \(x\ne 0\) and let \(1\le l\le k\) be least with \((\varphi-\lambda\iota)^l x=0\). Then it follows that \((\varphi-\lambda\iota)^{l-1}x\in F\) is an eigenvector.

Conversely, if \(\mu\)~is not completely reducible, there is~\(f\) monic irreducible with \(\deg f\ge 2\) and \(f\divides\mu\). Let \(F=K(f)\). Then \(F\ne 0\) is stable but \(F\)~contains no eigenvector, lest \(f=\mu_f\) would have a linear factor.
\end{proof}
\begin{rmk}
This is true in particular if \(\Gamma\)~is algebraically closed (for example \(\Gamma=\C\)).
\end{rmk}

\begin{exer}[2]
If the minimum polynomial~\(\mu\) of \(\varphi:E\to E\) is completely reducible, then there is a basis of~\(E\) with respect to which the matrix of~\(\varphi\) is triangular:
\[\begin{pmatrix}
\lambda_1&&0\\
&\ddots&\\
*&&\lambda_n
\end{pmatrix}\]
\end{exer}
\begin{proof}
By induction on~\(\dim E\).

Since \(E\ne 0\) is stable, there is an eigenvector~\(x_1\) with \(\varphi x_1=\lambda_1 x_1\). Let \(E_1=\gen{x_1}\) and consider \(\proj{\varphi}:E/E_1\to E/E_1\) with minimum polynomial~\(\proj{\mu}\). Since \(\proj{\mu}\divides\mu\), \(\proj{\mu}\)~is completely reducible, and by induction there is a basis \(\proj{x_2},\ldots,\proj{x_n}\) of~\(E/E_1\) with \(\proj{\varphi}(\proj{x_k})\in\gen{\proj{x_2},\ldots,\proj{x_k}}\) for \(2\le k\le n\). It follows that \(x_1,\ldots,x_n\) is a basis of~\(E\) with \(\varphi x_k\in\gen{x_1,\ldots,x_k}\) for \(1\le k\le n\).
\end{proof}
\begin{rmk}
The diagonal entries of this matrix are just the eigenvalues of~\(\varphi\).
\end{rmk}

\begin{exer}[9]
Let \(F\)~be a subspace of~\(E\) stable under \(\varphi:E\to E\) and consider the induced mappings \(\varphi_F:F\to F\) and \(\proj{\varphi}:E/F\to E/F\). Let \(E=\sum_i E_i\) be the generalized eigenspace decomposition of~\(E\) under~\(\varphi\).
\begin{enumerate}
\item[(a)] The generalized eigenspace decomposition of~\(F\) under~\(\varphi_F\) is
\[F=\sum_i F\sect E_i\]
\end{enumerate}
\end{exer}
\begin{proof}
For~(a), we already know from~(13.22) that
\[F=\sum_i F\sect E_i\tag{1}\]
Let
\[F=\sum_i F_i\tag{2}\]
be the generalized eigenspace decomposition of~\(F\) under~\(\varphi_F\). Since \(\mu_{\varphi_F}\divides\mu_{\varphi}\), we may assume that \(F_i=K_F(f_i^{j_i})\) and \(E_i=K(f_i^{k_i})\) for some~\(f_i\) with \(j_i\le k_i\). Then
\[F_i=K_F(f_i^{j_i})=F\sect K(f_i^{j_i})\subseteq F\sect K(f_i^{k_i})=F\sect E_i\tag{3}\]
It follows from (1), (2), and~(3) that \(F_i=F\sect E_i\), establishing~(a).
\end{proof}

\subsection*{\S~3}
\begin{rmk}
In the proof of the corollary to Proposition~IV, to see that \(\mu(\varphi)=0\), first observe that
\[\mu(\varphi)(a_0)=\varphi^n(a_0)-\sum_{\nu=0}^{n-1}\alpha_{\nu}\varphi^{\nu}(a_0)=\varphi(a_{n-1})-\sum_{\nu=0}^{n-1}\alpha_{\nu}a_{\nu}=0\]
Now if \(\mu(\varphi)(a_{\nu})=0\) for some \(0\le\nu\le n-2\), then
\[\mu(\varphi)(a_{\nu+1})=\mu(\varphi)(\varphi(a_{\nu}))=\varphi(\mu(\varphi)(a_{\nu}))=\varphi(0)=0\]
By induction, \(\mu(\varphi)(a_{\nu})=0\) for all \(0\le\nu\le n-1\), so \(\mu(\varphi)=0\).
\end{rmk}

% Multilinear algebra
\newpage
\part*{Multilinear Algebra}

\section*{Chapter~1}
\subsection*{\S~1}
\begin{rmk}
If \(\lambda^1\lambda^4-\lambda^2\lambda^3=0\), we want \(\xi^1,\xi^2,\eta^1,\eta^2\) with
\[\lambda^1=\xi^1\eta^1\qquad\lambda^2=\xi^1\eta^2\qquad\lambda^3=\xi^2\eta^1\qquad\lambda^4=\xi^2\eta^2\]
If \(\lambda^1=0\), then \(\lambda^2\lambda^3=\lambda^1\lambda^4=0\), so \(\lambda^2=0\) or \(\lambda^3=0\).
\begin{itemize}[itemsep=0pt]
\item If \(\lambda^2=0\), we take \(\xi^1=0\), \(\xi^2=1\), \(\eta^1=\lambda^3\), and \(\eta^2=\lambda^4\).
\item If \(\lambda^3=0\), we take \(\xi^1=\lambda^2\), \(\xi^2=\lambda^4\), \(\eta^1=0\), and \(\eta^2=1\).
\end{itemize}
If \(\lambda^1\ne 0\) and \(\lambda^2=0\), then \(\lambda^4=0\) and we take \(\xi^1=\lambda^1\), \(\xi^2=\lambda^3\), \(\eta^1=1\), and \(\eta^2=0\). If \(\lambda^1\ne 0\) and \(\lambda^2\ne 0\), we take \(\xi^1=1\), \(\xi^2=\lambda^3/\lambda^1=\lambda^4/\lambda^2\), \(\eta^1=\lambda^1\), and \(\eta^2=\lambda^2\).
\end{rmk}

\subsection*{\S~2}
\begin{rmk}
In the construction of the first induced bilinear map~\(\widetilde{\varphi}\), note that for each \(y\in F\) the linear map \(\varphi(-,y):E\to G\) sends \(E_1\) into~\(G_1\) and hence induces a linear map \(\proj{\varphi}(-,y):E/E_1\to G/G_1\) by \(\proj{\varphi}(\rho x,y)=\pi\varphi(x,y)\):
\begin{diagram}
E			&\rTo^{\varphi(-,y)}		&G\\
\dTo<{\rho}	&							&\dTo>{\pi}\\
E/E_1		&\rTo_{\proj{\varphi}(-,y)}	&G/G_1
\end{diagram}
Since \(\proj{\varphi}(-,y)\) depends linearly on~\(y\), \(\proj{\varphi}\)~is bilinear and we define \(\widetilde{\varphi}=\proj{\varphi}\).

In the construction of the second induced bilinear map~\(\widetilde{\varphi}\), note that the linear map \(y\mapsto\proj{\varphi}(-,y)\) kills~\(F_1\), so it factors through~\(\sigma\):
\begin{diagram}[nohug]
F				&\rTo^{y\mapsto\proj{\varphi}(-,y)}			&L(E/E_1;G/G_1)\\
\dTo<{\sigma}	&\ruTo>{\sigma y\mapsto\proj{\varphi}(-,y)}	&\\
F/F_1			&											&
\end{diagram}
This allows us to define the bilinear map \(\widetilde{\varphi}(\rho x,\sigma y)=\proj{\varphi}(\rho x,y)=\pi\varphi(x,y)\).
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
The claim in problem~5(b) is false because it implies, for example, that any two inner products in~\(\R^2\) agree on orthogonality, which is false. The claim holds if and only if \(\psi\)~preserves linear relations satisfied by~\(\varphi\).
\end{rmk}

\begin{exer}[7]
If \(E,E^*\) are finite-dimensional dual spaces and \(\Phi:E^*\times E\to\Gamma\) is a bilinear function such that
\[\Phi(\tau^{*-1}x^*,\tau x)=\Phi(x^*,x)\]
for every pair of dual automorphisms, then there is \(\lambda\in\Gamma\) such that
\[\Phi(x^*,x)=\lambda\sprod{x^*}{x}\]
\end{exer}
\begin{proof}
Let \(\varphi:E\to E\) be the linear transformation with \(\Phi(x^*,x)=\sprod{x^*}{\varphi x}\). If \(\tau\)~is an automorphism of~\(E\), then since \((\tau^{-1})^*=(\tau^*)^{-1}\),
\[\sprod{x^*}{\tau^{-1}\varphi\tau(x)}=\sprod{\tau^{*-1}x^*}{\varphi\tau(x)}=\Phi(\tau^{*-1}x^*,\tau x)=\Phi(x^*,x)=\sprod{x^*}{\varphi x}\]
It follows that \(\tau^{-1}\varphi\tau=\varphi\). Since \(\tau\)~was arbitrary, \(\varphi=\lambda\iota\) for some \(\lambda\in\Gamma\), from which the result follows.
\end{proof}

\subsection*{\S~4}
\begin{rmk}
The tensor product \(E\tprod F\) is a universal (initial) object in the category of ``vector spaces with bilinear maps of \(E\times F\) into them''. In this category, the objects are bilinear maps \(E\times F\to G\), and the arrows are linear maps \(G\to H\) which respect the bilinear maps:
\begin{diagram}
E\times F	&\rTo	&H\\
\dTo		&\ruTo	&\\
G			&		&
\end{diagram}
Every object \(E\times F\to G\) in this category can be obtained from the tensor product \(\tprod:E\times F\to E\tprod F\) in a unique way. This is why \(\tprod\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(E\tprod F\) satisfy only those relations required to make \(E\tprod F\) into a vector space and to make \(\tprod\)~bilinear. By category theoretic abstract nonsense, \(E\tprod F\)~is unique up to isomorphism.
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Lemma~1.5.1 generalizes the consequence of linear independence to tensor products other than scalar multiplication.

We provide an alternative proof. For a linear function \(g\in L(F)\), consider the bilinear map \(E\times F\to E\) defined by \((x,y)\mapsto g(y)x\). By the universal property of the tensor product, there is a linear map \(h:E\tprod F\to E\) with \(h(x\tprod y)=g(y)x\). Now
\[0=h(\sum a_i\tprod b_i)=\sum h(a_i\tprod b_i)=\sum g(b_i)a_i\]
By linear independence of the~\(a_i\), it follows that \(g(b_i)=0\) for all~\(i\). Since \(g\)~was arbitrary, it follows that \(b_i=0\) for all~\(i\).

Note \(h=\iota\tprod g:E\tprod F\to E\tprod\Gamma\) (see \S~16).
\end{rmk}

\begin{rmk}
Lemma~1.5.2 generalizes the existence and uniqueness of a representation relative to a basis.
\end{rmk}

\subsection*{\S~8}
\begin{rmk}
In the proof of Proposition~1.8.1, note that if \(f\)~is injective, it has a left inverse \(g:G\to E\tprod F\) with \(g\after f=\iota\). If \(\psi:E\times F\to K\) is bilinear, there is a linear map \(h:E\tprod F\to K\) with
\[\psi=h\after\tprod=h\after\iota\after\tprod=h\after g\after f\after\tprod=h\after g\after\varphi\]
So \(\psi\)~factors through~\(\varphi\). Since \(\psi\)~was arbitrary, \(\varphi\)~satisfies \(\medtprod_2\).
\end{rmk}

\begin{exer}[3]
If \(S\) and~\(T\) are sets, then \(C(S\times T)\iso C(S)\tprod C(T)\).
\end{exer}
\begin{proof}
By the universal property of the free space, there is a unique linear map \(\varphi:C(S\times T)\to C(S)\tprod C(T)\) with \(\varphi(s,t)=s\tprod t\) for \(s\in S\) and \(t\in T\):
\begin{diagram}[nohug]
S\times T	&\rTo^{i\times i}	&C(S)\times C(T)\\
\dTo<i		&					&\dTo>{\tprod}\\
C(S\times T)&\rTo>{\varphi}		&C(S)\tprod C(T)
\end{diagram}
Observe that \(\varphi\)~is injective since
\[0=\varphi\bigl(\,\sum_i\lambda_i(s_i,t_i)\bigr)=\sum_i\lambda_i\,s_i\tprod t_i\]
implies \(\lambda_i=0\) by linear independence of the distinct \(s_i\) in~\(C(S)\) and \(t_i\) in~\(C(T)\) (1.5.1). Also \(\varphi\)~is surjective since
\[\bigl(\,\sum_i\lambda_i s_i\bigr)\tprod\bigl(\,\sum_j\mu_j t_j\bigr)=\sum_{i,j}\lambda_i\mu_j\,s_i\tprod t_j=\sum_{i,j}\lambda_i\mu_j\,\varphi(s_i,t_j)\]
and the elements on the left generate \(C(S)\tprod C(T)\).
\end{proof}

\begin{exer}[4]
If \(a\tprod b\ne 0\), then \(a\tprod b=a'\tprod b'\) if and only if \(a'=\lambda a\) and \(b'=\lambda^{-1}b\) for some \(\lambda\ne 0\).
\end{exer}
\begin{proof}
For the forward direction, note all the vectors are nonzero by bilinearity of~\(\tprod\). Now
\[a\tprod b+a'\tprod(-b')=0\]
so we must have \(a'=\lambda a\) for some \(\lambda\ne 0\) (1.5.1). It follows that
\[a\tprod(b-\lambda b')=0\]
so \(b-\lambda b'=0\) and \(b'=\lambda^{-1}b\).

The reverse direction follows from bilinearity of~\(\tprod\).
\end{proof}

\subsection*{\S~10}
\begin{rmk}
To obtain the tensor product of \(E/E_1\) and~\(F/F_1\), just take the tensor product of \(E\) and~\(F\) and then kill off all the products by elements of~\(E_1\) and by elements of~\(F_1\).
\end{rmk}

\subsection*{\S~11}
\begin{rmk}
The tensor product operation is bilinear on \emph{spaces}:
\[\bigl(\bigdsum_{\alpha}E_{\alpha}\bigr)\tprod\bigl(\bigdsum_{\beta}F_{\beta}\bigr)=\bigdsum_{\alpha,\beta}E_{\alpha}\tprod F_{\beta}\]
In particular, \(E\tprod 0=0=0\tprod E\).
\end{rmk}

\subsection*{\S~12}
\begin{rmk}
In the proof of~(1.2), the idea is that \(E\iso\bigdsum_{\alpha}E_{\alpha}\) and \(F\iso\bigdsum_{\beta}F_{\beta}\), so
\[E\tprod F\iso\bigl(\bigdsum_{\alpha}E_{\alpha}\bigr)\tprod\bigl(\bigdsum_{\beta}F_{\beta}\bigr)=\bigdsum_{\alpha,\beta}E_{\alpha}\tprod F_{\beta}\]
by the result for external direct sums in the previous subsection. Observe that \(h=f\tprod g\) (see \S~16).
\end{rmk}

\subsection*{\S~15}
\begin{rmk}
By~(1.7), \emph{the intersection of tensor products is the tensor product of the intersections}. Observe that (1.4) and~(1.5) are special cases of~(1.7).

In the proof of~(1.4), \(u_{\beta}=v_{\beta}\) follows immediately from Lemma~1.5.2. In the proof of~(1.5), if \(z\in(E_1\tprod F)\sect(E\tprod F_1)\), then in particular \(z=x+y\) with \(x\in E_1\tprod F_1\) and \(y\in E_1\tprod F'\). Now \(y=z-x\in E\tprod F_1\), so
\[y\in(E\tprod F_1)\sect(E\tprod F')=E\tprod(F_1\sect F')=E\tprod 0=0\]
by~(1.4). Hence \(y=0\) and \(z=x\in E_1\tprod F_1\). Note that this argument makes~(1.6) superfluous.
\end{rmk}

\begin{rmk}
If \(\sum_{i=1}^r x_i\tprod y_i=\sum_{j=1}^s x_j'\tprod y_j'\) and the~\(x_i\) are linearly independent, then the~\(y_i\) are in the span of the~\(y_j'\).
\end{rmk}
\begin{proof}
By induction on~\(s\). If the vectors in \(\{x_i\}\union\{x_j'\}\) are linearly independent, then \(y_i=0\) for all~\(i\) (1.5.1), so the result holds trivially. Otherwise, since the~\(x_i\) are linearly independent, we may assume (relabeling if necessary) that
\[x_s=\sum_{i=1}^r\lambda_i x_i+\sum_{j=1}^{s-1}\mu_j x_j'\qquad(\lambda_i,\mu_j\in\Gamma)\]
By bilinearity of the tensor product, it follows that
\[\sum_{i=1}^r x_i\tprod(y_i-\lambda_i y_s')=\sum_{j=1}^{s-1}x_j'\tprod(y_j'+\mu_j y_s')\]
By induction, the \(y_i-\lambda_iy_s'\) are in the span of the \(y_j'+\mu_jy_s'\), so the~\(y_i\) are in the span of the~\(y_j'\).
\end{proof}

\begin{rmk}
If \(\sum_{i=1}^r x_i\tprod y_i=\sum_{j=1}^s x_j'\tprod y_j'\) and the \(x_i\) and~\(y_i\) are respectively linearly independent, then \(r\le s\).
\end{rmk}
\begin{proof}
By the previous remark and the elementary fact that the size of a linearly independent set is at most the size of a spanning set in a subspace.
\end{proof}

\begin{exer}[1]
If \(\sum_{i=1}^r x_i\tprod y_i=\sum_{j=1}^s x_j'\tprod y_j'\) and the \(x_i\), \(y_i\), \(x_j'\), and \(y_j'\) are each respectively linearly independent, then \(r=s\).
\end{exer}
\begin{proof}
By the previous remark, \(r\le s\) and \(s\le r\).
\end{proof}
\begin{rmk}
It follows from this result and the proof of Lemma~1.5.3 that the tensor representations of minimal length are precisely the representations by linearly independent vectors. These representations are \emph{not} unique, as already seen in problem~1.8.4.
\end{rmk}

\begin{exer}[2]
A bilinear mapping \(\varphi:E\times F\to G\) satisfies~\(\medtprod_2\) if and only if the vectors~\(\varphi(x_{\alpha},y_{\beta})\) are linearly independent whenever the vectors \(x_{\alpha}\in E\) and \(y_{\beta}\in F\) are linearly independent.
\end{exer}
\begin{proof}
If \(f:E\tprod F\to G\) is the induced linear map with \(\varphi=f\after\tprod\), then \(\varphi\)~satisfies \(\medtprod_2\) if and only if \(f\)~is injective (1.8.1)---that is, if and only if \(f\)~preserves linear independence. But \(f\)~preserves linear independence if and only if \(\varphi\)~does, since \(\tprod\)~does (1.5.1).
\end{proof}

\begin{exer}[3]
If \(A\ne 0\) is a finite-dimensional algebra forming a tensor product under the algebra multiplication, then \(\dim A=1\).
\end{exer}
\begin{proof}
By~(1.3) \(\dim A=(\dim A)^2\), and \(\dim A\ne 0\), so \(\dim A=1\).
\end{proof}

\begin{exer}[5]
If \(E,E^*\) and \(F,F^*\) are pairs of dual spaces of finite dimension and \(\beta:E\times F\to B(E^*,F^*)\) is the bilinear map given by
\[\beta_{x,y}(x^*,y^*)=\sprod{x^*}{x}\sprod{y^*}{y}\]
then \((B(E^*,F^*),\beta)\)~is the tensor product of \(E\) and~\(F\).
\end{exer}
\begin{proof}
Let \(x_1,\ldots,x_n\) be a basis in~\(E\) with dual basis \(x^{*1},\ldots,x^{*n}\) in~\(E^*\) and let \(y_1,\ldots,y_m\) be a basis in~\(F\) with dual basis \(y^{*1},\ldots,y^{*m}\) in~\(F^*\). Let \(\varphi^{*kl}:E^*\times F^*\to\Gamma\) be the basis function in~\(B(E^*,F^*)\) defined by
\[\varphi^{*kl}(x^{*i},y^{*j})=\delta^k_i\delta^l_j\]
Then
\[\beta_{x_k,y_l}(x^{*i},y^{*j})=\sprod{x^{*i}}{x_k}\sprod{y^{*j}}{y_l}=\delta^k_i\delta^l_j=\varphi^{*kl}(x^{*i},y^{*j})\]
so \(\beta_{x_k,y_l}=\varphi^{*kl}\). It follows that \(\im\beta=B(E^*,F^*)\), so \(\beta\)~satisfies~\(\medtprod_1\).

If \(\varphi:E\times F\to G\) is bilinear, define \(f:B(E^*,F^*)\to G\) by \(f(\varphi^{*kl})=\varphi(x_k,y_l)\). Then \(\varphi(x_k,y_l)=f(\beta_{x_k,y_l})\), so \(\varphi=f\beta\). It follows that \(\beta\)~satisfies \(\medtprod_2\).
\end{proof}
\begin{rmk}
This result allows us to view an element \(\sum x_i\tprod y_i\) of the tensor product as a bilinear function: given linear scalar substitutions \(x_i\to\lambda_i\) and \(y_i\to\mu_i\) as inputs, it produces the scalar \(\sum\lambda_i\mu_i\) as output. This is analogous to viewing a vector as a linear function. Indeed, if we identify~\(x\) with~\(\sprod{-}{x}\) and \(y\) with~\(\sprod{-}{y}\), then we can identify \(x\tprod y\) with \(\sprod{-}{x}\sprod{-}{y}\).
\end{rmk}

\subsection*{\S~16}
\begin{rmk}
The tensor product operation is a bifunctor in the category of vector spaces. It maps the objects \(E\) and~\(F\) to the object \(E\tprod F\) and the arrows \(\varphi:E\to E'\) and \(\psi:F\to F'\) to the arrow \(\varphi\tprod\psi:E\tprod F\to E'\tprod F'\).
\end{rmk}

\begin{rmk}
Corollary~III to Proposition~1.16.1 is not established in subsection~1.27, where it is also assumed that \(E'\) and~\(F'\) are finite-dimensional. However, it is true. In fact, for fixed bases in \(E,E',F,F'\), the tensor products of the induced basis maps in \(L(E;E')\) and \(L(F;F')\) are the basis maps in \(L(E\tprod F,E'\tprod F')\) induced by the tensor products of the basis vectors.\footnote{See problem~1.19.4.}
\end{rmk}

\subsection*{\S~19}
\begin{rmk}
The proof of~(1.12) is captured in the following commutative diagram:
\begin{diagram}[nohug]
E\tprod F						&														&\rTo^{\varphi\tprod\psi}	&		&E'\tprod F'\\
								&\rdLine_{\pi}											&							&\ruTo	&\\
\dTo<{\pi_1\tprod\pi_2}			&														&\rdLine\ruLine				&		&\uTo>{\chi}\\
								&\ruLine_{\ \proj{\varphi}\tprod\proj{\psi}}			&							&\rdTo	&\\
\proj{E}\tprod\proj{F}			&														&\rTo_g						&		&\proj{E\tprod F}
\end{diagram}
\end{rmk}

\begin{rmk}
The ``only if'' part of the claim in problem~1(a) requires \(E\ne 0\) and \(F\ne 0\). For example, if \(E\ne 0\) and \(F=0\) and \(\varphi=0\) and \(\psi=0\), then \(E\tprod F=0\) and \(\varphi\tprod\psi=0\) is injective despite the fact that \(\varphi\)~is not.
\end{rmk}

\begin{exer}[2]
If \(\varphi:E\to E\) and \(\psi:F\to F\) are linear with \(\dim E=n\) and \(\dim F=m\), then
\[\tr(\varphi\tprod\psi)=\tr\varphi\mult\tr\psi\]
and
\[\det(\varphi\tprod\psi)=(\det\varphi)^m\mult(\det\psi)^n\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) and \(f_1,\ldots,f_m\) be a basis of~\(F\). Write
\begin{align*}
\varphi e_i&=\sum_{k=1}^n\alpha^k_ie_k\\
\psi f_j&=\sum_{l=1}^m\beta^l_jf_l
\end{align*}
Then \(e_1\tprod f_1,\ldots,e_n\tprod f_m\) is a basis of~\(E\tprod F\) and
\[(\varphi\tprod\psi)(e_i\tprod f_j)=\varphi e_i\tprod\psi f_j=\sum_{k,l}\alpha^k_i\beta^l_je_k\tprod f_l\]
It follows that
\[\tr(\varphi\tprod\psi)=\sum_{i,j}\alpha^i_i\beta^j_j=\bigl(\sum_i\alpha^i_i\bigr)\bigl(\sum_j\beta^j_j\bigr)=\tr\varphi\mult\tr\psi\]
For the determinant, observe that
\[\varphi\tprod\psi=(\varphi\tprod\iota_F)\after(\iota_E\tprod\psi)\]
so
\[\det(\varphi\tprod\psi)=\det(\varphi\tprod\iota_F)\mult\det(\iota_E\tprod\psi)\]
But \(M(\varphi\tprod\iota_F)=(\alpha^k_i\delta^l_j)\) is block diagonal with \(m\)~blocks each equal to~\(M(\varphi)\), so \(\det(\varphi\tprod\iota_F)=(\det\varphi)^m\). Similarly \(\det(\iota_E\tprod\psi)=(\det\psi)^n\).
\end{proof}

\begin{exer}[3]
If \(\alpha,\beta:E\to E\) are linear with \(\dim E=n\), and \(\Phi:L(E;E)\to L(E;E)\) is defined by
\[\Phi\sigma=\alpha\after\sigma\after\beta\]
then
\[\tr\Phi=\tr\alpha\mult\tr\beta\]
and
\[\det\Phi=\det(\alpha\after\beta)^n\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) and let \(\psi_{ij}:E\to E\) be the induced basis transformation defined by \(\psi_{ij}e_k=\delta^i_ke_j\). The isomorphism \(\Psi:E\tprod E\to L(E;E)\) defined by \(\Psi(e_i\tprod e_j)=\psi_{ij}\) induces an isomorphism
\[\widehat{\Psi}:L(E\tprod E;E\tprod E)\iso L(L(E;E);L(E;E))\]
by \(\widehat{\Psi}(\sigma\tprod\tau)=\Psi\after(\sigma\tprod\tau)\after\Psi^{-1}\):
\begin{diagram}
E\tprod E					&\rTo^{\Psi}	&L(E;E)\\
\dTo<{\sigma\tprod\tau}		&				&\dTo>{\widehat{\Psi}(\sigma\tprod\tau)}\\
E\tprod E					&\rTo_{\Psi}	&L(E;E)
\end{diagram}
It is easy to verify that
\[\widehat{\Psi}(\beta^*\tprod\alpha)=\Phi\]
where \(\beta^*\)~is the transpose of~\(\beta\). Therefore by the previous exercise,
\[\tr\Phi=\tr(\beta^*\tprod\alpha)=\tr(\beta^*)\mult\tr\alpha=\tr\alpha\mult\tr\beta\]
and
\[\det\Phi=\det(\beta^*\tprod\alpha)=(\det\beta^*)^n(\det\alpha)^n=\det(\alpha\after\beta)^n\qedhere\]
\end{proof}
\begin{rmk}
It is more natural to do this problem using the composition algebra (see \S~26). Indeed, writing \(\alpha=a^*\tprod b\), \(\beta=c^*\tprod d\), and \(\sigma=x^*\tprod y\), we have
\begin{align*}
\alpha\after\sigma\after\beta&=(a^*\tprod b)\after(x^*\tprod y)\after(c^*\tprod d)\\
	&=(a^*\tprod b)\after\bigl(\sprod{x^*}{d}(c^*\tprod y)\bigr)\\
	&=\sprod{x^*}{d}\sprod{a^*}{y}(c^*\tprod b)\\
	&=\bigl(\sprod{x^*}{d}c^*\bigr)\tprod\bigl(\sprod{a^*}{y}b\bigr)\\
	&=\beta^*x^*\tprod\alpha y\\
	&=(\beta^*\tprod\alpha)\sigma
\end{align*}
so \(\Phi=\beta^*\tprod\alpha\). Here \(\beta^*=d\tprod c^*\).\footnote{See problem~1.29.1.} Alternately, we could use the isomorphism~\(\Omega\) from Proposition~1.29.1.
\end{rmk}

\subsection*{\S~20}
\begin{rmk}
A \emph{linear} map is universal if and only if it is an isomorphism. In fact, it satisfies~\(\medtprod_1\) if and only if it is surjective, and it satisfies~\(\medtprod_2\) if and only if it is injective (has a left inverse). In this sense, a tensor product map is a multilinear analogue of an isomorphism.
\end{rmk}

\begin{rmk}
To see that there is a unique isomorphism
\[f:(\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\to\tprods{E}{1}{p+q}\]
with
\[f((\tprods{x}{1}{p})\tprod(\tprods{x}{p+1}{p+q}))=\tprods{x}{1}{p+q}\tag{1}\]
observe that for each \(p\)-tuple \((x_1,\ldots,x_p)\in\timess{E}{1}{p}\), there is a \(q\)-linear map \(\varphi_{x_1\ldots x_p}:\timess{E}{p+1}{p+q}\to\tprods{E}{1}{p+q}\) given by
\[\varphi_{x_1\ldots x_p}(x_{p+1},\ldots,x_{p+q})=\tprods{x}{1}{p+q}\]
By the universal property of \(\tprods{E}{p+1}{p+q}\), it follows that there is a linear map \(f_{x_1\ldots x_p}:\tprods{E}{p+1}{p+q}\to\tprods{E}{1}{p+q}\) with
\[f_{x_1\ldots x_p}(\tprods{x}{p+1}{p+q})=\tprods{x}{1}{p+q}\]
Now the mapping \((x_1,\ldots,x_p)\mapsto f_{x_1\ldots x_p}\) is \(p\)-linear, so by the universal property of \(\tprods{E}{1}{p}\), there is a linear map
\[\widehat{f}:\tprods{E}{1}{p}\to L(\tprods{E}{p+1}{p+q};\tprods{E}{1}{p+q})\]
with \(\widehat{f}(\tprods{x}{1}{p})=f_{x_1\ldots x_p}\). Define
\[\varphi:(\tprods{E}{1}{p})\times(\tprods{E}{p+1}{p+q})\to\tprods{E}{1}{p+q}\]
by \(\varphi(x,y)=\widehat{f}(x)(y)\). Then \(\varphi\)~is bilinear, so by the universal property of \((\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\), there is a linear map~\(f\) satisfying \(f\after\tprod=\varphi\), which implies~(1).

By the universal property of \(\tprods{E}{1}{p+q}\), the \((p+q)\)-linear map
\[\psi:\timess{E}{1}{p+q}\to(\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\]
defined by
\[\psi(x_1,\ldots,x_{p+q})=(\tprods{x}{1}{p})\tprod(\tprods{x}{p+1}{p+q})\]
induces a linear map
\[g:\tprods{E}{1}{p+q}\to(\tprods{E}{1}{p})\tprod(\tprods{E}{p+1}{p+q})\]
with
\[g(\tprods{x}{1}{p+q})=(\tprods{x}{1}{p})\tprod(\tprods{x}{p+1}{p+q})\]
By universal properties again, \(f\after g=\iota\) and \(g\after f=\iota\) so \(f\)~is an isomorphism, and \(f\)~is uniquely determined by~(1).
\end{rmk}

\begin{rmk}
If the vectors~\(a^i_{\nu}\) are linearly independent in~\(E_i\) (\(i=1,\ldots,p\)), then their tensor products \(a^1_{\nu_1}\tprod\cdots\tprod a^p_{\nu_p}\) are linearly independent in \(\tprods{E}{1}{p}\). Indeed, if \(p=2\), this follows from Lemma~1.5.1. If \(p>2\), we use the natural isomorphism
\[\tprods{E}{1}{p}\iso(\tprods{E}{1}{p-1})\tprod E_p\]
If
\[\sum_{\nu_1,\ldots,\nu_p}\lambda_{\nu_1\ldots\nu_p}a^1_{\nu_1}\tprod\cdots\tprod a^p_{\nu_p}=0\]
then
\[\sum_{\nu_1,\ldots,\nu_p}(\lambda_{\nu_1\ldots\nu_p}a^1_{\nu_1}\tprod\cdots\tprod a^{p-1}_{\nu_{p-1}})\tprod a^p_{\nu_p}=0\]
Since the~\(a^p_{\nu_p}\) are linearly independent in~\(E_p\), it follows from Lemma~1.5.1 that
\[\lambda_{\nu_1\ldots\nu_p}a^1_{\nu_1}\tprod\cdots\tprod a^{p-1}_{\nu_{p-1}}=0\]
for all \(\nu_1,\ldots,\nu_p\). But \(a^1_{\nu_1}\tprod\cdots\tprod a^{p-1}_{\nu_{p-1}}\ne 0\) since \(a^i_{\nu}\ne 0\) for all \(i,\nu\) (see problem~1), so \(\lambda_{\nu_1\ldots\nu_p}=0\) for all \(\nu_1,\ldots,\nu_p\).

If the vectors~\(a^i_{\nu}\) span~\(E_i\) (\(i=1,\ldots,p\)), then clearly their tensor products span \(\tprods{E}{1}{p}\). It follows that if the vectors~\(a^i_{\nu}\) form a basis of~\(E_i\) (\(i=1,\ldots,p\)), then their tensor products form a basis of \(\tprods{E}{1}{p}\).
\end{rmk}

\begin{rmk}
To see that for \(\varphi_i:E_i\to F_i\) the map \((\varphi_1,\ldots,\varphi_p)\mapsto\tprods{\varphi}{1}{p}\) induces an injection
\[L(E_1;F_1)\tprod\cdots\tprod L(E_p;F_p)\to L(\tprods{E}{1}{p};\tprods{F}{1}{p})\]
proceed by induction on \(p\ge 2\). For \(p=2\), this is just Proposition~1.16.1. For \(p>2\), if we make the appropriate identifications we have
\begin{align*}
L(E_1;F_1)\tprod\cdots\tprod L(E_p;F_p)&=\bigl(L(E_1;F_1)\tprod\cdots\tprod L(E_{p-1};F_{p-1})\bigr)\tprod L(E_p;F_p)\\
	&\subseteq L(\tprods{E}{1}{p-1};\tprods{F}{1}{p-1})\tprod L(E_p;F_p)\\
	&\subseteq L((\tprods{E}{1}{p-1})\tprod E_p;(\tprods{F}{1}{p-1})\tprod F_p)\\
	&=L(\tprods{E}{1}{p};\tprods{F}{1}{p})
\end{align*}
\end{rmk}

\begin{exer}[1]
In \(\tprods{E}{1}{p}\):
\begin{enumerate}
\item[(a)] \(\tprods{x}{1}{p}=0\) if and only if at least one \(x_i=0\).
\item[(b)] If \(\tprods{x}{1}{p}\ne 0\), then
\[\tprods{x}{1}{p}=\tprods{y}{1}{p}\]
if and only if \(y_i=\lambda_i x_i\) with \(\lambda_1\cdots\lambda_p=1\).
\end{enumerate}
\end{exer}
\begin{proof}
The reverse directions follow from multilinearity of the tensor product. The forward directions follow by induction on~\(p\) using the natural isomorphism
\[\tprods{E}{1}{p}\iso(\tprods{E}{1}{p-1})\tprod E_p\]
\begin{enumerate}
\item[(a)] If \(p=2\), the result follows from Lemma~1.5.1. If \(p>2\), then \((\tprods{x}{1}{p-1})\tprod x_p=0\), so by Lemma~1.5.1 either \(\tprods{x}{1}{p-1}=0\) and \(x_i=0\) for some \(i=1,\ldots,p-1\) by induction, or else \(x_p=0\).
\item[(b)] If \(p=2\), the result follows from problem~1.8.4. If \(p>2\), then
\[(\tprods{x}{1}{p-1})\tprod x_p=(\tprods{y}{1}{p-1})\tprod y_p\]
so by the same problem,
\[\tprods{y}{1}{p-1}=\lambda_p^{-1}\tprods{x}{1}{p-1}\qquad\text{and}\qquad y_p=\lambda_p x_p\]
for some \(\lambda_p\ne 0\). Setting \(\mu=\lambda_p^{-1}/(p-1)\), we have
\[\tprods{y}{1}{p-1}=\mu x_1\tprod\cdots\tprod\mu x_{p-1}\]
so \(y_i=\mu_i\mu x_i\) for \(i=1,\ldots,p-1\) with \(\mu_1\cdots\mu_{p-1}=1\) by induction. Setting \(\lambda_i=\mu_i\mu\) for \(i=1,\ldots,p-1\), we have \(y_i=\lambda_i x_i\) for \(i=1,\ldots,p\) and \(\lambda_1\cdots\lambda_p=1\) as desired.\qedhere
\end{enumerate}
\end{proof}

\subsection*{\S~21}
\begin{rmk}
For \(p,q\ge 1\), let
\[\varphi_i:\prod_{j=1}^p E^i_j\to E^i_{p+1}\qquad(i=1,\ldots,q)\]
be a family of \(q\) \(p\)-linear maps. Then there is a unique \(p\)-linear map
\[\varphi=\tprods{\varphi}{1}{q}:\prod_{j=1}^p\Biggl(\bigtprod_{i=1}^q E^i_j\Biggr)\to\bigtprod_{i=1}^q E^i_{p+1}\]
with
\[\varphi(x^1_1\tprod\cdots\tprod x^q_1\,,\,\ldots\,,\,x^1_p\tprod\cdots\tprod x^q_p)=\varphi_1(x^1_1,\ldots,x^1_p)\tprod\cdots\tprod\varphi_q(x^q_1,\ldots,x^q_p)\]
Moreover,
\[\ker_j\varphi=\sum_{i=1}^q E^1_j\tprod\cdots\tprod\ker_j\varphi_i\tprod\cdots\tprod E^q_j\qquad(j=1,\ldots,p)\]
In particular, if each \(\varphi_i\)~is nondegenerate, then \(\varphi\)~is nondegenerate.
\end{rmk}

\subsection*{\S~22}
\begin{rmk}
If \(\Phi\)~is bilinear in \(E\times E'\) and \(\Psi\)~is bilinear in \(F\times F'\), then nondegeneracy of \(\Phi\tprod\Psi\) does not imply nondegeneracy of \(\Phi\) and~\(\Psi\) unless all the spaces are nonzero, contrary to what the book says. For example, if \(E\ne 0\) and \(E'=0\), \(F=F'=0\), and \(\Phi=0\) and \(\Psi=0\), then \(\Phi\)~is degenerate, but \(E\tprod F=0\) and \(E'\tprod F'=0\), so \(\Phi\tprod\Psi=0\) is nondegenerate.\footnote{This is essentially the same error as in problem~1.19.1(a) above.}
\end{rmk}

\subsection*{\S~26}
\begin{rmk}
If \(e_1,\ldots,e_n\) is a basis in~\(E\) and \(f_1,\ldots,f_n\) is its dual basis in~\(L(E)\), then the basis transformation~\(\varphi_{ij}\) in~\(L(E;E)\) with \(\varphi_{ij}(e_k)=\delta_{ik}e_j\) is given by \(x\mapsto f_i(x)e_j\). Therefore it is natural to consider the isomorphism \(T:L(E)\tprod E\to L(E;E)\) with \(T(f_i\tprod e_j)=\varphi_{ij}\). In the algebra induced by~\(T^{-1}\) in \(L(E)\tprod E\),
\[(f_i\tprod e_j)\after(f_k\tprod e_l)=T^{-1}(\varphi_{ij}\after\varphi_{kl})=T^{-1}(\delta_{il}\varphi_{kj})=f_i(e_l)(f_k\tprod e_j)\]
This motivates the definition of the composition algebra. An alternative way to discover the isomorphism is (see \S~28)
\[L(E;E)\iso L(E\tprod\Gamma;\Gamma\tprod E)\iso L(E;\Gamma)\tprod L(\Gamma;E)\iso L(E)\tprod E\]
\end{rmk}

\begin{exer}[1]
For the bilinear map
\[\gamma:L(E,E';E'')\times L(F,F';F'')\to L(E\tprod F,E'\tprod F';E''\tprod F'')\]
with
\[\gamma(\varphi,\psi):(x\tprod y,x'\tprod y')\mapsto\varphi(x,x')\tprod\psi(y,y')\]
the pair \((\im\gamma,\gamma)\) is the tensor product of \(L(E,E';E'')\) and \(L(F,F';F'')\).
\end{exer}
\begin{proof}
By transfer of Corollary~II of Proposition~1.16.1, using the isomorphism between linear and bilinear maps induced by the tensor product.

In detail, consider the following commutative diagram:
\begin{diagram}[nohug]
L(E\tprod E';E'')\tprod L(F\tprod F';F'')	&\rTo^f			&L((E\tprod E')\tprod(F\tprod F');E''\tprod F'')\\
											&				&\dTo>{\iso}\\
\uTo<{\iso}									&				&L((E\tprod F)\tprod(E'\tprod F');E''\tprod F'')\\
											&				&\dTo>{\iso}\\
L(E,E';E'')\tprod L(F,F';F'')				&\rTo^g			&L(E\tprod F,E'\tprod F';E''\tprod F'')\\
\uTo<{\tprod}								&\ruTo>{\gamma}	&\\
L(E,E';E'')\times L(F,F';F'')				&				&
\end{diagram}
Note \(g\)~is injective since \(f\)~is injective (1.16.1), so \(\gamma\)~satisfies \(\medtprod_2\) (1.8.1). Since \(\gamma\)~also satisfies~\(\medtprod_1\), it follows that \(\gamma\)~is the tensor product.
\end{proof}

\begin{exer}[2]
If \(E,E^*\) and \(F,F^*\) are pairs of dual spaces with \(E_1\subseteq E\) and \(F_1\subseteq F\) subspaces, then a scalar product is induced between
\[(E^*\tprod F^*)/(\oc{E_1}\tprod F^*+E^*\tprod\oc{F_1})\qquad\text{and}\qquad E_1\tprod F_1\]
by the scalar product between \(E^*\tprod F^*\) and \(E\tprod F\). In particular,
\[\oc{(E_1\tprod F_1)}=\oc{E_1}\tprod F^*+E^*\tprod\oc{F_1}\]
\end{exer}
\begin{proof}
A scalar product is induced between \(E^*/\oc{E_1}\) and~\(E_1\) by \(\sprod{\proj{x^*}}{x}=\sprod{x^*}{x}\), and between \(F^*/\oc{F_1}\) and~\(F_1\) by \(\sprod{\proj{y^*}}{y}=\sprod{y^*}{y}\). Therefore a scalar product is induced between \((E^*/\oc{E_1})\tprod(F^*/\oc{F_1})\) and \(E_1\tprod F_1\) by
\[\sprod{\proj{x^*}\tprod\proj{y^*}}{x\tprod y}=\sprod{x^*}{x}\sprod{y^*}{y}=\sprod{x^*\tprod y^*}{x\tprod y}\]
where the scalar product on the right is between \(E^*\tprod F^*\) and \(E\tprod F\). However,
\[(E^*\tprod F^*)/(\oc{E_1}\tprod F^*+E^*\tprod\oc{F_1})\iso(E^*/\oc{E_1})\tprod(F^*/\oc{F_1})\]
where \(\proj{x^*\tprod y^*}\mapsto\proj{x^*}\tprod\proj{y^*}\), so there is a scalar product
\[\sprod{\proj{x^*\tprod y^*}}{x\tprod y}=\sprod{x^*\tprod y^*}{x\tprod y}\]
as required.
\end{proof}

\begin{exer}[3]
If \(E,E^*\) are dual spaces and \(\varphi:E\to E\) and \(\varphi^*:E^*\from E^*\) are dual transformations, then \(\varphi\tprod\varphi^*\)~is self-dual.
\end{exer}
\begin{proof}
Recall that \(E\tprod E^*\) is self-dual under the scalar product
\[\sprod{x\tprod x^*}{y\tprod y^*}=\sprod{x^*}{y}\sprod{y^*}{x}\]
Now
\begin{align*}
\sprod{(\varphi\tprod\varphi^*)(x\tprod x^*)}{y\tprod y^*}&=\sprod{\varphi x\tprod\varphi^* x^*}{y\tprod y^*}\\
	&=\sprod{\varphi^* x^*}{y}\sprod{y^*}{\varphi x}\\
	&=\sprod{x^*}{\varphi y}\sprod{\varphi^* y^*}{x}\\
	&=\sprod{x\tprod x^*}{\varphi y\tprod\varphi^* y^*}\\
	&=\sprod{x\tprod x^*}{(\varphi\tprod\varphi^*)(y\tprod y^*)}
\end{align*}
so \((\varphi\tprod\varphi^*)^*=\varphi\tprod\varphi^*\).
\end{proof}

\subsection*{\S~28}
\textbf{In this subsection, all vector spaces are finite-dimensional.}

\begin{rmk}
By~(1.30), if we view \(a^*\tprod a\) as a linear transformation, then its trace is just \(\sprod{a^*}{a}\). This gives a natural (coordinate-free) definition of the trace. This also gives a slick proof that \(\tr(\varphi\tprod\psi)=\tr\varphi\mult\tr\psi\). Indeed, if we write \(\varphi=a^*\tprod a\) and \(\psi=b^*\tprod b\), then we can write \(\varphi\tprod\psi=(a^*\tprod b^*)\tprod(a\tprod b)\) since
\begin{align*}
(\varphi\tprod\psi)(x\tprod y)&=\varphi x\tprod\psi y\\
	&=(\sprod{a^*}{x}a)\tprod(\sprod{b^*}{y}b)\\
	&=\sprod{a^*}{x}\sprod{b^*}{y}(a\tprod b)\\
	&=\sprod{a^*\tprod b^*}{x\tprod y}(a\tprod b)
\end{align*}
Therefore
\[\tr(\varphi\tprod\psi)=\sprod{a^*\tprod b^*}{a\tprod b}=\sprod{a^*}{a}\sprod{b^*}{b}=\tr\varphi\mult\tr\psi\]
\end{rmk}

\subsection*{\S~29}
\textbf{In this subsection, all vector spaces are finite-dimensional.}

\begin{rmk}
In the proof of Proposition~1.29.1, note that if \(\alpha=a^*\tprod b\) and \(\beta=c^*\tprod d\), then
\[F(\alpha\tprod\beta)(x^*\tprod y)=\sprod{a^*\tprod b}{x^*\tprod y}(c^*\tprod d)=\sprod{x^*}{b}\sprod{a^*}{y}(c^*\tprod d)\]
Comparing with the calculation after problem~1.19.3 above, we immediately see that \(Q\), which swaps \(b\) and~\(d\), satisfies
\[F(Q(\alpha\tprod\beta))=\beta^*\tprod\alpha=\Omega(\alpha\tprod\beta)\]
These results show that we can view \(L(A;A)\) as a tensor product in multiple ways.
\end{rmk}

\begin{exer}[1]
Let \(E,E^*\) and \(F,F^*\) be pairs of dual spaces. If \(a^*\in E^*\) and \(b\in F\), then
\[(a^*\tprod b)^*=b\tprod a^*\]
\end{exer}
\begin{proof}
Recall \(a^*\tprod b:E\to F\) is defined by \(x\mapsto\sprod{a^*}{x}b\) and \(b\tprod a^*:F^*\to E^*\) is defined by \(y^*\mapsto\sprod{y^*}{b}a^*\). For \(y^*\in F^*\) and \(x\in E\),
\[\sprod{(b\tprod a^*)y^*}{x}=\sprod{y^*}{b}\sprod{a^*}{x}=\sprod{y^*}{(a^*\tprod b)x}\qedhere\]
\end{proof}

\begin{exer}[2]
If \(E,F\ne 0\) are Euclidean spaces with \(\varphi:E\to E\) and \(\psi:F\to F\), then \(\varphi\tprod\psi:E\tprod F\to E\tprod F\) is a rotation if and only if \(\varphi=\lambda\tau_E\) and \(\psi=\lambda^{-1}\tau_F\) where \(\tau_E\) and~\(\tau_F\) are rotations and \(\lambda\ne 0\).
\end{exer}
\begin{proof}
If \(\varphi=\lambda\tau_E\) and \(\psi=\lambda^{-1}\tau_F\), then \(\varphi\tprod\psi=\tau_E\tprod\tau_F\), and
\begin{align*}
\adj{\tau_E\tprod\tau_F}\after(\tau_E\tprod\tau_F)&=(\adj{\tau_E}\tprod\adj{\tau_F})\after(\tau_E\tprod\tau_F)\\
	&=(\adj{\tau_E}\after\tau_E)\tprod(\adj{\tau_F}\after\tau_F)\\
	&=\iota_E\tprod\iota_F\\
	&=\iota_{E\tprod F}
\end{align*}
so \(\adj{\varphi\tprod\psi}=(\varphi\tprod\psi)^{-1}\) and \(\varphi\tprod\psi\)~is a rotation.

Conversely if \(\varphi\tprod\psi\)~is a rotation, then
\[\adj{\varphi}\tprod\adj{\psi}=\adj{\varphi\tprod\psi}=(\varphi\tprod\psi)^{-1}=\varphi^{-1}\tprod\psi^{-1}\]
It follows that \(\adj{\varphi}=\mu\varphi^{-1}\) and \(\adj{\psi}=\mu^{-1}\psi^{-1}\) for some \(\mu\ne 0\).\footnote{See problem~1.8.4.} We may assume \(\mu>0\) (otherwise consider \(-\varphi\) and~\(-\psi\)). Set \(\lambda=\sqrt{\mu}>0\) and define \(\tau_E=\lambda^{-1}\varphi\) and \(\tau_F=\lambda\psi\). Then \(\adj{\tau_E}\after\tau_E=\iota_E\), so \(\adj{\tau_E}=\tau_E^{-1}\) and \(\tau_E\)~is a rotation, and similarly for~\(\tau_F\).
\end{proof}
\begin{rmk}
The proofs of the other parts of this problem in the book are similar.
\end{rmk}

\begin{exer}[3]
Let \(E\)~be a real vector space with \(\dim E=n\) and \(\varphi,\psi:E\to E\) be regular transformations.
\begin{enumerate}
\item[(a)] If \(n\)~is even, then \(\varphi\tprod\psi\) preserves orientation.
\item[(b)] If \(n\)~is odd, then \(\varphi\tprod\psi\) preserves orientation if and only if \(\varphi\) and~\(\psi\) either both preserve orientation or both reverse orientation.
\end{enumerate}
\end{exer}
\begin{proof}
This follows from \(\det(\varphi\tprod\psi)=(\det\varphi\mult\det\psi)^n\).\footnote{See problem~1.19.2.}
\end{proof}

\section*{Chapter~2}
\subsection*{\S~2}
\begin{rmk}
If \(A\) and~\(B\) are algebras and \(A\tprod B\) denotes the tensor product of the underlying \emph{vector spaces}, define \(\varphi:A\times B\times A\times B\to A\tprod B\) by
\[\varphi(x_1,y_1,x_2,y_2)=x_1x_2\tprod y_1y_2\]
Then \(\varphi\)~is multilinear, so it induces a linear map \(f:(A\tprod B)\tprod(A\tprod B)\to A\tprod B\) satisfying
\[f((x_1\tprod y_1)\tprod(x_2\tprod y_2))=x_1x_2\tprod y_1y_2\]
Now the bilinear map \(f\after\tprod\) makes \(A\tprod B\) into an algebra with
\[(x_1\tprod y_1)(x_2\tprod y_2)=x_1x_2\tprod y_1y_2\]
This construction avoids the use of structure maps.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
If \(\varphi_1:A_1\to B_1\) and \(\varphi_2:A_2\to B_2\) are algebra homomorphisms and \(\varphi=\varphi_1\tprod\varphi_2\) is the tensor product of the underlying \emph{linear maps}, then
\begin{align*}
\varphi((x_1\tprod x_2)(y_1\tprod y_2))&=\varphi(x_1y_1\tprod x_2y_2)\\
	&=\varphi_1(x_1y_1)\tprod\varphi_2(x_2y_2)\\
	&=(\varphi_1x_1\varphi_1y_1)\tprod(\varphi_2x_2\varphi_2y_2)\\
	&=(\varphi_1x_1\tprod\varphi_2 x_2)(\varphi_1y_1\tprod\varphi_2 y_2)\\
	&=\varphi(x_1\tprod x_2)\varphi(y_1\tprod y_2)
\end{align*}
Since \(\varphi\)~is linear, it follows that \(\varphi\)~is an algebra homomorphism. This proof avoids the use of structure maps.
\end{rmk}

\section*{Chapter~3}
\subsection*{\S~3}
\begin{rmk}
The tensor algebra~\(\medtprod E\) is a universal (initial) object in the category of ``associative unital algebras with linear maps of~\(E\) into them''. In this category, the objects are linear maps \(E\to A\), for associative unital algebras~\(A\), and the arrows are algebra homomorphisms \(A\to B\) which preserve the units and respect the linear maps from~\(E\):
\begin{diagram}
E&\rTo&B\\
\dTo&\ruTo&\\
A&&
\end{diagram}
Every object in this category can be obtained from the tensor algebra~\(\medtprod E\) in a unique way. This is why \(\medtprod E\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(\medtprod E\) satisfy only those properties that are required to make~\(\medtprod E\) into an associative unital algebra containing~\(E\). By category theoretic abstract nonsense, \(\medtprod E\)~is unique up to isomorphism (\S~4).
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
We have a functor from the category of vector spaces into the category of associative unital algebras, which sends vector spaces \(E\) and~\(F\) to the tensor algebras \(\medtprod E\) and~\(\medtprod F\), and which sends a linear map \(\varphi:E\to F\) to the algebra homomorphism \(\varphi_{\tprod}:\medtprod E\to\medtprod F\).
\end{rmk}

\begin{rmk}
By a remark from \S~21 of chapter~1 above,
\[\ker\varphi_{\tprod}=\sum_{p=0}^{\infty}\sum_{i=1}^p E\tprod\cdots\tprod\ker\varphi\tprod\cdots\tprod E\]
where \(\ker\varphi\)~is in position \(i\)~of~\(p\) on the right.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
There is a connection between the derivation and the trace of a linear transformation. If \(E\)~is an \(n\)-dimensional vector space and \(\varphi:E\to E\) is linear, let \(\Delta:E^n\to\Gamma\) be a nonzero determinant function in~\(E\). Recall
\[\tr\varphi\mult\Delta(x_1,\ldots,x_n)=\sum_{i=1}^n\Delta(x_1,\ldots,\varphi x_i,\ldots,x_n)\]
By the universal property of the tensor product~\(\medtprod^n E\), there is an induced linear function \(\Delta_{\tprod}:\medtprod^n E\to\Gamma\) with \(\Delta_{\tprod}(x_1\tprod\cdots\tprod x_n)=\Delta(x_1,\ldots,x_n)\).
Now
\begin{align*}
\tr\varphi\mult\Delta_{\tprod}(x_1\tprod\cdots\tprod x_n)&=\sum_{i=1}^n \Delta_{\tprod}(x_1\tprod\cdots\tprod\varphi x_i\tprod\cdots\tprod x_n)\\
	&=\Delta_{\tprod}(\sum_{i=1}^n x_1\tprod\cdots\tprod\varphi x_i\tprod\cdots\tprod x_n)\\
	&=\Delta_{\tprod}(\theta_{\circled{n}}(\varphi)(x_1\tprod\cdots\tprod x_n))
\end{align*}
It follows that
\[\Delta_{\tprod}\after(\tr\varphi\mult\iota)=\tr\varphi\mult\Delta_{\tprod}=\Delta_{\tprod}\after\theta_{\circled{n}}(\varphi)\]
Since \(\theta_{\circled{n}}\)~is linear by~(3.7), it follows that the trace is also linear. If \(\psi:E\to E\) is another linear transformation, then it follows from~(3.8) that
\[\tr(\varphi\psi)-\tr(\psi\varphi)=\tr(\varphi\psi-\psi\varphi)=\tr\varphi\mult\tr\psi-\tr\psi\mult\tr\varphi=0\]
so \(\tr(\varphi\psi)=\tr(\psi\varphi)\).\footnote{This is what problem~3.7.3 was supposed to be.}
\end{rmk}

\subsection*{\S~7}
\begin{exer}[1]
If \(u_1=a_1\tprod b_1\ne 0\) and \(u_2=a_2\tprod b_2\) are decomposable tensors, then \(u_1+u_2\) is decomposable if and only if \(a_2=\lambda a_1\) or \(b_2=\mu b_1\) for some \(\lambda,\mu\in\Gamma\).
\end{exer}
\begin{proof}
By the remark after problem~1.15.1 above.
\end{proof}

\subsection*{\S~11}
\begin{rmk}
In the finite-dimensional case, the following are familiar types of mixed tensors in \(E^*,E\):
\begin{center}
\begin{tabular}{|c|l|}
\hline
\textbf{Bidegree}&\textbf{Type}\\
\hline
(0,0)&Scalar\\
(0,1)&Vector\\
(1,0)&Linear function\\
(1,1)&Linear transformation\\
\hline
\end{tabular}
\end{center}
\end{rmk}

\subsection*{\S~12}
\begin{rmk}
We have
\[\medtprod(E^*,E)=\sum_{p,q}\medtprod^p_q(E^*,E)\]
An element of this algebra is a finite sum of decomposable homogeneous mixed tensors. As an example of multiplication, if \(x^*\tprod x\) and \(y^*\tprod y\) are (1,1)-tensors, then
\[(x^*\tprod x)(y^*\tprod y)=x^*\tprod y^*\tprod x\tprod y\]
is a (2,2)-tensor.
\end{rmk}

\begin{rmk}
The space \(\medtprod(E^*,E)\) is clearly dual to itself, and to \(\medtprod(E,E^*)\).
\end{rmk}

\subsection*{\S~14}
\begin{rmk}
The mapping \(T:GL(E)\to GL(\medtprod^p_q(E^*,E))\) given by \(\alpha\mapsto T_{\alpha}\) is a group homomorphism. The map~\(T_{\alpha}\) is tensorial if \(\alpha\)~is in the center of~\(GL(E)\).
\end{rmk}

\begin{rmk}
We have \(T_{\alpha}^*=T_{\alpha^{-1}}\). Indeed, since \((\alpha^{\tprod})^{-1}=(\alpha^{-1})^{\tprod}\),
\begin{align*}
\sprod{y^*\tprod y}{T_{\alpha}(x^*\tprod x)}&=\sprod{y^*\tprod y}{(\alpha^{\tprod})^{-1}x^*\tprod\alpha_{\tprod}x}\\
	&=\sprod{y^*}{\alpha_{\tprod}x}\sprod{(\alpha^{\tprod})^{-1}x^*}{y}\\
	&=\sprod{\alpha^{\tprod}y^*}{x}\sprod{x^*}{(\alpha^{-1})_{\tprod}y}\\
	&=\sprod{\alpha^{\tprod}y^*\tprod(\alpha^{-1})_{\tprod}y}{x^*\tprod x}\\
	&=\sprod{T_{\alpha^{-1}}(y^*\tprod y)}{x^*\tprod x}
\end{align*}
\end{rmk}

\begin{rmk}
Writing \(\medtprod^p_q(E^*,E)\tprod\medtprod^p_q(E^*,E)=\medtprod^p_q(E^*\tprod E^*,E\tprod E)\) (see problem~10), we have \(T_{\alpha}\tprod T_{\beta}=T_{\alpha\tprod\beta}\). Writing \(\medtprod^p_q(E^*,E)\tprod\medtprod^r_s(E^*,E)=\medtprod^{p+r}_{q+s}(E^*,E)\), we have \(T_{\alpha}\tprod T_{\alpha}=T_{\alpha}\).
\end{rmk}

\begin{rmk}
If \(e_i,e^{*i}\) and \(\bar{e}_i,\bar{e}^{*i}\) are pairs of dual bases in \(E,E^*\) and \(\alpha\)~is the change of basis transformation \(e_i\mapsto\bar{e}_i\) in~\(E\), then \(\alpha^{*-1}\)~is the corresponding change of basis transformation \(e^{*i}\mapsto\bar{e}^{*i}\) in~\(E^*\).\footnote{See the remark in chapter~III, \S~3 of~\cite{greub1} above.} Therefore in \(\medtprod^p_q(E^*,E)\),
\[T_{\alpha}(e^{\nu_1\cdots\nu_p}_{\mu_1\cdots\mu_q})=\bar{e}^{\nu_1\cdots\nu_p}_{\mu_1\cdots\mu_q}\]
In other words, \(T_{\alpha}\)~is the induced change of basis transformation in \(\medtprod^p_q(E^*,E)\). It follows that a mapping is tensorial if and only if its matrix is invariant under this type of change of basis.
\end{rmk}

\begin{rmk}
For \(\alpha=\lambda\iota\) with \(\lambda\ne0\), \(\alpha^{*-1}=\lambda^{-1}\iota\), so \(T_{\alpha}=\lambda^{q-p}\iota\) on \(\medtprod^p_q(E^*,E)\).
\end{rmk}

\begin{rmk}
If \(E\)~is finite-dimensional and we identify \(\medtprod^1_1(E^*,E)\) with \(L(E;E)\) in the natural way, then for any automorphism \(\alpha\) of~\(E\),
\[T_{\alpha}(a^*\tprod b)=\alpha\after(a^*\tprod b)\after\alpha^{-1}\]
\end{rmk}

\begin{exer}[1]
If \(e_{\nu},e^{*\nu}\) and \(\bar{e}_{\nu},\bar{e}^{*\nu}\) are pairs of dual bases in \(E,E^*\) related by
\[\bar{e}_{\nu}=\sum_{\lambda}\alpha_{\nu}^{\lambda}e_{\lambda}\qquad\text{and}\qquad\bar{e}^{*\nu}=\sum_{\lambda}\beta^{\nu}_{\lambda}e^{*\lambda}\]
then the components of a mixed tensor \(w\in\medtprod^p_q(E^*,E)\) are related by
\[\bar{\zeta}^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}=\sum_{(\kappa),(\lambda)}\alpha_{\nu_1}^{\lambda_1}\cdots\alpha_{\nu_p}^{\lambda_p}\beta^{\mu_1}_{\kappa_1}\cdots\beta^{\mu_q}_{\kappa_q}\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}\]
\end{exer}
\begin{proof}
In this proof, we use the Einstein summation convention to simplify the notation. Since \(e_{\kappa}=\beta_{\kappa}^{\mu}\bar{e}_{\mu}\) and \(e^{*\lambda}=\alpha^{\lambda}_{\nu}\bar{e}^{*\nu}\), we have
\begin{align*}
w&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}e^{\lambda_1\cdots\lambda_p}_{\kappa_1\cdots\kappa_q}\\
	&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}e^{*\lambda_1}\tprod\cdots\tprod e^{*\lambda_p}\tprod e_{\kappa_1}\tprod\cdots\tprod e_{\kappa_q}\\
	&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}(\alpha^{\lambda_1}_{\nu_1}\bar{e}^{*\nu_1})\tprod\cdots\tprod(\alpha^{\lambda_p}_{\nu_p}\bar{e}^{*\nu_p})\tprod(\beta_{\kappa_1}^{\mu_1}\bar{e}_{\mu_1})\tprod\cdots\tprod(\beta_{\kappa_q}^{\mu_q}\bar{e}_{\mu_q})\\
	&=\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}\alpha^{\lambda_1}_{\nu_1}\cdots\alpha^{\lambda_p}_{\nu_p}\beta_{\kappa_1}^{\mu_1}\cdots\beta_{\kappa_q}^{\mu_q}\bar{e}^{\nu_1\cdots\nu_p}_{\mu_1\cdots\mu_q}
\end{align*}
Therefore
\[\bar{\zeta}^{\mu_1\cdots\mu_q}_{\nu_1\cdots\nu_p}=\alpha_{\nu_1}^{\lambda_1}\cdots\alpha_{\nu_p}^{\lambda_p}\beta^{\mu_1}_{\kappa_1}\cdots\beta^{\mu_q}_{\kappa_q}\zeta^{\kappa_1\cdots\kappa_q}_{\lambda_1\cdots\lambda_p}\qedhere\]
\end{proof}
\begin{rmk}
This relationship serves as the basis of an alternative definition of a \emph{tensor} as a multidimensional array of numbers which transforms according to this rule under change of basis.
\end{rmk}

\begin{exer}[3]
For \(u^*\in\medtprod^p E^*\) and \(u\in\medtprod^p E\),
\[\sprod{u^*}{u}=(C^1_1)^p(u^*\tprod u)\]
\end{exer}
\begin{proof}
By induction on~\(p\). For \(p=0\) the result is trivial, and for \(p=1\) it follows from the definition of~\(C^1_1\). For \(p>1\), write \(u^*=u^{*1}\tprod\tilde{u}^*\) and \(u=u_1\tprod\tilde{u}\). Then
\begin{align*}
(C^1_1)^p(u^*\tprod u)&=(C^1_1)^{p-1}(C^1_1(u^*\tprod u))\\
	&=\sprod{u^{*1}}{u_1}(C^1_1)^{p-1}(\tilde{u}^*\tprod\tilde{u})\\
	&=\sprod{u^{*1}}{u_1}\sprod{\tilde{u}^*}{\tilde{u}}\\
	&=\sprod{u^*}{u}\qedhere
\end{align*}
\end{proof}

\begin{exer}[4]
If the mappings
\[\Phi:\medtprod^p_q(E^*,E)\to\medtprod^r_s(E^*,E)\quad\text{and}\quad\Phi^*:\medtprod^q_p(E^*,E)\from\medtprod^s_r(E^*,E)\]
are dual and \(\Phi\)~is tensorial, then \(\Phi^*\)~is tensorial.
\end{exer}
\begin{proof}
If \(\alpha\)~is an automorphism of~\(E\), then by a remark above
\[\Phi^*\after T_{\alpha}=\Phi^*\after T_{\alpha^{-1}}^*=(T_{\alpha^{-1}}\after\Phi)^*=(\Phi\after T_{\alpha^{-1}})^*=T_{\alpha^{-1}}^*\after\Phi^*=T_{\alpha}\after\Phi^*\qedhere\]
\end{proof}

\begin{exer}[5]
The sum, composite, and tensor product of tensorial mappings are tensorial.
\end{exer}
\begin{proof}
If \(\Phi\) and~\(\Psi\) are tensorial, then it is obvious that \(\Phi+\Psi\) and \(\Phi\after\Psi\) are also tensorial. By a remark above,
\begin{align*}
(\Phi\tprod\Psi)\after T_{\alpha}&=(\Phi\tprod\Psi)\after(T_{\alpha}\tprod T_{\alpha})\\
	&=(\Phi\after T_{\alpha})\tprod(\Psi\after T_{\alpha})\\
	&=(T_{\alpha}\after\Phi)\tprod(T_{\alpha}\after\Psi)\\
	&=(T_{\alpha}\tprod T_{\alpha})\after(\Phi\tprod\Psi)\\
	&=T_{\alpha}\after(\Phi\tprod\Psi)
\end{align*}
so \(\Phi\tprod\Psi\) is tensorial.
\end{proof}

\begin{exer}[6]
If \(\Phi:\medtprod^p_q(E^*,E)\to\medtprod^r_s(E^*,E)\) is a nonzero tensorial mapping, then \(r-p=s-q\).
\end{exer}
\begin{proof}
Let \(\lambda>1\). By a remark above, \(\lambda^{q-p}\Phi=\lambda^{s-r}\Phi\) since \(\Phi\)~is tensorial. It follows that \(\lambda^{q-p+r-s}=1\) since \(\Phi\ne 0\), so \(q-p+r-s=0\) since \(\lambda>1\).
\end{proof}

\begin{exer}[7]
If \(E\)~is finite-dimensional and for \(a\in E^*\tprod E\) the linear map~\(\mu(a)\) is defined by \(z\mapsto a\tprod z\) for \(z\in E\), then \(\mu(a)\)~is tensorial if and only if \(a=\lambda t\) where \(t\)~is the unit tensor.
\end{exer}
\begin{proof}
By direct computation, it is easily verified that \(\mu(a)\)~is tensorial if and only if \(T_{\alpha}(a)=a\) for all automorphisms \(\alpha\) of~\(E\). By a remark above, the latter is true if and only if \(a\), when viewed as a transformation of~\(E\), is preserved under conjugation, which is true if and only if it is a scalar transformation. Since the unit tensor corresponds to the identity transformation, the result follows.
\end{proof}

\begin{exer}[8]
If \(E\)~is finite-dimensional, then every tensorial map \(\Phi:E^*\tprod E\to\Gamma\) is of the form \(\Phi=\lambda\mult C\) where \(C\)~is the contraction operator.
\end{exer}
\begin{proof}
This follows from problem~1.3.7 applied to \(\Phi\after\tprod\).
\end{proof}

\subsection*{\S~15}
\begin{rmk}
The induced inner products in \(\medtprod^p E\) and \(\medtprod E\) are just the induced scalar products from \S~8 with \(E^*=E\) under the inner product.
\end{rmk}

\subsection*{\S~17}
\begin{rmk}
The metric tensors encode all of the information required to compute inner products, and hence metric properties (length, angle, etc.), in \(E\) and~\(E^*\).
\end{rmk}

\begin{rmk}
We have
\[\tau_{\tprod}(g)=\sum_{\nu}\tau e_{\nu}\tprod\tau e_{\nu}=g^*\]
so \(\tau_{\tprod}^{-1}(g^*)=g\). This yields
\begin{align*}
\iprod{x^*}{y^*}&=\iprod{\tau^{-1}x^*}{\tau^{-1}y^*}\\
	&=\sprod{g^*}{\tau^{-1}x^*\tprod\tau^{-1}y^*}\\
	&=\sprod{g^*}{\tau_{\tprod}^{-1}(x^*\tprod y^*)}\\
	&=\sprod{x^*\tprod y^*}{\tau_{\tprod}^{-1}g^*}\\
	&=\sprod{x^*\tprod y^*}{g}
\end{align*}
\end{rmk}

\begin{exer}[4]
If \(E,E^*\) is a pair of dual Euclidean spaces and \(\varphi=a^*\tprod b\) is a linear transformation of~\(E\), then \(\adj{\varphi}=\tau b\tprod\tau^{-1}a^*\).
\end{exer}
\begin{proof}
Let \(\psi=\tau b\tprod\tau^{-1}a^*\). For \(x\in E\),
\[\psi x=\sprod{\tau b}{x}\tau^{-1}a^*=\iprod{x}{b}\tau^{-1}a^*\]
so for \(y\in E\),
\[\iprod{\psi x}{y}=\iprod{x}{b}\iprod{\tau^{-1}a^*}{y}=\iprod{x}{b}\sprod{a^*}{y}=\iprod{x}{\varphi y}\]
Therefore \(\psi=\adj{\varphi}\).
\end{proof}

\begin{exer}[7]
If \(E,E^*\) is a pair of dual Euclidean spaces and \(x_{\nu},x^{*\nu}\) a pair of dual bases, then
\[g^*=\sum_{\nu,\mu}\iprod{x_{\nu}}{x_{\mu}}x^{*\nu}\tprod x^{*\mu}\qquad\text{and}\qquad g=\sum_{\nu,\mu}\iprod{x^{*\nu}}{x^{*\mu}}x_{\nu}\tprod x_{\mu}\]
\end{exer}
\begin{proof}
Since \(x_{\nu}\tprod x_{\mu}\) and \(x^{*\nu}\tprod x^{*\mu}\) are dual bases of \(E\tprod E\) and \(E^*\tprod E^*\), we have
\[g^*=\sum_{\nu,\mu}g_{\nu\mu}x^{*\nu}\tprod x^{*\mu}\qquad\text{and}\qquad g=\sum_{\nu,\mu}g^{\nu\mu}x_{\nu}\tprod x_{\mu}\]
where
\[g_{\nu\mu}=\sprod{g^*}{x_{\nu}\tprod x_{\mu}}=\iprod{x_{\nu}}{x_{\mu}}\]
and
\[g^{\nu\mu}=\sprod{x^{*\nu}\tprod x^{*\mu}}{g}=\iprod{x^{*\nu}}{x^{*\mu}}\qedhere\]
\end{proof}
\begin{rmk}
It follows that the components of~\(g^*\) vary in the same way as the basis vectors in~\(E\) under a change of basis, while the components of~\(g\) vary inversely. This is why \(g^*\)~is called ``covariant'' and \(g\)~is called ``contravariant''.
\end{rmk}

\subsection*{\S~19}
\begin{rmk}
The definition of the substitution operator~\(i_{\nu}(h)\) does not specify the behavior on \(p\)-linear functions with \(p<\nu\).
\end{rmk}

\begin{rmk}
The definitions of the substitution operators \(i_A(h)\) and~\(i_S(h)\) given in this subsection are inappropriate for use with the algebras of skew-symmetric and symmetric multilinear functions, respectively. See below.
\end{rmk}

\subsection*{\S~23}
\begin{rmk}
We have
\[T^p_q(E)\iso(\medtprod^pE^*)\tprod(\medtprod^q E)=\medtprod^p_q(E^*,E)\]
and
\[T(E)\iso(\medtprod E^*)\tprod(\medtprod E)=\medtprod(E^*,E)\]
It follows that \(T^p_q(E)\)~is dual to~\(T^q_p(E)\), and \(T(E)\)~is dual to itself and to~\(T(E^*)\).
\end{rmk}

\begin{rmk}
The results in this subsection show that it is possible to develop tensor algebra using multilinear function spaces instead of tensor product spaces (at least in the finite-dimensional case). While it is easier to get started this way, it is messier in the long run.
\end{rmk}

\section*{Chapter~4}
\subsection*{\S~4}
\begin{rmk}
We see from~(4.10) that the skew-symmetric part of a product depends only on the skew-symmetric parts of the factors.
\end{rmk}

\begin{exer}[1]
The map \(\sigma:\medtprod^p E\to\medtprod^p E\) is tensorial. If \(E\)~is finite-dimensional, then \(\sigma\)~is generated by \(\mu(t)\) and~\(C^i_j\), where \(t\)~is the unit tensor.
\end{exer}
\begin{proof}
If \(\alpha\)~is an automorphism of~\(E\) and \(u=\tprods{x}{1}{p}\in\medtprod^p E\), then
\[\sigma T_{\alpha}u=\tprods{\alpha x}{\sigma^{-1}(1)}{\sigma^{-1}(p)}=T_{\alpha}\sigma u\]
so \(\sigma T_{\alpha}=T_{\alpha}\sigma\) and hence \(\sigma\)~is tensorial.

For the second claim, we may assume that \(\sigma=(12\cdots j)\) where \(1<j\). Write \(t=\sum_{\nu}e^{*\nu}\tprod e_{\nu}\) where \(e_{\nu},e^{*\nu}\) is any pair of dual bases in \(E,E^*\). Then
\begin{align*}
C^1_{j+1}(t\tprod u)&=\sum_{\nu}C^1_{j+1}(e^{*\nu}\tprod e_{\nu}\tprod\tprods{x}{1}{p})\\
	&=\sum_{\nu}\sprod{e^{*\nu}}{x_j}e_{\nu}\tprod\tprods{x}{1}{j-1}\tprod\tprods{x}{j+1}{p}\\
	&=x_j\tprod\tprods{x}{1}{j-1}\tprod\tprods{x}{j+1}{p}\\
	&=\sigma u
\end{align*}
so \(C^1_{j+1}\after\mu(t)=\sigma\).
\end{proof}

\subsection*{\S~5}
\begin{rmk}
In the following subsections, an algebra of skew-symmetric tensors is constructed in two ways: first from the top down, by starting with the tensor algebra~\(\medtprod E\) and killing off tensors with equal factors in~\(N(E)\) to form \(\medtprod E/N(E)\); and then from the bottom up, by collecting skew-symmetric tensors in~\(\medtprod E\) to form~\(X(E)\). The two constructions are essentially equivalent by~(4.17), although scalar products between dual algebras are defined slightly differently in~\S~8.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If we make the appropriate identifications, we have
\[\pi(\medtprod^p E)=\medtprod^p E/(N(E)\sect\medtprod^p E)=\medtprod^p E/N^p(E)=X^p(E)\]
so we already know
\[\medtprod E/N(E)=\sum_p \pi(\medtprod^p E)=\sum_p X^p(E)=X(E)\]
\end{rmk}

\begin{rmk}
To see how (4.16)~follows from~(4.13), write \(u=\pi(a)\) and \(v=\pi(b)\) with \(a\in\medtprod^p E\) and \(b\in\medtprod^q E\). By (4.15), (4.17), and~(4.13),
\[uv=\pi a\mult\pi b=\pi(a\tprod b)=\rho\pi_A(a\tprod b)=(-1)^{pq}\rho\pi_A(b\tprod a)=(-1)^{pq}vu\]
\end{rmk}

\subsection*{\S~7}
\begin{rmk}
\(X(E)\)~is not a subalgebra of~\(\medtprod E\), but can be made into an algebra (see problem~1).
\end{rmk}

\begin{rmk}
The inverse of~\(\rho\) is the induced isomorphism \(\overline{\pi_X}:\medtprod E/N(E)\iso X(E)\):
\begin{diagram}[nohug]
\medtprod E			&\rTo^{\pi_X}						&X(E)\\
\dTo<{\pi}			&\ruTo>{\rho^{-1}=\overline{\pi_X}}	&\\
\medtprod E/N(E)	&									&
\end{diagram}
\end{rmk}

\begin{exer}[1]
If we define a product in~\(X(E)\) by
\[u\mult v=\pi_A(u\tprod v)\qquad u,v\in X(E)\]
then \(X(E)\)~becomes an algebra and \(\rho\)~an algebra isomorphism. Moreover,
\[\pi_A u\mult\pi_A v=\pi_A(u\tprod v)\qquad u,v\in\medtprod E\]
\end{exer}
\begin{proof}
Clearly the product makes~\(X(E)\) into an algebra. By (4.17) and~(4.15),
\[\rho(u\mult v)=\rho\pi_A(u\tprod v)=\pi(u\tprod v)=\pi u\mult\pi v=\rho u\mult\rho v\]
for all \(u,v\in X(E)\), so \(\rho\)~is an algebra isomorphism. By~(4.10),
\[\pi_A u\mult\pi_A v=\pi_A(\pi_A u\tprod\pi_A v)=\pi_A(u\tprod v)\]
for all \(u,v\in\medtprod E\).
\end{proof}
\begin{rmk}
The proof also shows that the product is uniquely determined by the requirement that \(\rho\)~be an algebra isomorphism.
\end{rmk}

\section*{Chapter~5}
\subsection*{\S~1}
\begin{exer}[1]
Let \(L_p(E;F)\) denote the space of \(p\)-linear mappings \(E^p\to F\), and \(A_p(E;F)\) the subspace of skew-symmetric mappings. If \(T:L_p(E;F)\to L_p(E;F)\) is linear such that \(T\varphi=\varphi\) for all \(\varphi\in A_p(E;F)\), and \(T(\sigma\varphi)=\sign{\sigma}T\varphi\) for all \(\sigma\in S_p\) and \(\varphi\in L_p(E;F)\), then \(T=A\) (the alternator).
\end{exer}
\begin{proof}
By assumption, \(TA=A\) and
\[(TA)\varphi=T(A\varphi)=\frac{1}{p!}\sum_{\sigma}\sign{\sigma}T(\sigma\varphi)=\frac{1}{p!}\sum_{\sigma}T\varphi=T\varphi\]
so \(TA=T\).
\end{proof}
\begin{rmk}
This result shows that the alternator is the unique ``skew-symmetric'' transformation of \(p\)-linear mappings which fixes skew-symmetric mappings.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
The \(p\)-th exterior power \(\medeprod^p E\) of~\(E\) is a universal (initial) object in the category of ``vector spaces with skew-symmetric \(p\)-linear maps of~\(E\) into them''. In this category, the objects are skew-symmetric \(p\)-linear maps \(E^p\to F\), and the arrows are linear maps \(F\to G\) which respect the \(p\)-linear maps:
\begin{diagram}
E^p		&\rTo	&G\\
\dTo	&\ruTo	&\\
F		&		&
\end{diagram}
Every object \(E^p\to F\) in this category can be obtained from the exterior product \(\medeprod^p:E^p\to\medeprod^p E\) in a unique way. This is why \(\medeprod^p\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(\medeprod^p E\) satisfy only those relations required to make \(\medeprod^p\) skew-symmetric and \(p\)-linear. By category theoretic abstract nonsense, \(\medeprod^p E\)~is unique up to isomorphism.
\end{rmk}

\begin{rmk}
Geometrically, the \(p\)-vector
\[\medeprod^p(x_1,\ldots,x_p)=\eprods{x}{1}{p}\]
represents the oriented (signed) volume of the \(p\)-dimensional parallelepiped determined by \(x_1,\ldots, x_p\). It can also be thought of as a family of \(p\)-dimensional parallelepipeds with the same orientation and volume.
\end{rmk}

\begin{rmk}
If \(E\)~is \(n\)-dimensional and \(\Delta\ne 0\) is a determinant function in~\(E\), then since \(\medeprod^n\)~is skew-symmetric and \(n\)-linear in~\(E\),
\[\eprods{x}{1}{n}=\Delta(x_1,\ldots,x_n)\mult\eprods{e}{1}{n}\]
where \(\Delta(e_1,\ldots,e_n)=1\). Here \(\eprods{e}{1}{n}\) represents the unit volume under~\(\Delta\). On the other hand, since \(\Delta\)~is skew-symmetric and \(n\)-linear, there is a linear function \(\Delta_{\eprod}:\medeprod^n E\to\Gamma\) with
\[\Delta_{\eprod}(\eprods{x}{1}{n})=\Delta(x_1,\ldots,x_n)\]
Note \(\Delta_{\eprod}\)~is inverse to \(\eprods{e}{1}{n}\mult\iota\), so it is an isomorphism. Also
\[\Delta_{\eprod}(\eprods{x}{1}{n})=\Delta_{\tprod}(\tprods{x}{1}{n})\]
These results show that the universality of determinant functions is a special case of the universality of the exterior product.
\end{rmk}

\subsection*{\S~4}
\begin{rmk}
The projection \(\pi:\medtprod E\to\medtprod E/N(E)\) and the projection \(\pi:\medtprod E\to\medeprod E\) constructed in this subsection are related by the following commutative diagram:
\begin{diagram}[nohug]
\medtprod E		&\rTo^{\pi}	&\medeprod E\\
\dTo<{\pi}		&\ruTo>{f}	&\\
\medtprod E/N(E)&			&
\end{diagram}
\end{rmk}

\begin{rmk}
If \(u\in\medeprod^p E\) and \(v\in\medeprod^q E\) with \(pq\)~even, then
\begin{align*}
(u+v)^k&=\frac{1}{k!}\underbrace{(u+v)\eprod\cdots\eprod(u+v)}_k\\
	&=\frac{1}{k!}\sum_{i+j=k}\frac{k!}{i!(k-i)!}\underbrace{u\eprod\cdots\eprod u}_i\eprod\underbrace{v\eprod\cdots\eprod v}_j\\
	&=\sum_{i+j=k}\frac{1}{i!j!}(i!\,u^i)\eprod(j!\,v^j)\\
	&=\sum_{i+j=k}u^i\eprod v^j
\end{align*}
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
The exterior algebra \(\medeprod E\) is a universal (initial) object in the category of ``associative unital algebras with linear maps of~\(E\) into them whose squares are zero''. In this category, the objects are linear maps \(\varphi:E\to A\) for associative unital algebras~\(A\) with \(\varphi^2=0\), and the arrows are algebra homomorphisms \(A\to B\) which preserve the units and respect the linear maps:
\begin{diagram}
E	&\rTo	&B\\
\dTo&\ruTo	&\\
A	&		&
\end{diagram}
Every object in this category can be obtained from the exterior algebra \(\medeprod E\) in a unique way. This is why \(\medeprod E\)~is said to satisfy the ``universal property''. This is only possible because the elements of~\(\medeprod E\) satisfy only those properties that are required to make \(\medeprod E\) into an associative unital algebra containing~\(E\) with skew-symmetric multiplication in~\(E\). By category theoretic abstract nonsense, \(\medeprod E\)~is unique up to isomorphism.
\end{rmk}

\subsection*{\S~7}
\begin{rmk}
If \(E\)~is \(n\)-dimensional, then \(\medeprod^p E=0\) for \(p>n\). Geometrically, this just reflects the fact that you can't fit a \(p\)-dimensional parallelepiped into the \(n\)-dimensional space.
\end{rmk}

\begin{rmk}
If \(E\)~is a Euclidean space and \(x_1,\ldots,x_p\in E\), then
\[\norm{\eprods{x}{1}{p}}=\sqrt{\det\iprod{x_i}{x_j}}\]
is the volume of the \(p\)-dimensional parallelepiped determined by \(x_1,\ldots,x_p\).
\end{rmk}

\begin{exer}[1]
Let \(E,E^*\) be a pair of finite-dimensional dual vector spaces and \(A\subseteq L(E^*;E)\) be the subspace of linear maps \(\varphi:E^*\to E\) for which \(\varphi^*=-\varphi\). If \(\varphi:E\times E\to A\) is the bilinear map defined by
\[\varphi_{a,b}(x^*)=\sprod{x^*}{a}b-\sprod{x^*}{b}a\]
then \((A,\varphi)\) is a second exterior power of~\(E\).
\end{exer}
\begin{proof}
Identifying \(E\tprod E\) with \(L(E^*;E)\) under the natural isomorphism
\[a\tprod b\mapsto\bigl(x^*\mapsto\sprod{x^*}{a}b\,\bigr)\]
\(A\)~is just the subspace of skew-symmetric tensors in~\(E\tprod E\), and
\[\varphi_{a,b}=a\tprod b-b\tprod a\]
is (up to a factor of~\(2\)) just the skew-symmetric part of \(a\tprod b\).
\end{proof}

\begin{exer}[4 - Lagrange identity]
\[
\renewcommand{\arraystretch}{1.5}
\begin{vmatrix}
\sum_{\nu=1}^n\xi_1^{\nu}\eta^1_{\nu}&\cdots&\sum_{\nu=1}^n\xi_1^{\nu}\eta^p_{\nu}\\
\vdots&\ddots&\vdots\\
\sum_{\nu=1}^n\xi_p^{\nu}\eta^1_{\nu}&\cdots&\sum_{\nu=1}^n\xi_p^{\nu}\eta^p_{\nu}
\end{vmatrix}
=
\sum_{<}
\begin{vmatrix}
\xi_1^{\nu_1}&\cdots&\xi_1^{\nu_p}\\
\vdots&\ddots&\vdots\\
\xi_p^{\nu_1}&\cdots&\xi_p^{\nu_p}
\end{vmatrix}
\begin{vmatrix}
\eta^1_{\nu_1}&\cdots&\eta^1_{\nu_p}\\
\vdots&\ddots&\vdots\\
\eta^p_{\nu_1}&\cdots&\eta^p_{\nu_p}
\end{vmatrix}
\tag{1}\]
\end{exer}
\begin{proof}
Let \(L\)~denote the left-hand side of~(1). Let \(E,E^*\) be a pair of dual vector spaces with dual bases \(e_1,\ldots,e_n\) and \(e^{*1},\ldots,e^{*n}\). Define
\[x_i=\sum_{\nu=1}^n\xi_i^{\nu}e_{\nu}\qquad\text{and}\qquad y^{*j}=\sum_{\mu=1}^n\eta^j_{\mu}e^{*\mu}\qquad(i,j=1,\ldots,p)\]
Since
\[\sprod{y^{*j}}{x_i}=\sum_{\nu=1}^n\xi_i^{\nu}\eta^j_{\nu}\]
it follows from (5.10) and~(5.13) that
\begin{align*}
L&=\sprod{y^{*1}\eprod\cdots\eprod y^{*p}}{x_1\eprod\cdots\eprod x_p}\\
	&=\sum_{(\nu),(\mu)}\xi_1^{\nu_1}\cdots\,\xi_p^{\nu_p}\eta^1_{\mu_1}\cdots\,\eta^p_{\mu_p}\sprod{e^{*\mu_1}\eprod\cdots\eprod e^{*\mu_p}}{e_{\nu_1}\eprod\cdots\eprod e_{\nu_p}}\\
	&=\sum_{(\nu),(\mu)}\xi_1^{\nu_1}\cdots\,\xi_p^{\nu_p}\eta^1_{\mu_1}\cdots\,\eta^p_{\mu_p}\det(\delta_{\nu_i}^{\mu_j})
\end{align*}
Now
\[\det(\delta_{\nu_i}^{\mu_j})=\sum_{\sigma}\sign{\sigma}\delta_{\nu_{\sigma(1)}}^{\mu_1}\cdots\,\delta_{\nu_{\sigma(p)}}^{\mu_p}\]
so \(\det(\delta_{\nu_i}^{\mu_j})=\sign{\tau}\) if the~\(\nu_i\) are distinct and \(\mu_i=\nu_{\tau(i)}\) for some permutation~\(\tau\), otherwise \(\det(\delta_{\nu_i}^{\mu_j})=0\). The~\(\nu_i\) are distinct if and only if \(\nu_{\sigma(1)}<\cdots<\nu_{\sigma(p)}\) for some permutation~\(\sigma\), so we have
\begin{align*}
L&=\sum_{\nu_1<\cdots<\nu_p}\sum_{\sigma}\xi_1^{\nu_{\sigma(1)}}\cdots\,\xi_p^{\nu_{\sigma(p)}}\sum_{\tau}\sign{\tau}\eta^1_{\nu_{\tau\sigma(1)}}\cdots\,\eta^p_{\nu_{\tau\sigma(p)}}\\
	&=\sum_{\nu_1<\cdots<\nu_p}\sum_{\sigma}\sign{\sigma}\xi_1^{\nu_{\sigma(1)}}\cdots\,\xi_p^{\nu_{\sigma(p)}}\sum_{\tau}\sign{\tau\sigma}\eta^1_{\nu_{\tau\sigma(1)}}\cdots\,\eta^p_{\nu_{\tau\sigma(p)}}
\end{align*}
Since \(\tau\sigma\)~runs over all permutations as~\(\tau\) does, we have
\[L=\sum_{<}\Bigl(\sum_{\sigma}\sign{\sigma}\xi_1^{\nu_{\sigma(1)}}\cdots\,\xi_p^{\nu_{\sigma(p)}}\Bigr)\Bigl(\sum_{\varrho}\sign{\varrho}\eta^1_{\nu_{\varrho(1)}}\cdots\,\eta^p_{\nu_{\varrho(p)}}\Bigr)\]
which is just~(1).
\end{proof}

\subsection*{\S~8}
\begin{rmk}
Let \(E\)~be an \(n\)-dimensional space and \(\Delta\) a determinant function in~\(E\) with \(\Delta(e_1,\ldots,e_n)=1\). If \(\varphi:E\to E\) is linear, then
\begin{align*}
\varphi_{\eprod}(\eprods{x}{1}{n})&=\eprods{\varphi x}{1}{n}\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_n)\mult\eprods{e}{1}{n}\\
	&=\det\varphi\mult\Delta(x_1,\ldots,x_n)\mult\eprods{e}{1}{n}\\
	&=\det\varphi\mult\eprods{x}{1}{n}
\end{align*}
for all \(x_1,\ldots,x_n\in E\). Therefore
\[\varphi_{\eprod}^{(n)}=\det\varphi\mult\iota\]
where \(\varphi_{\eprod}^{(n)}\)~is the restriction of~\(\varphi_{\eprod}\) to~\(\medeprod^n E\). Equivalently
\[\tr\varphi_{\eprod}^{(n)}=\det\varphi\]
\end{rmk}

\subsection*{\S~10}
\begin{rmk}
Let \(E\)~be an \(n\)-dimensional space and \(\Delta\) a determinant function in~\(E\) with \(\Delta(e_1,\ldots,e_n)=1\). If \(\varphi:E\to E\) is linear, then
\begin{align*}
\theta_{\eprod}(\varphi)(\eprods{x}{1}{n})&=\sum_{i=1}^n x_1\eprod\cdots\eprod\varphi x_i\eprod\cdots\eprod x_n\\
	&=\sum_{i=1}^n\Delta(x_1,\ldots,\varphi x_i,\ldots,x_n)\mult\eprods{e}{1}{n}\\
	&=\tr\varphi\mult\Delta(x_1,\ldots,x_n)\mult\eprods{e}{1}{n}\\
	&=\tr\varphi\mult\eprods{x}{1}{n}
\end{align*}
for all \(x_1,\ldots,x_n\in E\). Therefore
\[\theta_{\eprod}^{(n)}(\varphi)=\tr\varphi\mult\iota\]
where \(\theta_{\eprod}^{(n)}(\varphi)\)~is the restriction of~\(\theta_{\eprod}(\varphi)\) to~\(\medeprod^n E\). Equivalently
\[\tr\theta_{\eprod}^{(n)}(\varphi)=\tr\varphi\]
This is also equivalent to the result in subsection~3.6 above, by the following commutative diagram:
\begin{diagram}[nohug]
				&										&E^n				&						&\\
				&										&\dTo<{\tprod^n}	&\rdTo>{\Delta}			&\\
\medtprod^n E	&\rTo^{\theta_{\circled{n}}(\varphi)}	&\medtprod^n E		&\rTo>{\Delta_{\tprod}}	&\Gamma\\
\dTo<{\pi}		&										&\dTo<{\pi}			&\ruTo>{\Delta_{\eprod}}&\\
\medeprod^n E	&\rTo_{\theta_{\eprod}^{(n)}(\varphi)}	&\medeprod^n E		&				&
\end{diagram}
\end{rmk}

\subsection*{\S~11}
\begin{rmk}
If \(\omega=\iota_{\eprod}\) and \(\varphi:E\to E\) is linear, then \(\Omega(\varphi)=\theta_{\eprod}(\varphi)\). This shows that the construction in \S~10 is just a special case of the construction in this subsection.
\end{rmk}

\subsection*{\S~12}
\begin{exer}[8]
If \(\psi:E\to E\) is a linear transformation with \(\dim E=n\), then there exist linear transformations \(\psi_i^{(n)}:\medeprod^n E\to\medeprod^n E\) unique such that
\[(\psi-\lambda\iota)_{\eprod}^{(n)}=\sum_{i=0}^n\psi_i^{(n)}\lambda^{n-i}\qquad\lambda\in\Gamma\]
Moreover, \(\tr\psi_i^{(n)}=\alpha_i\) where \(\alpha_i\)~is the \(i\)-th characteristic coefficient of~\(\psi\).
\end{exer}
\begin{proof}
By an argument similar to that given in subsection~4.19 of~\cite{greub1},
\[(\psi-\lambda\iota)_{\eprod}^{(n)}=\sum_{i=0}^n\tilde{S}_i\]
where
\[\tilde{S}_i(\eprods{x}{1}{n})=\frac{(-\lambda)^{n-i}}{i!(n-i)!}\sum_{\sigma}\sign{\sigma}\eprods{\varphi x}{\sigma(1)}{\sigma(i)}\eprod\eprods{x}{\sigma(i+1)}{\sigma(n)}\]
This uniquely determines \(\psi_i^{(n)}\) with \(\tilde{S}_i=\psi_i^{(n)}\lambda^{n-i}\). By (4.47) and~(4.48) of~\cite{greub1},
\[\Delta_{\eprod}\after\tilde{S}_i=\alpha_i\lambda^{n-i}\Delta_{\eprod}=\Delta_{\eprod}\after(\alpha_i\lambda^{n-i}\iota)\]
where \(\Delta\ne 0\) is a determinant function in~\(E\). Since \(\Delta_{\eprod}\)~is injective, this implies
\[\psi_i^{(n)}\lambda^{n-i}=\tilde{S}_i=\alpha_i\lambda^{n-i}\iota\]
so \(\psi_i^{(n)}=\alpha_i\iota\) and \(\tr\psi_i^{(n)}=\alpha_i\).
\end{proof}
\begin{rmk}
We have \(\psi_0^{(n)}=(-1)^n\iota\), \(\psi_{1}^{(n)}=(-1)^{n-1}\theta_{\eprod}^{(n)}(\psi)\), and \(\psi_n^{(n)}=\psi_{\eprod}^{(n)}\).
\end{rmk}

\subsection*{\S~14}
\begin{exer}[1]
If \(u^*\in\medeprod^p E^*\) and \(i(a)u^*=0\) for all \(a\in\medeprod^k E\) for some \(k\le p\), then \(u^*=0\).
\end{exer}
\begin{proof}
We claim \(\sprod{u^*}{v}=0\) for all \(v\in\medeprod^p E\), from which it follows that \(u^*=0\). Since \(k\le p\), we may write
\[v=\sum_{\nu}a_{\nu}\eprod v_{\nu}\qquad a_{\nu}\in\medeprod^k E,\ v_{\nu}\in\medeprod^{p-k}E\]
Then
\[\sprod{u^*}{v}=\sum_{\nu}\sprod{u^*}{a_{\nu}\eprod v_{\nu}}=\sum_{\nu}\sprod{i(a_{\nu})u^*}{v_{\nu}}=0\qedhere\]
\end{proof}
\begin{rmk}
This result generalizes Proposition~5.14.2.
\end{rmk}

\begin{exer}[2]
Let \(E,E^*\) be a pair of dual \(n\)-dimensional vector spaces and \(e_{\nu},e^{*\nu}\) a pair of dual bases. If \(\varphi:E\to E\) is linear, then
\begin{align*}
\theta_{\eprod}(\varphi)&=\sum_{\nu}\mu(\varphi e_{\nu})i(e^{*\nu})\tag{1}\\
\theta^{\eprod}(\varphi)&=\sum_{\nu}\mu(e^{*\nu})i(\varphi e_{\nu})\tag{2}
\end{align*}
\end{exer}
\begin{proof}
By Corollary~II to Proposition~5.14.1,
\[i(e^{*\nu})(\eprods{x}{1}{p})=\sum_{\mu=1}^p(-1)^{\mu-1}\sprod{e^{*\nu}}{x_{\mu}}x_1\eprod\cdots\eprod\delete{x_{\mu}}\eprod\cdots\eprod x_p\]
It follows that
\[\mu(\varphi e_{\nu})i(e^{*\nu})(\eprods{x}{1}{p})=\sum_{\mu=1}^p x_1\eprod\cdots\eprod\varphi\bigl(\sprod{e^{*\nu}}{x_{\mu}}e_{\nu}\bigr)\eprod\cdots\eprod x_p\]
and so
\begin{align*}
\sum_{\nu=1}^n\mu(\varphi e_{\nu})i(e^{*\nu})(\eprods{x}{1}{p})&=\sum_{\mu=1}^p x_1\eprod\cdots\eprod\varphi\bigl(\sum_{\nu=1}^n\sprod{e^{*\nu}}{x_{\mu}}e_{\nu}\bigr)\eprod\cdots\eprod x_p\\
	&=\sum_{\mu=1}^p x_1\eprod\cdots\eprod\varphi x_{\mu}\eprod\cdots\eprod x_p\\
	&=\theta_{\eprod}(\varphi)(\eprods{x}{1}{p})
\end{align*}
This establishes~(1). Now (2)~follows from~(1) by duality since
\[\theta^{\eprod}(\varphi)=\theta_{\eprod}(\varphi)^*=\sum_{\nu=1}^n i(e^{*\nu})^*\mu(\varphi e_{\nu})^*=\sum_{\nu=1}^n\mu(e^{*\nu})i(\varphi e_{\nu})\qedhere\]
\end{proof}

\begin{exer}[3]
Let \(E,E^*\) be a pair of dual \(n\)-dimensional vector spaces and \(e_{\nu},e^{*\nu}\) a pair of dual bases.
\begin{enumerate}
\item[(a)] For \(x\in E\) and \(x^*\in E^*\),
\[i(x^*)\mu(x)+\mu(x)i(x^*)=\sprod{x^*}{x}\iota\]
\item[(b)] For \(u\in\medeprod^p E\),
\[\sum_{\nu}\mu(e_{\nu})i(e^{*\nu})u=pu\]
\item[(c)] For \(u\in\medeprod^p E\),
\[\sum_{\nu}i(e^{*\nu})\mu(e_{\nu})u=(n-p)u\]
\end{enumerate}
\end{exer}
\begin{proof}
For~(a), dualize Corollary~I to Proposition~5.14.1; for~(b), apply problem~2 with \(\varphi=\iota\); for~(c), apply (a) and~(b) to obtain
\[\sum_{\nu=1}^n i(e^{*\nu})\mu(e_{\nu})u=\sum_{\nu=1}^n\sprod{e^{*\nu}}{e_{\nu}}u-\sum_{\nu=1}^n\mu(e_{\nu})i(e^{*\nu})u=nu-pu\qedhere\]
\end{proof}

\begin{exer}[6]
If \(a\in\medeprod^p E\) and \(p\le q\), then
\[i(a)(\eprods{x^*}{1}{q})=\sum_{\nu_1<\cdots<\nu_p}(-1)^{\sum_{i=1}^p(\nu_i-i)}\sprod{\eprods{x^*}{\nu_1}{\nu_p}}{a}\eprods{x^*}{\nu_{p+1}}{\nu_q}\]
where \((\nu_{p+1},\ldots,\nu_q)\) is the ordered tuple complementary to \((\nu_1,\ldots,\nu_p)\).
\end{exer}
\begin{proof}
By induction on~\(p\). For \(p=0\) the result is trivial, and for \(p=1\) it is just Corollary~II to Proposition~5.14.1. We illustrate the induction step for \(p=2\), where we may assume that \(a=a_1\eprod a_2\) and write \(u^*=\eprods{x^*}{1}{q}\):
\begin{align*}
i(a)u^*&=i(a_2)i(a_1)(\eprods{x^*}{1}{q})\\
	&=i(a_2)\sum_{\nu_1}(-1)^{\nu_1-1}\sprod{x^*_{\nu_1}}{a_1}x^*_1\eprod\cdots\eprod\delete{x^*_{\nu_1}}\eprod\cdots\eprod x^*_q\\
	&=\ \ \sum_{\nu_1}(-1)^{\nu_1-1}\sprod{x^*_{\nu_1}}{a_1}i(a_2)(x^*_1\eprod\cdots\eprod\delete{x^*_{\nu_1}}\eprod\cdots\eprod x^*_q)\\
	&=\sum_{\nu_1<\nu_2}(-1)^{(\nu_1-1)+(\nu_2-2)}C_{\nu_1,\nu_2} x^*_1\eprod\cdots\eprod\delete{x^*_{\nu_1}}\eprod\cdots\eprod\delete{x^*_{\nu_2}}\eprod\cdots\eprod x^*_q
\end{align*}
where
\[C_{\nu_1,\nu_2}=\sum_{\sigma}\sign{\sigma}\sprod{x^*_{\nu_{\sigma(1)}}}{a_1}\sprod{x^*_{\nu_{\sigma(2)}}}{a_2}=\det(\sprod{x^*_{\nu_i}}{a_j})=\sprod{x^*_{\nu_1}\eprod x^*_{\nu_2}}{a_1\eprod a_2}\qedhere\]
\end{proof}

\subsection*{\S~29}
\begin{rmk}
Recall that if \(E,E^*\) is a pair of finite-dimensional dual vector spaces, then there is a natural isomorphism \(\alpha:\medtprod^p E^*\to T^p(E)\).\footnote{See subsection~3.20.} Since
\[\sprod{x^{*1}}{x_{\sigma(1)}}\cdots\sprod{x^{*p}}{x_{\sigma(p)}}=\sprod{x^{*\sigma^{-1}(1)}}{x_1}\cdots\sprod{x^{*\sigma^{-1}(p)}}{x_p}\]
for all \(x_i\in E\), \(x^{*i}\in E^*\), and \(\sigma\in S_p\), the following diagram commutes:
\begin{diagram}
\medtprod^p E^*	&\rTo^{\sigma}	&\medtprod^p E^*\\
\dTo<{\alpha}	&				&\dTo>{\alpha}\\
T^p(E)			&\rTo_{\sigma}	&T^p(E)
\end{diagram}
It follows that the alternator diagram also commutes:
\begin{diagram}
\medtprod^p E^*	&\rTo^{\pi^A}	&\medtprod^p E^*\\
\dTo<{\alpha}	&				&\dTo>{\alpha}\\
T^p(E)			&\rTo_{A}		&T^p(E)
\end{diagram}
Therefore (5.81)~follows from~(4.11).
\end{rmk}

\subsection*{\S~32}
\begin{rmk}
In this subsection, the substitution operator~\(i_A(h)\) should be redefined on~\(T^p(E)\) as
\[i_A(h)=\frac{1}{p}\sum_{\nu=1}^p(-1)^{\nu-1}i_{\nu}(h)\]
Then for \(\Phi\in A^p(E)\),
\begin{align*}
p\mult(i_A(h)\Phi)(x_1,\ldots,x_{p-1})&=\sum_{\nu=1}^p(-1)^{\nu-1}(i_{\nu}(h)\Phi)(x_1,\ldots,x_{p-1})\\
	&=\sum_{\nu=1}^p(-1)^{\nu-1}\Phi(x_1,\ldots,x_{\nu-1},h,x_{\nu+1},\ldots,x_{p-1})\\
	&=\sum_{\nu=1}^p\Phi(h,x_1,\ldots,x_{p-1})\\
	&=p\mult\Phi(h,x_1,\ldots,x_{p-1})\\
	&=p\mult(i_1(h)\Phi)(x_1,\ldots,x_{p-1})
\end{align*}
so
\[i_A(h)\Phi=i_1(h)\Phi\]
as desired. This redefinition is required to make~\(i_A(h)\) an antiderivation in the algebra of skew-symmetric multilinear functions (5.32.1), although under the redefinition \(i_A(h)\)~is no longer an antiderivation in the algebra of multilinear functions.\footnote{See subsection~3.19.}
\end{rmk}

\subsection*{\S~33}
\begin{rmk}
The algebra isomorphism \(\beta\after\eta:\medeprod E^*\to A^{\adot}(E)\) satisfies
\[\eprods{f^*}{1}{p}\mapsto\eprods{f^*}{1}{p}=p!\,A(\fprods{f^*}{1}{p})\qquad f^*_i\in E^*\]
The product on the left is the exterior product and the product on the right is the Grassmann product, where \(f^*_i\)~has been identified with \(\sprod{f^*_i}{-}\).
\end{rmk}

\subsection*{\S~34}
\begin{rmk}
We see that the algebra isomorphisms \(\medeprod E^*\iso A^{\adot}(E)\) and \(\medeprod E\iso A_{\adot}(E)\) preserve scalar products.
\end{rmk}

% References
\newpage
\begin{thebibliography}{0}
\bibitem{greub1} Greub, W. \textit{Linear Algebra}, 4th~ed. Springer, 1975.
\bibitem{greub2} Greub,W. \textit{Multilinear Algebra}, 2nd~ed. Springer, 1978.
\end{thebibliography}
\end{document}