% Notes and exercises from Linear Algebra by Greub
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier,diagrams,stmaryrd}

\newcommand{\R}{\mathbb{R}}

\newcommand{\from}{\leftarrow}
\newcommand{\iso}{\cong}
\renewcommand{\equiv}{\sim}

\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\tr}{tr}

\newcommand{\sect}{\cap}
\newcommand{\after}{\circ}
\newcommand{\dsum}{\oplus}
\newcommand{\mult}{\cdot}

\newcommand{\delete}{\widehat}
\newcommand{\gen}[1]{\langle#1\rangle}
\newcommand{\pair}[2]{\langle#1,#2\rangle}
\newcommand{\sprod}[2]{\langle#1,#2\rangle}
\newcommand{\oc}[1]{#1^{\perp}}
\newcommand{\occ}[1]{#1^{\perp\perp}}
\newcommand{\opp}[1]{#1^{\mathrm{opp}}}

\newarrow{Dashto}{}{dash}{}{dash}>

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

% Meta
\title{Notes and exercises from\\\textit{Linear Algebra}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

\section*{Introduction}
This document contains notes and exercises from~\cite{greub}. Unless otherwise stated, \(\Gamma\)~denotes a field of characteristic~\(0\).

\section*{Chapter~I}
\subsection*{\S~1}
\begin{rmk}
The free vector space~\(C(X)\) is intuitively the space of all ``formal linear combinations'' of \(x\in X\).
\end{rmk}

\subsection*{\S~2}
\begin{exer}[5 - Universal property of~\(C(X)\)]
Let \(X\)~be a set and \(C(X)\)~the free vector space on~\(X\) (subsection~1.7). Recall
\[C(X)=\{\,f:X\to\Gamma\mid f(x)=0\text{ for all but finitely many }x\in X\,\}\]
The inclusion map \(i_X:X\to C(X)\) is defined by \(a\mapsto f_a\) where \(f_a\)~is the ``characteristic function'' of~\(a\): \(f_a(a)=1\) and \(f_a(x)=0\) for all \(x\ne a\). For \(f\in C(X)\), \(f=\sum_{a\in X}f(a)f_a\).
\begin{enumerate}
\item[(i)] If \(F\)~is a vector space and \(f:X\to F\), there is a unique \emph{linear} \(\varphi:C(X)\to F\) ``extending~\(f\)'' in the sense that \(\varphi\after i_X=f\):
\begin{diagram}[nohug]
X	&\rTo^{i_X}	&C(X)\\
	&\rdTo_f	&\dDashto>{\varphi}\\
	&			&F
\end{diagram}

\item[(ii)] If \(\alpha:X\to Y\), there is a unique \emph{linear} \(\alpha_*:C(X)\to C(Y)\) which makes the following diagram commute:
\begin{diagram}
X			&\rTo^{\alpha}			&Y\\
\dTo<{i_X}	&						&\dTo>{i_Y}\\
C(X)		&\rDashto_{\alpha_*}	&C(Y)
\end{diagram}
If \(\beta:Y\to Z\), then \((\beta\after\alpha)_*=\beta_*\after\alpha_*\).

\item[(iii)] If \(E\)~is a vector space, there is a unique linear map \(\pi_E:C(E)\to E\) such that \(\pi_E\after i_E=\iota_E\) (where \(\iota_E:E\to E\) is the identity map):
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&C(E)\\
	&\rdTo_{\iota_E}	&\dDashto>{\pi_E}\\
	&			&E
\end{diagram}

\item[(iv)] If \(E\) and~\(F\) are vector spaces and \(\varphi:E\to F\), then \(\varphi\)~is linear if and only if \(\pi_F\after\varphi_*=\varphi\after\pi_E\):
\begin{diagram}[nohug]
E			&				&\rTo^{\varphi}		&				&F				&				&\\
			&\rdTo			&					&				&\vLine>{i_F}	&\rdTo			&\\
\dTo<{i_E}	&				&E					&\rTo^{\varphi}	&\HonV			&				&F\\
			&\ruTo>{\pi_E}	&					&				&\dTo			&\ruTo>{\pi_F}	&\\
C(E)		&				&\rTo_{\varphi_*}	&				&C(F)			&				&
\end{diagram}

\item[(v)] Let \(E\)~be a vector space and \(N(E)\)~the subspace of~\(C(E)\) generated by all elements of the form
\[f_{\lambda a+\mu b}-\lambda f_a-\mu f_b\qquad(a,b\in E\text{ and }\lambda,\mu\in\Gamma)\]
Then \(\ker\pi_E=N(E)\).
\end{enumerate}
\end{exer}
\begin{proof}\
\begin{enumerate}
\item[(i)] By Proposition~II, since \(i_X(X)\)~is a basis of~\(C(X)\).

\item[(ii)] By~(i), applied to~\(i_Y\after\alpha\). Note \(\beta_*\after\alpha_*\)~is linear such that
\[(\beta_*\after\alpha_*)\after i_X=i_Z\after(\beta\after\alpha)\]
so \(\beta_*\after\alpha_*=(\beta\after\alpha)_*\) by uniqueness:
\begin{diagram}
X			&\rTo^{\alpha}		&Y			&\rTo^{\beta}	&Z\\
\dTo<{i_X}	&					&\dTo>{i_Y}	&				&\dTo>{i_Z}\\
C(X)		&\rTo_{\alpha_*}	&C(Y)		&\rTo_{\beta_*}	&C(Z)
\end{diagram}

\item[(iii)] By~(i), applied to~\(\iota_E\).

\item[(iv)] If \(\varphi\)~is linear, then \(\varphi\after\pi_E:C(E)\to F\) is linear and extends~\(\varphi\) in the sense that \(\varphi\after\pi_E\after i_E=\varphi\after\iota_E=\varphi\). However, \(\pi_F\after\varphi_*:C(E)\to F\) is also linear and extends~\(\varphi\) since
\[\pi_F\after\varphi_*\after i_E=\pi_F\after i_F\after\varphi=\iota_F\after\varphi=\varphi\]
By uniqueness, these two maps must be equal. Conversely, if these two maps are equal, then \(\varphi\)~is linear since \(\pi_F\after\varphi_*\)~is linear and \(\pi_E\)~is surjective.

\item[(v)] By~(iii),
\begin{align*}
\pi_E(f_{\lambda a+\mu b}-\lambda f_a-\mu f_b)&=\pi_E(f_{\lambda a+\mu b})-\lambda\pi_E(f_a)-\mu\pi_E(f_b)\\
	&=\lambda a+\mu b-\lambda a-\mu b\\
	&=0
\end{align*}
for all \(a,b\in E\) and \(\lambda,\mu\in\Gamma\). It follows that \(N(E)\subseteq\ker\pi_E\) since \(N(E)\)~is the \emph{smallest} subspace containing these elements and \(\ker\pi_E\)~is a subspace.

On the other hand, it follows from the fact that \(N(E)\)~is a subspace that
\[\sum\lambda_i f_{a_i}-f_{\;\sum\lambda_i a_i}\in N(E)\]
for all (finite) linear combinations. Now if \(g=\sum_{a\in E}g(a)f_a\in\ker\pi_E\), then
\[0=\pi_E(g)=\sum_{a\in E}g(a)\pi_E(f_a)=\sum_{a\in E}g(a)a\]
This implies \(f_{\;\sum_{a\in E}g(a)a}=f_0\in N(E)\). But by the above, \(g-f_0\in N(E)\), so \(g\in N(E)\). Therefore also \(\ker\pi_E\subseteq N(E)\).\qedhere
\end{enumerate}
\end{proof}
\begin{rmk}
Note (i)~shows that \(C(X)\)~is a universal (initial) object in the category of ``vector spaces with maps of~\(X\) into them''. In this category, the objects are maps \(X\to F\), for vector spaces~\(F\), and the arrows are \emph{linear} (i.e. structure-preserving) maps \(F\to G\) between the vector spaces which respect the mappings of~\(X\):
\begin{diagram}[nohug]
X	&\rTo	&F\\
	&\rdTo	&\dTo\\
	&		&G
\end{diagram}
By~(i), every object \(X\to F\) in this category can be obtained from the inclusion map \(X\to C(X)\) in a unique way. This is why \(C(X)\) is called ``universal''. This is only possible because \(C(X)\)~is free from any nontrivial relations among the elements of~\(X\), so any relations among the images of those elements in~\(F\) can be obtained starting from~\(C(X)\). This is why \(C(X)\)~is called ``free''. It is immediate from the universal property that \(C(X)\)~is unique up to isomorphism: if \(X\to U\) is also universal, then the composites \(\psi\after\varphi\) and~\(\varphi\after\psi\) of the induced linear maps \(\varphi:C(X)\to U\) and \(\psi:U\to C(X)\) are linear and extend the inclusion maps, so must be the identity maps on \(C(X)\) and~\(U\) by uniqueness; that is, \(\varphi\) and~\(\psi\) are mutually inverse and hence \emph{isomorphisms}. In fact they are also unique by the universal property.

Now (ii)~shows that we have a \emph{functor} from the category of sets into the category of vector spaces, which sends sets \(X\) and~\(Y\) to the vector spaces \(C(X)\) and~\(C(Y)\), and which sends a set map \(\alpha:X\to Y\) to the linear map \(\alpha_*:C(X)\to C(Y)\). The functor preserves the category structure of composites of arrows.

In~(iii), we are ``forgetting'' the linear structure of~\(E\) when forming~\(C(E)\). For example, if \(E=\R^2\), then \(\pair{1}{1}=\pair{1}{0}+\pair{0}{1}\) in~\(E\), but \emph{not} in~\(C(E)\). The ``formal'' linear combination
\[\pair{1}{1}-\pair{1}{0}-\pair{0}{1}\]
is not zero in~\(C(E)\) because the pairs are unrelated elements (symbols) which are \emph{linearly independent}. Note \(\pi_E\)~is surjective (since \(\iota_E\)~is), so \(E\)~is a projection of~\(C(E)\). In~(iv), we see that \(\varphi:E\to F\) is linear if and only if it is a ``projection'' of \(\varphi_*:C(E)\to C(F)\).

In~(v), we see that \(\pi_E\)~just recalls the linear structure of~\(E\) that was forgotten in~\(C(E)\). In particular, \(C(E)/N(E)\iso E\). In other words, if you start with~\(E\), then forget about its linear structure, then recall that linear structure, you just get \(E\)~again.
\end{rmk}

\subsection*{\S~4}
\begin{exer}[11]
Let \(E\)~be a real vector space and \(E_1\)~a vector hyperplane in~\(E\) (that is, a subspace of codimension~\(1\)). Define an equivalence relation on \(E^1=E-E_1\) as follows: for \(x,y\in E^1\), \(x\equiv y\) if the segment
\[x(t)=(1-t)x+ty\qquad(0\le t\le 1)\]
is disjoint from~\(E_1\). Then there are precisely two equivalence classes.
\end{exer}
\begin{proof}
Fix \(e\in E^1\) with \(E=E_1\dsum\gen{e}\) and define \(\alpha:E\to\R\) by \(x-\alpha(x)e\in E_1\) for all \(x\in E\). It is clear that \(\alpha\)~is linear, and \(x\in E_1\) if and only if \(\alpha(x)=0\). For \(x,y\in E^1\), it follows that \(x\equiv y\) if and only if
\[0\ne\alpha(x(t))=\alpha((1-t)x+ty)=(1-t)\alpha(x)+t\alpha(y)\]
for all \(0\le t\le 1\). But this is just equivalent to \(\alpha(x)\alpha(y)>0\).

Now if \(x\in E^1\), then \(\alpha(x)\ne0\), so \(\alpha(x)^2>0\) and \(x\equiv x\). If \(x\equiv y\), then \(\alpha(y)\alpha(x)=\alpha(x)\alpha(y)>0\), so \(y\equiv x\). If also \(y\equiv z\), then \(\alpha(y)\alpha(z)>0\), so \(\alpha(x)\alpha(z)>0\) and \(x\equiv z\). In other words, this is indeed an equivalence relation.

Note there are at least two equivalence classes since \(\alpha(e)=1\) and \(\alpha(-e)=-1\), so \(\alpha(e)\alpha(-e)=-1<0\) and \(e\not\equiv -e\). On the other hand, there are at most two classes since if \(x\in E^1\), then either \(\alpha(x)>0\) and \(x\equiv e\) or \(\alpha(x)<0\) and \(x\equiv -e\).
\end{proof}
\begin{rmk}
This result shows that the hyperplane separates the vector space into two disjoint half-spaces.
\end{rmk}

\section*{Chapter~II}
\subsection*{\S~4}
\begin{rmk}
The direct sum \(E\dsum F\) is a coproduct in the category of vector spaces in the following sense: if \(\varphi:E\to G\) and \(\psi:F\to G\) are linear maps, there is a unique linear map \(\chi:E\dsum F\to G\) such that \(\varphi=\chi\after i_E\) and \(\psi=\chi\after i_F\), where \(i_E\) and~\(i_F\) are the canonical injections:
\begin{diagram}[nohug]
E	&\rTo^{i_E}			&E\dsum F		&\lTo^{i_F}		&F\\
	&\rdTo<{\varphi}	&\dDashto{\chi}	&\ldTo>{\psi}	&\\
	&					&G				&				&
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x+y)=\varphi(x)+\psi(y)\) for \(x\in E\), \(y\in F\). It is the unique linear map ``extending'' both \(\varphi\) and~\(\psi\). This property makes \(E\dsum F\) unique up to a unique isomorphism.

Dually, \(E\dsum F\) is a product in the following sense: if \(\varphi:G\to E\) and \(\psi:G\to F\) are linear maps, there is a unique linear map \(\chi:G\to E\dsum F\) such that \(\varphi=\pi_E\after\chi\) and \(\psi=\pi_F\after\chi\):
\begin{diagram}[nohug]
	&					&G				&				&\\
	&\ldTo<{\varphi}	&\dDashto{\chi}	&\rdTo>{\psi}	&\\
E	&\lTo^{\pi_E}		&E\dsum F		&\rTo^{\pi_F}	&F
\end{diagram}
Indeed, \(\chi\)~is given by \(\chi(x)=\varphi(x)+\psi(x)\), and ``combines'' \(\varphi\) and~\(\psi\). This property also makes \(E\dsum F\) unique up to a unique isomorphism. An infinite direct sum is also a coproduct, but \emph{not} a product, essentially because it has no infinite sums of elements.

In the proof of Proposition~I, \(\sigma\)~is the product map and \(\tau\)~is the coproduct map. If \(\varphi_1:E_1\to F_1\) and \(\varphi_2:E_2\to F_2\) are linear maps, then \(\varphi=\varphi_1\dsum\varphi_2\) is both a coproduct and product map:
\begin{diagram}
E_1				&\pile{\rTo\\\lTo}	&E_1\dsum E_2	&\pile{\lTo\\\rTo}	&E_2\\
\dTo<{\varphi_1}&					&\dTo>{\varphi}	&					&\dTo>{\varphi_2}\\
F_1				&\pile{\rTo\\\lTo}	&F_1\dsum F_2	&\pile{\lTo\\\rTo}	&F_2
\end{diagram}
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Let \(E\)~be a vector space and \((x_{\alpha})_{\alpha\in A}\) be a basis of~\(E\). For each \(x\in E\), write \(x=\sum_{\alpha\in A}f_{\alpha}(x)x_{\alpha}\). Then \(f_{\alpha}\in L(E)\) for each \(\alpha\in A\). The function \(f_{\alpha}\) is called the \emph{\(\alpha\)-th coordinate functional} for the basis.

Coordinate functionals can be used in an alternative proof of Proposition~IV. If \(E_1\)~is a subspace of~\(E\), let \(B_1\)~be a basis of~\(E_1\) and extend it to a basis \(B\) of~\(E\). For each \(x_{\alpha}\in B-B_1\), we have \(f_{\alpha}\in\oc{E_1}\). If \(x\in\occ{E_1}\), then \(f_{\alpha}(x)=\sprod{f_{\alpha}}{x}=0\) for all such \(\alpha\), so \(x\in E_1\). In other words, \(\occ{E_1}\subseteq E_1\).
\end{rmk}

\begin{rmk}
For \(\varphi:E\to F\) a linear map, let \(L(\varphi):L(E)\from L(F)\) be the dual map given by \(L(\varphi)(f)=f\after\varphi\) (2.50). Then \(L\)~linearly embeds \(L(E;F)\) in \(L(L(F);L(E))\), by (2.43) and~(2.44). Also, \(L(\psi\after\varphi)=L(\varphi)\after L(\psi)\) and \(L(\iota_{E})=\iota_{L(E)}\). This shows that \(L\)~is a contravariant functor in the category of vector spaces. This functor preserves exactness of sequences (see~2.29), and finite direct sums, which are just (co)products in the category (see~2.30), among other things.
\end{rmk}

\subsection*{\S~6}
\begin{rmk}
If \(E\)~has finite dimension, then every basis of a dual space~\(E^*\) is a dual basis. Indeed, if \(f_1,\ldots,f_n\) is a basis of~\(E^*\), let \(e_1,\ldots,e_n\) be its dual basis in~\(E\). Then \(\sprod{f_i}{e_j}=\delta_{ij}\) by~(2.62), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\), again by~(2.62).

Alternatively, with \(E^*=L(E)\), let \(f_1^*,\ldots,f_n^*\) be the dual basis of \(f_1,\ldots,f_n\) in \(E^{**}=L(L(E))\), so \(\sprod{f_j^*}{f_i}=\delta_{ij}\) by~(2.62). Let \(e_1,\ldots,e_n\in E\) be defined by \(\sprod{f_j^*}{f}=\sprod{f}{e_j}\) for all \(f\in E^*\) (see \S~5, problem~3). Then \(\sprod{f_i}{e_j}=\sprod{f_j^*}{f_i}=\delta_{ij}\), so \(f_1,\ldots,f_n\) is the dual basis of \(e_1,\ldots,e_n\).

The first proof here uses the symmetry between \(E\) and~\(E^*\), while the second uses the natural isomorphism \(E\iso E^{**}\).
\end{rmk}

\section*{Chapter~III}
\begin{rmk}
Greub's notational choices in this chapter are insane.
\end{rmk}

\subsection*{\S~3}
\begin{rmk}
In subsection~3.13, note \((\alpha^{\mu}_{\nu})=M(\iota;\bar{x}_{\nu},x_{\mu})\) by~(3.22), \((\check{\alpha}^{\mu}_{\nu})=M(\iota;x_{\nu},\bar{x}_{\mu})\) by~(3.23), and \((\beta^{\varrho}_{\sigma})=M(\iota;\bar{x}^{*\varrho},x^{*\sigma})\) by~(3.24). It follows from~(3.4) that
\[\bigl(\beta^{\mu}_{\nu}\bigr)=\bigl(\check{\alpha}^{\mu}_{\nu}\bigr)^*=\bigl(\bigl(\alpha^{\mu}_{\nu}\bigr)^{-1}\bigr)^*\]
In other words, the matrix of the dual basis transformation \(x^{*\nu}\mapsto\bar{x}^{*\nu}\) in~\(E^*\) is the \emph{transpose} of the inverse of the matrix of the basis transformation \(x_{\nu}\mapsto\bar{x}_{\nu}\) in~\(E\), contrary to what the book says. It's easier to remember that the matrix of \(x^{*\nu}\mapsfrom\bar{x}^{*\nu}\) (arrow reversed!) is the transpose of the matrix of \(x_{\nu}\mapsto\bar{x}_{\nu}\).
\end{rmk}

\section*{Chapter~IV}
\subsection*{\S~1}
\begin{rmk} To see why~(4.1) holds, observe by definition of~\(\tau(\sigma\Phi)\) that
\[(\tau(\sigma\Phi))(x_1,\ldots,x_p)=(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})\]
Let \(y_i=x_{\tau(i)}\). Then by definition of \(\sigma\Phi\) and~\((\tau\sigma)\Phi\),
\begin{align*}
(\sigma\Phi)(x_{\tau(1)},\ldots,x_{\tau(p)})&=(\sigma\Phi)(y_1,\ldots,y_p)\\
	&=\Phi(y_{\sigma(1)},\ldots,y_{\sigma(p)})\\
	&=\Phi(x_{\tau(\sigma(1))},\ldots,x_{\tau(\sigma(p))})\\
	&=\Phi(x_{(\tau\sigma)(1)},\ldots,x_{(\tau\sigma)(p)})\\
	&=((\tau\sigma)\Phi)(x_1,\ldots,x_p)
\end{align*}
Therefore \(\tau(\sigma\Phi)=(\tau\sigma)\Phi\).
\end{rmk}

\begin{rmk}
By Proposition~I(iii) and Proposition~II, a determinant function \(\Delta\ne 0\) ``determines'' linear independence in the sense that \(\Delta(x_1,\ldots,x_n)\ne0\) if and only if \(x_1,\ldots,x_n\) are linearly independent. By~(4.8), it follows that \(\det\varphi\) ``determines'' whether a linear transformation~\(\varphi\) preserves linear independence, i.e. whether or not \(\varphi\)~is invertible.
\end{rmk}

\begin{rmk}
We provide an alternative proof of Proposition~IV. First note
\[(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)=\Delta(x_1,\ldots,x,\ldots,x_n)\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{\(\delete{x_j}\)~denotes deletion of \(x_j\)~from the sequence on the left.} Therefore
\[\sum_{j=1}^n(-1)^{j-1}\Delta(x,x_1,\ldots,\delete{x_j},\ldots,x_n)x_j=\Delta(x,x_2,\ldots,x_n)x_1+\cdots+\Delta(x_1,\ldots,x_{n-1},x)x_n\]
Viewing this as a function of \(x_1,\ldots,x_n\) (that is, a set map from \(E^n\to L(E;E)\)), it is obviously multilinear and skew symmetric (by Proposition~I(ii)). Therefore if \(x_1,\ldots,x_n\) are linearly dependent, it is zero (by Proposition~I(iii)). If \(x_1,\ldots,x_n\) are linearly independent (and hence a basis), then viewing it as a function of~\(x\), its value at~\(x_i\) is just \(\Delta(x_1,\ldots,x_n)x_i\) (by Proposition~I(ii)), so it agrees on a basis with \(\Delta(x_1,\ldots,x_n)x\) and hence is equal to it.
\end{rmk}


\subsection*{\S~2}
\begin{rmk}
In subsection~4.6, we want a transformation~\(\psi\) with \(\psi\varphi=(\det\varphi)\iota\). We can choose a basis \(x_1,\ldots,x_n\) in~\(E\) with \(\Delta(x_1,\ldots,x_n)=1\), for which we want
\begin{align*}
(\psi\varphi)x_i=\psi(\varphi x_i)&=(\det\varphi)x_i\\
	&=(\det\varphi)\Delta(x_1,\ldots,x_n)x_i\\
	&=\Delta(\varphi x_1,\ldots,\varphi x_n)x_i
\end{align*}
To obtain this, we can define
\[\psi(x)=\sum_{j=1}^n\Delta(\varphi x_1,\ldots,x,\ldots,\varphi x_n)x_j\]
where \(x\)~is in the \(j\)-th position on the right.\footnote{See the remark on Proposition~IV above.} Then \(\psi\)~obviously satisfies the above properties, by multilinearity and skew symmetry of~\(\Delta\).

To obtain~\(\psi\) in a ``coordinate-free''  manner (without choosing a basis), we observe that the construction on the right is multilinear and skew symmetric in \(x_1,\ldots,x_n\) when viewed as a mapping \(\Phi:E^n\to L(E;E)\). By the universal property of~\(\Delta\) (Proposition~III), there is a unique \(\psi\in L(E;E)\) satisfying the above; this~\(\psi\) is also seen to be independent of the choice of~\(\Delta\).
\end{rmk}

\begin{rmk}
In subsection~4.7, observe that
\[\Delta(x_1,\ldots,x_p,y_1,\ldots,y_q)\]
induces a determinant function on~\(E_2\) when \(x_1,\ldots,x_p\in E\) are fixed, and induces a determinant function on~\(E_1\) when \(y_1,\ldots,y_q\in E\) are fixed. Now let \(a_1,\ldots,a_p\) be a basis of~\(E_1\), so \(a_1,\ldots,a_p,b_1,\ldots,b_q\) is a basis of~\(E\). Then by~(4.8),
\begin{align*}
\det\varphi\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)&=\Delta(\varphi_1 a_1,\ldots,\varphi_1 a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\Delta(a_1,\ldots,a_p,\varphi_2 b_1,\ldots,\varphi_2 b_q)\\
	&=\det\varphi_1\mult\det\varphi_2\mult\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)
\end{align*}
Since \(\Delta(a_1,\ldots,a_p,b_1,\ldots,b_q)\ne0\), it follows that \(\det\varphi=\det\varphi_1\mult\det\varphi_2\). Note this result shows that
\[\det(\varphi_1\dsum\varphi_2)=\det\varphi_1\mult\det\varphi_2\]
\end{rmk}

\begin{exer}[2]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\overline{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\det\varphi=\det\varphi_1\mult\det\overline{\varphi}\]
\end{exer}
\begin{proof}
Let \(e_1,\ldots,e_n\) be a basis of~\(E\) where \(e_1,\ldots,e_p\) is a basis of~\(E_1\). Let \(\Delta\ne 0\) be a determinant function in~\(E\). First observe that
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,\varphi e_{p+1},\ldots,\varphi e_n)\tag{1}\]
is a determinant function in~\(E_1\) and
\[\Delta_2(\,\overline{x_{p+1}},\ldots,\overline{x_n}\,)=\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)\tag{2}\]
is a well-defined determinant function in~\(E/E_1\). Now
\[\det\overline{\varphi}\mult\Delta_2(\,\overline{x_{p+1}},\ldots,\overline{x_n}\,)=\Delta_2(\,\overline{\varphi}\ \overline{x_{p+1}},\ldots,\overline{\varphi}\ \overline{x_n}\,)=\Delta_2(\,\overline{\varphi x_{p+1}},\ldots,\overline{\varphi x_n}\,)\tag{3}\]
It follows from (2) and~(3) that
\[\det\overline{\varphi}\mult\Delta(e_1,\ldots,e_p,x_{p+1},\ldots,x_n)=\Delta(e_1,\ldots,e_p,\varphi x_{p+1},\ldots,\varphi x_n)\tag{4}\]
Now
\begin{align*}
\det\varphi\mult\Delta(e_1,\ldots,e_n)&=\Delta(\varphi e_1,\ldots,\varphi e_n)&&\\
	&=\Delta_1(\varphi_1 e_1,\ldots,\varphi_1 e_p)&&\text{by~(1)}\\
	&=\det\varphi_1\mult\Delta_1(e_1,\ldots,e_p)&&\\
	&=\det\varphi_1\mult\det\overline{\varphi}\mult\Delta(e_1,\ldots,e_n)&&\text{by (1), (4)}
\end{align*}
Since \(\Delta(e_1,\ldots,e_n)\ne 0\), the result follows.
\end{proof}

\subsection*{\S~4}
\begin{rmk}
If \(A\)~is an \(n\times n\) matrix of the form
\[A=\begin{pmatrix}
A_1&\\
*&A_2
\end{pmatrix}\]
where \(A_1\)~is \(p\times p\) and \(A_2\)~is \((n-p)\times(n-p)\), then
\[\det A=\det A_1\mult\det A_2\tag{1}\]
Indeed, let \(E\)~be an \(n\)-dimensional vector space and \(\varphi:E\to E\) be defined by \(M(\varphi;e_1,\ldots,e_n)=A\), so \(\det A=\det\varphi\). Let \(E_1=\gen{e_1,\ldots,e_p}\) and \(E_2=\gen{e_{p+1},\ldots,e_n}\). Then \(E=E_1\dsum E_2\) and \(E_1\)~is stable under~\(\varphi\). If \(\varphi_1:E_1\to E_1\) is the induced map, then \(A_1=M(\varphi_1)\), so \(\det A_1=\det\varphi_1\). Dually, \(E^*=E_1^*\dsum E_2^*\) where \(E_1^*=\gen{e_1^*,\ldots,e_p^*}\) and \(E_2^*=\gen{e_{p+1}^*,\ldots,e_n^*}\), and \(E_2^*\)~is stable under~\(\varphi^*\) since
\[M(\varphi^*;e_1^*,\ldots,e_n^*)=A^*=\begin{pmatrix}
A_1^*&*\\
&A_2^*
\end{pmatrix}\]
If \(\varphi_2^*:E_2^*\to E_2^*\) is the induced map, then \(A_2^*=M(\varphi_2^*)\) and \(\det A_2=\det A_2^*=\det\varphi_2^*\). So we must prove that \(\det\varphi=\det\varphi_1\mult\det\varphi_2^*\).

Let \(\Delta\ne0\) be a determinant function in~\(E\) and \(\Delta^*\)~its dual in~\(E^*\). We claim
\begin{multline*}
\Delta^*(e_1^*,\ldots,e_n^*)\mult\Delta(e_1,\ldots,e_p,\varphi e_{p+1},\ldots,\varphi e_n)\\=\Delta(e_1,\ldots,e_n)\mult\Delta^*(e_1^*,\ldots,e_p^*,\varphi^* e_{p+1}^*,\ldots,\varphi^* e_n^*)\tag{2}
\end{multline*}
Indeed, by~(4.26) the left side of~(2) is a determinant of the form
\[\begin{vmatrix}
J&\\
*&B
\end{vmatrix}\]
where \(J\)~is the \(p\times p\) identity matrix and \(B=(\beta_i^j)\) with \(\beta_i^j=\sprod{e_j^*}{\varphi e_i}=\sprod{\varphi^* e_j^*}{e_i}\). However, since the determinant is multilinear in its rows,\footnote{See subsection~4.9, item~4.} it is equal to
\[\begin{vmatrix}
J&\\
&B
\end{vmatrix}\]
A similar argument shows that the same is true of the right side of~(2). Now
\begin{align*}
\det\varphi&=\det\varphi\mult\Delta(e_1,\ldots,e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\Delta(\varphi_1e_1,\ldots,\varphi_1e_p,\varphi e_{p+1},\ldots,\varphi e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\det\varphi_1\mult\Delta(e_1,\ldots,e_p,\varphi e_{p+1},\ldots,\varphi e_n)\mult\Delta^*(e_1^*,\ldots,e_n^*)&&\\
	&=\det\varphi_1\mult\Delta^*(e_1^*,\ldots,e_p^*,\varphi_2^* e_{p+1}^*,\ldots,\varphi_2^* e_n^*)\mult\Delta(e_1,\ldots,e_n)&\text{by~(2)}\\
	&=\det\varphi_1\mult\det\varphi_2^*\mult\Delta^*(e_1^*,\ldots,e_n^*)\mult\Delta(e_1,\ldots,e_n)\\
	&=\det\varphi_1\mult\det\varphi_2^*
\end{align*}
The same result~(1) holds when \(A\)~has the form
\[A=\begin{pmatrix}
A_1&*\\
&A_2
\end{pmatrix}\]
Indeed, by the above,
\[\det A=\det A^*=\det A_1^*\mult\det A_2^*=\det A_1\mult\det A_2\]
\end{rmk}

\subsection*{\S~5}
\begin{rmk}
Recall that the system (4.39) is equivalent to \(\varphi x=y\) where \(\varphi:\Gamma^n\to\Gamma^n\) is defined by \(M(\varphi)=(\alpha^j_k)=A\), \(x=(\xi^i)\), and \(y=(\eta^j)\). If \(\det A\ne0\), then \(\varphi\)~is invertible and
\[x=\varphi^{-1}y=\frac{1}{\det A}\adj(\varphi)(y)\]
It follows from the analysis of the adjoint matrix in subsection~4.13 that
\[\xi^i=\frac{1}{\det A}\sum_j\cof(\alpha^j_i)\eta^j\]
Moreover, it follows from~(4.38) that \(\sum_j\cof(\alpha^j_i)\eta^j=\det A_i\) where \(A_i\)~is the matrix obtained from~\(A\) by replacing the \(i\)-th row with~\(y\).\footnote{The cofactors of \(A_i\) and~\(A\) along the \(i\)-th row agree since \(A_i\) and~\(A\) agree on the other rows.} Therefore
\[\xi^i=\frac{\det A_i}{\det A}\]
\end{rmk}

\begin{rmk}
In subsection~4.14, \(\det B^j_i=\det S^j_i\) does \emph{not} follow from~(4.38), which only tells us that \(\det B^j_i=\det B^j_i\). However, it follows from~(4.16), or from our remarks in~\S~4 above.
\end{rmk}

\subsection*{\S~6}
\begin{exer}[5]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\) are linear, then
\[\chi(\varphi_1\dsum\varphi_2)=\chi(\varphi_1)\mult\chi(\varphi_2)\]
where \(\chi(\varphi)\)~denotes the characteristic polynomial of~\(\varphi\).
\end{exer}
\begin{proof}
This follows from the result in subsection~4.7 and the fact that
\[\varphi_1\dsum\varphi_2-\lambda\iota=(\varphi_1-\lambda\iota_{E_1})\dsum(\varphi_2-\lambda\iota_{E_2})\qedhere\]
\end{proof}

\begin{exer}[6]
Let \(\varphi:E\to E\) be linear with \(E_1\)~a stable subspace. If \(\varphi_1:E_1\to E_1\) and \(\overline{\varphi}:E/E_1\to E/E_1\) are the induced maps, then
\[\chi(\varphi)=\chi(\varphi_1)\mult\chi(\overline{\varphi})\]
\end{exer}
\begin{proof}
This follows from problem~2 in \S~2, the fact that \(\varphi-\lambda\iota_E\) restricted to~\(E_1\) is just \(\varphi_1-\lambda\iota_{E_1}\), and \(\overline{\varphi-\lambda\iota_E}=\overline{\varphi}-\lambda\iota_{E/E_1}\).
\end{proof}
\begin{rmk}
Taking \(E_1=\ker\varphi\), we have \(\chi(\varphi_1)=\chi(0_{E_1})=(-\lambda)^p\) where \(p=\dim E_1\), so \(\chi(\varphi)=(-\lambda)^p\chi(\overline{\varphi})\).
\end{rmk}

\begin{exer}[7]
A linear map \(\varphi:E\to E\) is nilpotent if and only if \(\chi(\varphi)=(-\lambda)^n\).
\end{exer}
\begin{proof}
If \(\varphi\)~is nilpotent, we proceed by induction on~\(k\) least such that \(\varphi^k=0\). If \(k=1\), the result is trivial. If \(k>1\), let \(E_1=\ker\varphi\) and \(\overline{\varphi}:E/E_1\to E/E_1\) the induced map. Then \(\overline{\varphi}^{k-1}=0\) since
\[\overline{\varphi}^{k-1}(\overline{x})=\overline{\varphi^{k-1}}(\overline{x})=\overline{\varphi^{k-1}(x)}=0\]
as \(\varphi^{k-1}(x)\in E_1\). By the induction hypothesis, \(\chi(\overline{\varphi})=(-\lambda)^{n-p}\) where \(p=\dim E_1\), so by the previous problem,
\[\chi(\varphi)=(-\lambda)^p(-\lambda)^{n-p}=(-\lambda)^n\]
Conversely, if \(\varphi\ne0\) and \(\chi(\varphi)=(-\lambda)^n\), then the constant term \(\det\varphi=0\), so \(p>0\) and by the previous problem \((-\lambda)^n=(-\lambda)^p\chi(\overline{\varphi})\), which implies \(\chi(\overline{\varphi})=(-\lambda)^{n-p}\). By induction, \(\overline{\varphi}\)~is nilpotent. If \(\overline{\varphi}^k=0\), then \(\varphi^{k+1}=0\), so \(\varphi\)~is nilpotent.
\end{proof}

\subsection*{\S~7}
\begin{exer}[12]
If \(\varphi_1:E_1\to E_1\) and \(\varphi_2:E_2\to E_2\), then
\[\tr(\varphi_1\dsum\varphi_2)=\tr\varphi_1+\tr\varphi_2\]
\end{exer}
\begin{proof}
Immediate since
\[M(\varphi_1\dsum\varphi_2)=\begin{pmatrix}
M(\varphi_1)&\\
&M(\varphi_2)
\end{pmatrix}\qedhere\]
\end{proof}

\subsection*{\S~8}
\begin{rmk}
In~(4.68), if instead we define
\[\Delta_1(x_1,\ldots,x_p)=\Delta(x_1,\ldots,x_p,e_{p+1},\ldots,e_n)\]
then \(\Delta_1\)~represents the original orientation in~\(E_1\). Indeed, in this case
\[\Delta_1(e_1,\ldots,e_p)=\Delta(e_1,\ldots,e_p,e_{p+1},\ldots,e_n)=\Delta_2(e_{p+1},\ldots,e_n)>0\]
\end{rmk}

\section*{Chapter~V}
\subsection*{\S~1}
In the problems below, \(E\)~is a finite-dimensional vector space.
\begin{exer}[12]
The mapping
\[\Phi:A(E;E)\to\opp{A(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
\(\Phi\)~is a homomorphism by the results of chapter~II, subsection~2.25, and is an isomorphism since \(\varphi^{**}=\varphi\).
\end{proof}

\begin{exer}[16]
Every algebra automorphism \(\Phi:A(E;E)\to A(E;E)\) is an \emph{inner} automorphism; that is, there exists \(\alpha\in GL(E)\) such that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{exer}
\begin{proof}
First, observe that every basis~\((e_i)\) of~\(E\) induces a basis~\((\varphi_{ij})\) of~\(A(E;E)\) defined by \(\varphi_{ij}(e_k)=\delta_{jk}e_i\). This basis satisfies
\[\varphi_{ij}\varphi_{lk}=\delta_{jl}\varphi_{ik}\qquad\text{and}\qquad\sum_i\varphi_{ii}=\iota\tag{1}\]
Conversely, every basis satisfying these properties is induced by a basis of~\(E\) in this manner (see problem~14). Moreover, any two of these bases are conjugate to each other via the change of basis transformation between their inducing bases of~\(E\) (see problem~15).

Now fix \((e_i)\) and~\((\varphi_{ij})\) as above. Since \(\Phi\)~is an automorphism, \((\Phi(\varphi_{ij}))\)~is also a basis of~\(A(E;E)\) which satisfies~(1), so there is \(\alpha\in GL(E)\) with \(\Phi(\varphi_{ij})=\alpha\varphi_{ij}\alpha^{-1}\) for all \(i,j\). It follows that \(\Phi(\varphi)=\alpha\varphi\alpha^{-1}\) for all \(\varphi\in A(E;E)\).
\end{proof}
\begin{rmk}
The result is also true for any nonzero endomorphism~\(\Phi\), since \(A(E;E)\)~is simple (see subsection~5.12).
\end{rmk}

\section*{Chapter~VI}
\subsection*{\S~1}
\begin{exer}[6]
Let \(E,E^*\) and \(F,F^*\) be pairs of dual \(G\)-graded vector spaces and let \(\varphi:E\to F\) and \(\varphi^*:E^*\from F^*\) be dual linear maps. If \(\varphi\)~is homogeneous of degree~\(k\), then \(\varphi^*\)~is homogeneous of degree~\(-k\).
\end{exer}
\begin{proof}
We have direct sum decompositions
\[E=\sum_{m\in G}E_m\qquad E^*=\sum_{m\in G}E^{*m}\]
and
\[F=\sum_{n\in G}F_n\qquad F^*=\sum_{n\in G}F^{*n}\]
where the pairs \(E_m,E^{*m}\) and \(F_n,F^{*n}\) are dual for all \(m,n\) under the restrictions of the scalar products between \(E,E^*\) and \(F,F^*\), respectively (see subsection~6.5). We also have \(\varphi E_m\subseteq F_{m+k}\) for all~\(m\). We must prove \(\varphi^* F^{*n}\subseteq E^{*n-k}\) for all~\(n\).

Let \(y^*\in F^{*n}\) and \(x\in E\). Write \(x=\sum_m x_m\) where \(x_m\in E_m\). Then
\[\sprod{\varphi^* y^*}{x}=\sprod{y^*}{\varphi x}=\sum_m\sprod{y^*}{\varphi x_m}=\sprod{y^*}{\varphi x_{n-k}}=\sprod{\varphi^* y^*}{x_{n-k}}\]
which implies
\[\sprod{\varphi^* y^*}{x-\pi_{n-k}x}=0\tag{1}\]
where \(\pi_{n-k}:E\to E_{n-k}\) is the canonical projection. Now write \(\varphi^* y^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*m}=0\) for all \(m\ne n-k\). Indeed, for \(m\ne n-k\) and \(x\in E_m\) we have \(\pi_{n-k}x=0\), so by~(1)
\[\sprod{x^{*m}}{x}=\sum_p\sprod{x^{*p}}{x}=\sprod{\varphi^* y^*}{x}=0\]
Therefore \(x^{*m}=0\). It follows that \(\varphi^* y^*=x^{*n-k}\in E^{*n-k}\), as desired.
\end{proof}

\begin{exer}[8]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. If \(F\)~is a \(G\)-graded subspace of~\(E\), then \(\oc{F}\)~is a \(G\)-graded subspace of~\(E^*\) and \(\occ{F}=F\).
\end{exer}
\begin{proof}
We have direct sums \(E=\sum_{m\in G}E_m\) and \(E^*=\sum_{m\in G}E^{*m}\) where the pairs \(E_m,E^{*m}\) are dual under the restrictions of the scalar product between \(E,E^*\) and \(\dim E_m=\dim E^{*m}<\infty\) for all~\(m\). By assumption, \(F=\sum_{m\in G}F\sect E_m\).
We must prove
\[\oc{F}=\sum_{m\in G}\oc{F}\sect E^{*m}\tag{1}\]
Let \(x^*\in\oc{F}\) and write \(x^*=\sum_m x^{*m}\) where \(x^{*m}\in E^{*m}\). We claim \(x^{*n}\in\oc{F}\) for all~\(n\). Indeed, if \(x\in F\), write \(x=\sum_m x_m\) where \(x_m\in F\sect E_m\). Then
\[\sprod{x^{*n}}{x}=\sum_m\sprod{x^{*n}}{x_m}=\sprod{x^{*n}}{x_n}=\sum_m\sprod{x^{*m}}{x_n}=\sprod{x^*}{x_n}=0\]
This establishes~(1). By symmetry, we have
\[\occ{F}=\sum_{m\in G}\occ{F}\sect E_m\tag{2}\]
We claim \(\occ{F}\sect E_n\subseteq F\sect E_n\) for all~\(n\). To prove this, we first show
\[\occ{F}\sect E_n\subseteq(F\sect E_n)^{\perp_n\perp_n}\tag{3}\]
where \(\perp_n\)~is taken relative to the scalar product between \(E_n,E^{*n}\). Indeed, let \(x\in\occ{F}\sect E_n\) and \(x^*\in(F\sect E_n)^{\perp_n}\subseteq E^{*n}\). If \(y\in F\), write \(y=\sum_m y_m\) where \(y_m\in F\sect E_m\). Then
\[\sprod{x^*}{y}=\sum_m\sprod{x^*}{y_m}=\sprod{x^*}{y_n}=0\]
This implies \(x^*\in\oc{F}\), which implies \(\sprod{x^*}{x}=0\), which in turn implies~(3). Now \((F\sect E_n)^{\perp_n\perp_n}=F\sect E_n\) since \(\dim E_n<\infty\), which establishes the claim. Finally, it follows from~(2) that \(\occ{F}=F\).
\end{proof}

\subsection*{\S~2}
\begin{rmk}
If \(E\)~is a finite-dimensional \(G\)-graded vector space and \(\varphi:E\to E\) is linear and homogeneous with \(\deg\varphi\ne 0\), then \(\tr\varphi=0\).
\end{rmk}
\begin{proof}
Write \(E=E_{k_1}\dsum\cdots\dsum E_{k_n}\) with \(k_i\in G\) and \(d_i=\dim E_{k_i}<\infty\). Let \((e_{ij})\)~be a basis of~\(E\) such that for each \(1\le i\le n\), \((e_{ij})\)~is a basis of~\(E_{k_i}\) for \(1\le j\le d_i\). Let \(\Delta\)~be a determinant function in~\(E\) with \(\Delta(e_{ij})=1\). Then
\[\tr\varphi=\sum_{i,j}\Delta(e_{11},\ldots,e_{1d_1},\ldots,\varphi(e_{ij}),\ldots,e_{n1},\ldots,e_{n\,d_n})\]
By assumption, \(\varphi(e_{ij})\in E_{k_l}\) for some \(l\ne i\), so each term in this sum is zero, and hence \(\tr\varphi=0\).
\end{proof}
\noindent As an example, formal differentiation in the space of polynomials of degree at most~\(n\) (graded by the degrees of monomials) is homogeneous of degree~\(-1\), so has zero trace. This is also obvious from its matrix representation with respect to the standard basis.

\begin{exer}[1]
Let \(A\)~be a \(G\)-graded algebra. If \(x\in A\) is an invertible element homogeneous of degree~\(k\), then \(x^{-1}\)~is homogeneous of degree~\(-k\). If \(A\)~is nonzero and positively graded, then \(k=0\).
\end{exer}
\begin{proof}
Write \(A=\sum_{m\in G}A_m\) and \(x^{-1}=\sum_m x_m\) with \(x_m\in A_m\). Then
\[e=xx^{-1}=\sum_m xx_m\]
Since \(\deg e=0\) and \(\deg(xx_m)=m+k\), it follows that \(xx_m=0\) for all \(m\ne-k\). Therefore \(e=xx_{-k}\) and \(x^{-1}=x_{-k}\), so \(x^{-1}\)~is homogeneous of degree~\(-k\).

If \(A\ne 0\), then \(x\ne 0\) and \(x^{-1}\ne 0\), so \(A_k\ne 0\) and \(A_{-k}\ne 0\). If \(A\)~is positively graded, this forces \(k=0\).
\end{proof}

\begin{exer}[4]
Let \(E\)~be a \(G\)-graded vector space. Then the subspace \(A_G(E;E)\) of~\(A(E;E)\) generated by homogeneous linear transformations of~\(E\) forms a \(G\)-graded subalgebra of~\(A(E;E)\).
\end{exer}
\begin{proof}
First observe that \(A_G(E;E)\)~is naturally graded as a vector space by the degrees of the homogeneous transformations (see problem~3).
If \(\varphi,\psi\in A_G(E;E)\) are homogeneous with \(\deg\varphi=m\) and \(\deg\psi=n\), then it is obvious that \(\varphi\psi\)~is homogeneous with \(\deg(\varphi\psi)=m+n\). It follows from this that \(A_G(E;E)\)~is a \(G\)-graded subalgebra.
\end{proof}

\begin{exer}[7]
Let \(E,E^*\) be a pair of almost finite dual \(G\)-graded vector spaces. Then the mapping
\[\Phi:A_G(E;E)\to\opp{A_G(E^*;E^*)}\]
defined by \(\varphi\mapsto\varphi^*\) is an algebra isomorphism.
\end{exer}
\begin{proof}
The mapping is well defined by problems 6 and~10 of~\S~1, and is an isomorphism by problem~12 of Chapter~V, \S~1.
\end{proof}

% References
\begin{thebibliography}{0}
\bibitem{greub} Greub, W. \textit{Linear Algebra}, 4th~ed. Springer, 1975.
\end{thebibliography}
\end{document}